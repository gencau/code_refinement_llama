{"id": 0, "old": " int s2n_connection_get_peer_cert_chain(const struct s2n_connection *conn, struct\n \n     int cert_count = sk_X509_num(cert_chain_validated);\n \n-    /* Check to ensure cert_count is non-negative before casting */\n     POSIX_ENSURE_GTE(cert_count, 0);\n \n     for (size_t cert_idx = 0; cert_idx < (size_t) cert_count; cert_idx++) {", "new": " int s2n_connection_get_peer_cert_chain(const struct s2n_connection *conn, struct\n \n     int cert_count = sk_X509_num(cert_chain_validated);\n \n     POSIX_ENSURE_GTE(cert_count, 0);\n \n     for (size_t cert_idx = 0; cert_idx < (size_t) cert_count; cert_idx++) {", "review": "nit: I'm not really sure if these comments add very much. If the check comes right after the call, it might be clear enough what the check is for.\r\n```suggestion\r\n    POSIX_ENSURE_GTE(cert_count, 0);\r\n```", "dataset": "codereview_new", "original_id": 6273, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Documentation-remove", "category_type_user1": "Documentation-remove", "category_type_user2": "Documentation-remove", "gpt_code": "\nint s2n_connection_get_peer_cert_chain(const struct s2n_connection *conn, struct\n\n    int cert_count = sk_X509_num(cert_chain_validated);\n\n    // Ensure cert_count is non-negative before casting\n    POSIX_ENSURE_GTE(cert_count, 0);\n\n    for (size_t cert_idx = 0; cert_idx < (size_t) cert_count; cert_idx++) {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 85.80624, "model_code": " int s2n_connection_get_peer_cert_chain(const struct s2n_connection *conn, struct \n *cert_chain_validated) { \n int cert_count = sk_X509_num(cert_chain_validated); \n for (size_t cert_idx = 0; cert_idx < (size_t) cert_count; cert_idx++) {", "model_em_trim": 0, "model_bleu_trim": 83.62424073717608, "error_type_gpt": "Unclear Location"}
{"id": 1, "old": " public void testGetWebSocketReadUri(String msgId, String msgIdQueryParam) throws\n     }\n \n     @Test\n-    public void testPrseMessageId() {\n         assertEquals(CmdRead.parseMessageId(\"latest\"), MessageId.latest);\n         assertEquals(CmdRead.parseMessageId(\"earliest\"), MessageId.earliest);\n         assertEquals(CmdRead.parseMessageId(\"20:-1\"), new MessageIdImpl(20, -1, -1));", "new": " public void testGetWebSocketReadUri(String msgId, String msgIdQueryParam) throws\n     }\n \n     @Test\n+    public void testParseMessageId() {\n         assertEquals(CmdRead.parseMessageId(\"latest\"), MessageId.latest);\n         assertEquals(CmdRead.parseMessageId(\"earliest\"), MessageId.earliest);\n         assertEquals(CmdRead.parseMessageId(\"20:-1\"), new MessageIdImpl(20, -1, -1));", "review": "maybe typo:\r\n```suggestion\r\n    public void testParseMessageId() {\r\n```", "dataset": "codereview_new", "original_id": 7316, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\npublic void testGetWebSocketReadUri(String msgId, String msgIdQueryParam) throws\n    }\n\n    @Test\n    public void testParseMessageId() {\n        assertEquals(CmdRead.parseMessageId(\"latest\"), MessageId.latest);\n        assertEquals(CmdRead.parseMessageId(\"earliest\"), MessageId.earliest);\n        assertEquals(CmdRead.parseMessageId(\"20:-1\"), new MessageIdImpl(20, -1, -1));\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " public void testGetWebSocketReadUri(String msgId, String msgIdQueryParam) throws \n } \n \n @Test \n public void testParseMessageId() { \n assertEquals(CmdRead.parseMessageId(\"latest\"), MessageId.latest); \n assertEquals(CmdRead.parseMessageId(\"earliest\"), MessageId.earliest); \n assertEquals(CmdRead.parseMessageId(\"20:-1\"), new MessageIdImpl(20, -1, -1));", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 2, "old": " func IsTokenExpired() bool {\n \t\treturn false\n \t}\n }\n-\n-// Set ServiceAccountPath\n-func SetDefaultServiceAccountPath(path string) {\n-\tDefaultServiceAccountPath = path\n-}\n-\n-// Set Token Expiration\n-func SetTokenExpireDuration(expireTime time.Duration) {\n-\ttokenExpireDuration = expireTime\n-}", "new": " func IsTokenExpired() bool {\n \t\treturn false\n \t}\n }", "review": "Are these being used anywhere?", "dataset": "codereview_new", "original_id": 6377, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n// Check if token is expired\nfunc IsTokenExpired() bool {\n    return false\n}\n\n// Set default service account path\nfunc SetDefaultServiceAccountPath(path string) {\n    DefaultServiceAccountPath = path\n}\n\n// Set token expiration duration\nfunc SetTokenExpireDuration(expireTime time.Duration) {\n    tokenExpireDuration = expireTime\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 90.48374, "model_code": " func IsTokenExpired() bool { \n return false \n } \n }", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Need Domain Knowledge"}
{"id": 3, "old": " def users_with_translations_counts(\n     # Assign properties to user objects.\n     contributors = User.objects.filter(pk__in=user_stats.keys())\n \n-    # Exclude system users\n-    contributors = contributors.exclude(profile__system_user=True)\n \n     # Exclude deleted users.\n     contributors = contributors.filter(is_active=True)", "new": " def users_with_translations_counts(\n     # Assign properties to user objects.\n     contributors = User.objects.filter(pk__in=user_stats.keys())\n \n+    # Exclude system users.\n+    contributors = contributors.filter(profile__system_user=False)\n \n     # Exclude deleted users.\n     contributors = contributors.filter(is_active=True)", "review": "To be consistent with the line you added above, please invert this QuerySet to use `exclude()` instead of `filter()` and filp the boolean.\r\n\r\nNit: Please also make sure punctuation in the comments is consistent. :)", "dataset": "codereview_new", "original_id": 5247, "relevance": "not", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\ndef users_with_translations_counts(\n    # Assign properties to user objects.\n    contributors=User.objects.filter(pk__in=user_stats.keys())\n        .exclude(profile__system_user=True)\n        .exclude(is_active=False)  # Inverted filter to exclude deleted users.\n):\n", "gpt_em_trim": 0, "gpt_bleu_trim": 71.95549, "model_code": " def users_with_translations_counts( \n # Assign properties to user objects. \n contributors = User.objects.filter(pk__in=user_stats.keys())", "model_em_trim": 0, "model_bleu_trim": 37.76889859517124}
{"id": 4, "old": " dummy_func(\n             STAT_INC(FOR_ITER, deferred);\n             DECREMENT_ADAPTIVE_COUNTER(cache->counter);\n             #endif  /* ENABLE_SPECIALIZATION */\n-            /* before: [iter]; after: [iter, iter()] *or* [] (and jump an extra instr.) */\n             next = (*Py_TYPE(iter)->tp_iternext)(iter);\n             if (next == NULL) {\n                 if (_PyErr_Occurred(tstate)) {", "new": " dummy_func(\n             STAT_INC(FOR_ITER, deferred);\n             DECREMENT_ADAPTIVE_COUNTER(cache->counter);\n             #endif  /* ENABLE_SPECIALIZATION */\n+            /* before: [iter]; after: [iter, iter()] *or* [] (and jump over END_FOR.) */\n             next = (*Py_TYPE(iter)->tp_iternext)(iter);\n             if (next == NULL) {\n                 if (_PyErr_Occurred(tstate)) {", "review": "```suggestion\r\n            /* before: [iter]; after: [iter, iter()] *or* [] (and jump over END_FOR.) */\r\n```", "dataset": "codereview_new", "original_id": 9355, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\ndummy_func(\n            STAT_INC(FOR_ITER, deferred);\n            DECREMENT_ADAPTIVE_COUNTER(cache->counter);\n            #endif  /* ENABLE_SPECIALIZATION */\n            /* before: [iter]; after: [iter, iter()] *or* [] (and jump over END_FOR.) */\n            next = (*Py_TYPE(iter)->tp_iternext)(iter);\n            if (next == NULL) {\n                if (_PyErr_Occurred(tstate)) {\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " dummy_func( \n STAT_INC(FOR_ITER, deferred); \n DECREMENT_ADAPTIVE_COUNTER(cache->counter); \n #endif  /* ENABLE_SPECIALIZATION */ \n next = (*Py_TYPE(iter)->tp_iternext)(iter); \n if (next == NULL) { \n if (_PyErr_Occurred(tstate)) {", "model_em_trim": 0, "model_bleu_trim": 64.55459688837038}
{"id": 5, "old": " func (fnb *FlowNodeBuilder) InitIDProviders() {\n \tfnb.Module(\"id providers\", func(node *NodeConfig) error {\n \t\tidCache, err := cache.NewProtocolStateIDCache(node.Logger, node.State, node.ProtocolEvents)\n \t\tif err != nil {\n-\t\t\treturn err\n \t\t}\n \t\tnode.IDTranslator = idCache\n \n \t\t// The following wrapper allows to black-list byzantine nodes via an admin command:\n \t\t// the wrapper overrides the 'Ejected' flag of blocked nodes to true\n \t\tnode.IdentityProvider, err = cache.NewNodeBlocklistWrapper(idCache, node.DB)\n \t\tif err != nil {\n-\t\t\treturn err\n \t\t}\n \n \t\tnode.SyncEngineIdentifierProvider = id.NewIdentityFilterIdentifierProvider(", "new": " func (fnb *FlowNodeBuilder) InitIDProviders() {\n \tfnb.Module(\"id providers\", func(node *NodeConfig) error {\n \t\tidCache, err := cache.NewProtocolStateIDCache(node.Logger, node.State, node.ProtocolEvents)\n \t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"could not initialize ProtocolStateIDCache: %w\", err)\n \t\t}\n \t\tnode.IDTranslator = idCache\n \n \t\t// The following wrapper allows to black-list byzantine nodes via an admin command:\n \t\t// the wrapper overrides the 'Ejected' flag of blocked nodes to true\n \t\tnode.IdentityProvider, err = cache.NewNodeBlocklistWrapper(idCache, node.DB)\n \t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"could not initialize NodeBlocklistWrapper: %w\", err)\n \t\t}\n \n \t\tnode.SyncEngineIdentifierProvider = id.NewIdentityFilterIdentifierProvider(", "review": "```suggestion\r\n\t\t\treturn fmt.Errorf(\"could not initialize black list wrapper: %w\", err)\r\n```", "dataset": "codereview_new", "original_id": 5525, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfunc (fnb *FlowNodeBuilder) InitIDProviders() {\n\tfnb.Module(\"id providers\", func(node *NodeConfig) error {\n\t\tidCache, err := cache.NewProtocolStateIDCache(node.Logger, node.State, node.ProtocolEvents)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tnode.IDTranslator = idCache\n\n\t\t// The following wrapper allows to black-list byzantine nodes via an admin command:\n\t\t// the wrapper overrides the 'Ejected' flag of blocked nodes to true\n\t\tnode.IdentityProvider, err = cache.NewNodeBlocklistWrapper(idCache, node.DB)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"could not initialize black list wrapper: %w\", err)\n\t\t}\n\n\t\tnode.SyncEngineIdentifierProvider = id.NewIdentityFilterIdentifierProvider(\n", "gpt_em_trim": 0, "gpt_bleu_trim": 87.09182, "model_code": " func (fnb *FlowNodeBuilder) InitIDProviders() { \n fnb.Module(\"id providers\", func(node *NodeConfig) error { \n node.IDTranslator = cache.NewProtocolStateIDCache(node.Logger, node.State, node.ProtocolEvents) \n node.IdentityProvider, err = cache.NewNodeBlocklistWrapper(node.DB) \n if err != nil { \n return err \n } \n node.SyncEngineIdentifierProvider = id.NewIdentityFilterIdentifierProvider(", "model_em_trim": 0, "model_bleu_trim": 33.07787963345403}
{"id": 6, "old": " interface TransferRepository {\n     fun removeTransferById(id: Long)\n     fun removeAllTransfersFromAccount(accountName: String)\n     fun getTransferById(id: Long): OCTransfer?\n-    fun getAllTransfers(): LiveData<List<OCTransfer>>\n     fun getLastTransferFor(remotePath: String, accountName: String): OCTransfer?\n     fun getCurrentAndPendingTransfers(): List<OCTransfer>\n     fun getFailedTransfers(): List<OCTransfer>", "new": " interface TransferRepository {\n     fun removeTransferById(id: Long)\n     fun removeAllTransfersFromAccount(accountName: String)\n     fun getTransferById(id: Long): OCTransfer?\n+    fun getAllTransfersAsLiveData(): LiveData<List<OCTransfer>>\n     fun getLastTransferFor(remotePath: String, accountName: String): OCTransfer?\n     fun getCurrentAndPendingTransfers(): List<OCTransfer>\n     fun getFailedTransfers(): List<OCTransfer>", "review": "I would add a suffix to see directly that we retrieve a livedata instead of a list. But we need to check all the app naming after we finish the synchronization rearchitecture, so, up to you  \ud83d\udc4d ", "dataset": "codereview_new", "original_id": 1639, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\ninterface TransferRepository {\n    fun removeTransferById(id: Long)\n    fun removeAllTransfersFromAccount(accountName: String)\n    fun getTransferById(id: Long): OCTransfer?\n    fun getAllTransfersLiveData(): LiveData<List<OCTransfer>> // Updated method name\n    fun getLastTransferFor(remotePath: String, accountName: String): OCTransfer?\n    fun getCurrentAndPendingTransfers(): List<OCTransfer>\n    fun getFailedTransfers(): List<OCTransfer>\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 87.95245, "model_code": " interface TransferRepository { \n fun removeTransferById(id: Long) \n fun removeAllTransfersFromAccount(accountName: String) \n fun getAllTransfers(): LiveData<List<OCTransfer>> \n fun getLastTransferFor(remotePath: String, accountName: String): OCTransfer? \n fun getCurrentAndPendingTransfers(): List<OCTransfer> \n fun getFailedTransfers(): List<OCTransfer>", "model_em_trim": 0, "model_bleu_trim": 81.53554373217247, "error_type_gpt": "Need Domain Knowledge"}
{"id": 7, "old": " class PermissionsPerWebsiteActivity : DuckDuckGoActivity(), PermissionsSettingsS\n     private fun setViews() {\n         setupToolbar(toolbar)\n         supportActionBar?.title = url.websiteFromGeoLocationsApiOrigin()\n-        binding.sitePermissionsSectionTitle.text = String.format(\n-            getString(R.string.permissionPerWebsiteText),\n-            url.websiteFromGeoLocationsApiOrigin()\n-        )\n         binding.permissionsPerWebsiteRecyclerView.adapter = adapter\n     }\n ", "new": " class PermissionsPerWebsiteActivity : DuckDuckGoActivity(), PermissionsSettingsS\n     private fun setViews() {\n         setupToolbar(toolbar)\n         supportActionBar?.title = url.websiteFromGeoLocationsApiOrigin()\n+        binding.sitePermissionsSectionTitle.text = String.format(getString(R.string.permissionPerWebsiteText), url.websiteFromGeoLocationsApiOrigin())\n         binding.permissionsPerWebsiteRecyclerView.adapter = adapter\n     }\n ", "review": "is this intented?", "dataset": "codereview_new", "original_id": 2029, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "no code", "gpt_em_trim": 0, "gpt_bleu_trim": 2.4799999999999997e-84, "model_code": " class PermissionsPerWebsiteActivity : DuckDuckGoActivity(), PermissionsSettingsS \n private fun setViews() { \n setupToolbar(toolbar) \n binding.sitePermissionsSectionTitle.text = String.format( \n getString(R.string.permissionPerWebsiteText), \n url.websiteFromGeoLocationsApiOrigin() \n ) \n binding.permissionsPerWebsiteRecyclerView.adapter = adapter \n }", "model_em_trim": 0, "model_bleu_trim": 81.23885501465215, "error_type_gpt": "Need Domain Knowledge"}
{"id": 8, "old": " type errNPMUnavailable struct {\n }\n \n func (err *errNPMUnavailable) Error() string {\n-\treturn fmt.Sprintf(`\"npm\" is required to override with the Cloud Development Kit: %v`, err.parent)\n }\n \n // RecommendActions implements the cli.actionRecommender interface.", "new": " type errNPMUnavailable struct {\n }\n \n func (err *errNPMUnavailable) Error() string {\n+\treturn fmt.Sprintf(`\"npm\" cannot be found: \"npm\" is required to override with the Cloud Development Kit: %v`, err.parent)\n }\n \n // RecommendActions implements the cli.actionRecommender interface.", "review": "Maybe make this error msg more explicit. Like starting with\r\n`\"npm\" is not available ...` or `cannot find \"npm\"...`", "dataset": "codereview_new", "original_id": 4970, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\ntype errNPMUnavailable struct {\n}\n\nfunc (err *errNPMUnavailable) Error() string {\n\treturn fmt.Sprintf(`\"npm\" is not available to override with the Cloud Development Kit: %v`, err.parent)\n}\n\n// RecommendActions implements the cli.actionRecommender interface.\n", "gpt_em_trim": 0, "gpt_bleu_trim": 83.99285, "model_code": " type errNPMUnavailable struct { \n } \n // RecommendActions implements the cli.actionRecommender interface.", "model_em_trim": 0, "model_bleu_trim": 5.311528631961922, "error_type_gpt": "Need Domain Knowledge"}
{"id": 9, "old": " def build_mypy(target_dir: str) -> None:\n     env = os.environ.copy()\n     env[\"CC\"] = \"clang\"\n     env[\"MYPYC_OPT_LEVEL\"] = \"2\"\n-    cmd = [\"python3\", \"setup.py\", \"--use-mypyc\", \"build_ext\", \"--inplace\"]\n     subprocess.run(cmd, env=env, check=True, cwd=target_dir)\n \n ", "new": " def build_mypy(target_dir: str) -> None:\n     env = os.environ.copy()\n     env[\"CC\"] = \"clang\"\n     env[\"MYPYC_OPT_LEVEL\"] = \"2\"\n+    cmd = [sys.executable, \"setup.py\", \"--use-mypyc\", \"build_ext\", \"--inplace\"]\n     subprocess.run(cmd, env=env, check=True, cwd=target_dir)\n \n ", "review": "`python3` doesn't work on Windows; it's better to use `sys.executable` for cross-platform compatibility\r\n\r\n```suggestion\r\n    cmd = [sys.executable, \"setup.py\", \"--use-mypyc\", \"build_ext\", \"--inplace\"]\r\n```", "dataset": "codereview_new", "original_id": 8712, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nimport sys\n\ndef build_mypy(target_dir: str) -> None:\n    env = os.environ.copy()\n    env[\"CC\"] = \"clang\"\n    env[\"MYPYC_OPT_LEVEL\"] = \"2\"\n    cmd = [sys.executable, \"setup.py\", \"--use-mypyc\", \"build_ext\", \"--inplace\"]\n    subprocess.run(cmd, env=env, check=True, cwd=target_dir)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 100.0, "model_code": " def build_mypy(target_dir: str) -> None: \n env = os.environ.copy() \n cmd = [sys.executable, \"setup.py\", \"--use-mypyc\", \"build_ext\", \"--inplace\"] \n subprocess.run(cmd, env=env, check=True, cwd=target_dir)", "model_em_trim": 0, "model_bleu_trim": 72.04834107201367, "error_type_gpt": "Code Style Difference"}
{"id": 10, "old": " func (c APIClient) CreatePipeline(\n \treturn grpcutil.ScrubGRPC(err)\n }\n \n-// InspectPipeline returns info about a specific pipeline.\n-func (c APIClient) InspectPipeline(pipelineAncestry string, details bool) (*pps.PipelineInfo, error) {\n \tpipelineInfo, err := c.PpsAPIClient.InspectPipeline(\n \t\tc.Ctx(),\n \t\t&pps.InspectPipelineRequest{\n-\t\t\tPipeline: NewPipeline(pipelineAncestry),\n \t\t\tDetails:  details,\n \t\t},\n \t)", "new": " func (c APIClient) CreatePipeline(\n \treturn grpcutil.ScrubGRPC(err)\n }\n \n+// InspectPipeline returns info about a specific pipeline.  The name may include\n+// ancestry syntax or be a bare name.\n+func (c APIClient) InspectPipeline(pipelineName string, details bool) (*pps.PipelineInfo, error) {\n \tpipelineInfo, err := c.PpsAPIClient.InspectPipeline(\n \t\tc.Ctx(),\n \t\t&pps.InspectPipelineRequest{\n+\t\t\tPipeline: NewPipeline(pipelineName),\n \t\t\tDetails:  details,\n \t\t},\n \t)", "review": "I think the ancestry stuff should just be considered a pipeline name. Having the variables be called pipeline ancestry seems more confusing than potentially having a name with ancestry syntax.", "dataset": "codereview_new", "original_id": 8644, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\nfunc (c APIClient) CreatePipeline(\n\treturn grpcutil.ScrubGRPC(err)\n}\n\n// InspectPipeline returns info about a specific pipeline.\nfunc (c APIClient) InspectPipeline(pipelineName string, details bool) (*pps.PipelineInfo, error) {\n\tpipelineInfo, err := c.PpsAPIClient.InspectPipeline(\n\t\tc.Ctx(),\n\t\t&pps.InspectPipelineRequest{\n\t\t\tPipeline: NewPipeline(pipelineName),\n\t\t\tDetails:  details,\n\t\t},\n\t)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 83.8631, "model_code": " func (c APIClient) CreatePipeline( \n return grpcutil.ScrubGRPC(err) \n }", "model_em_trim": 0, "model_bleu_trim": 0.5946217356472094, "error_type_gpt": "Need Domain Knowledge"}
{"id": 11, "old": " func (e containerEnvVars) humanString(w io.Writer) {\n \tprintTable(w, headers, rows)\n }\n \n-// envSecret contains secrets for an rdws service.\n-type envSecret struct {\n \tEnvironment string `json:\"environment\"`\n \tName        string `json:\"name\"`\n \tValueFrom   string `json:\"value\"`\n }\n \n-type envSecrets []*envSecret\n \n-func (e envSecrets) humanString(w io.Writer) {\n \theaders := []string{\"Name\", \"Environment\", \"Value\"}\n \tvar rows [][]string\n-\tsort.SliceStable(e, func(i, j int) bool { return e[i].Environment < e[j].Environment })\n-\tsort.SliceStable(e, func(i, j int) bool { return e[i].Name < e[j].Name })\n \tfor _, v := range e {\n \t\trows = append(rows, []string{v.Name, v.Environment, v.ValueFrom})\n \t}", "new": " func (e containerEnvVars) humanString(w io.Writer) {\n \tprintTable(w, headers, rows)\n }\n \n+// rdwsSecret contains secrets for an rdws service.\n+type rdwsSecret struct {\n \tEnvironment string `json:\"environment\"`\n \tName        string `json:\"name\"`\n \tValueFrom   string `json:\"value\"`\n }\n \n+type rdwsSecrets []*rdwsSecret\n \n+func (e rdwsSecrets) humanString(w io.Writer) {\n \theaders := []string{\"Name\", \"Environment\", \"Value\"}\n \tvar rows [][]string\n+\tsort.SliceStable(e, func(i, j int) bool {\n+\t\tif e[i].Name == e[j].Name {\n+\t\t\treturn e[i].Environment < e[j].Environment\n+\t\t}\n+\t\treturn e[i].Name < e[j].Name\n+\t})\n \tfor _, v := range e {\n \t\trows = append(rows, []string{v.Name, v.Environment, v.ValueFrom})\n \t}", "review": "nit: perhaps calling `SliceStable` twice gives us the result that we want for now, but it might not in the future because go lib is free to change whatever algorithm they use under the hood. I think this \u2b07\ufe0f should guarantee to get what's intended here\r\n```suggestion\r\n\tsort.SliceStable(e, func(i, j int) bool { \r\n\t  if e[i].Name == e[j].Name {\r\n\t     return  e[i].Environment < e[j].Environment\r\n\t  }\r\n\treturn e[i].Name < e[j].Name \r\n\t})\r\n```", "dataset": "codereview_new", "original_id": 4934, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfunc (e containerEnvVars) humanString(w io.Writer) {\n\tprintTable(w, headers, rows)\n}\n\n// envSecret contains secrets for an rdws service.\ntype envSecret struct {\n\tEnvironment string `json:\"environment\"`\n\tName        string `json:\"name\"`\n\tValueFrom   string `json:\"value\"`\n}\n\ntype envSecrets []*envSecret\n\nfunc (e envSecrets) humanString(w io.Writer) {\n\theaders := []string{\"Name\", \"Environment\", \"Value\"}\n\tvar rows [][]string\n\tsort.SliceStable(e, func(i, j int) bool {\n\t\tif e[i].Name == e[j].Name {\n\t\t\treturn e[i].Environment < e[j].Environment\n\t\t}\n\t\treturn e[i].Name < e[j].Name\n\t})\n\tfor _, v := range e {\n\t\trows = append(rows, []string{v.Name, v.Environment, v.ValueFrom})\n\t}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 93.79811, "model_code": " func (e containerEnvVars) humanString(w io.Writer) { \n printTable(w, headers, rows) \n }", "model_em_trim": 0, "model_bleu_trim": 0.03659391562044393, "error_type_gpt": "Need Domain Knowledge"}
{"id": 12, "old": " sub get_tags {\n     my ($name, $path, $suffix) = fileparse($lrr_info->{file_path}, qr/\\.[^.]*/);\n     my $path_nearby_json = $path . $name . '.json';\n \n-    my $filepath = undef;\n-    my $delete_after_parse = undef;\n     \n     #Extract info.json\n     if($path_in_archive) {", "new": " sub get_tags {\n     my ($name, $path, $suffix) = fileparse($lrr_info->{file_path}, qr/\\.[^.]*/);\n     my $path_nearby_json = $path . $name . '.json';\n \n+    my $filepath;\n+    my $delete_after_parse;\n     \n     #Extract info.json\n     if($path_in_archive) {", "review": "The critic CI shouldve yelled about this too (looks like its checks API integration is broken oops), but there's no need to initialize variables to `undef`:  \r\n\r\n```\r\nmy $filepath;\r\nmy $delete_after_parse;\r\n```", "dataset": "codereview_new", "original_id": 2916, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nsub get_tags {\n    my ($name, $path, $suffix) = fileparse($lrr_info->{file_path}, qr/\\.[^.]*/);\n    my $path_nearby_json = $path . $name . '.json';\n\n    my $filepath;\n    my $delete_after_parse;\n    \n    #Extract info.json\n    if($path_in_archive) {\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " sub get_tags { \n my ($name, $path, $suffix) = fileparse($lrr_info->{file_path}, qr/\\.[^.]*/); \n my $path_nearby_json = $path . $name . '.json'; \n #Extract info.json \n if($path_in_archive) {", "model_em_trim": 0, "model_bleu_trim": 84.88144832418679}
{"id": 13, "old": " import scala.collection.immutable.SortedSet\n \n /**\n   * Environment tracking the rigidity of type variables.\n   */\n case class RigidityEnv(s: SortedSet[Symbol.KindedTypeVarSym]) {\n ", "new": " import scala.collection.immutable.SortedSet\n \n /**\n   * Environment tracking the rigidity of type variables.\n+  *\n+  * `s` holds the set of rigid variable symbols.\n+  * All variables not in `s` are considered flexible.\n   */\n case class RigidityEnv(s: SortedSet[Symbol.KindedTypeVarSym]) {\n ", "review": "Describe the invariant, i.e. only rigid variables are in the set. Others are flexible.", "dataset": "codereview_new", "original_id": 3811, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Documentation-add", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-add", "gpt_code": "\nimport scala.collection.immutable.SortedSet\n\n/**\n  * Environment tracking the rigidity of type variables.\n  * The set only contains rigid type variables. All other variables are flexible.\n  */\ncase class RigidityEnv(s: SortedSet[Symbol.KindedTypeVarSym]) {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 63.21297, "model_code": " import scala.collection.immutable.SortedSet \n /** \n * Environment tracking the rigidity of type variables. \n * \n * Note: only rigid variables are in the set. \n */ \n case class RigidityEnv(s: SortedSet[Symbol.KindedTypeVarSym]) { \n", "model_em_trim": 0, "model_bleu_trim": 62.15257246173185, "error_type_gpt": "Need Domain Knowledge"}
{"id": 14, "old": " generator_funs$new <- function(...) {\n     if (!identical(cloneable, inherit$cloneable)) {\n       if (inherit$cloneable) {\n         inherit[[\"public_methods\"]][[\"clone\"]] <- NULL\n-      }\n-\n-      if (!inherit$cloneable) {\n-        message(c(\n-          \"Subclass wants to allow cloning, but superclass has turned it off. \",\n-          \"Therefore, cloning will also be turned off for subclass.\"\n-        ))\n \n         public_methods[[\"clone\"]] <- NULL\n       }", "new": " generator_funs$new <- function(...) {\n     if (!identical(cloneable, inherit$cloneable)) {\n       if (inherit$cloneable) {\n         inherit[[\"public_methods\"]][[\"clone\"]] <- NULL\n+      } else {\n+        message(\n+          \"Superclass \", get_superclassnames(inherit), \" has cloneable=FALSE, but subclass \", classname, \" has cloneable=TRUE. \",\n+          \"A subclass cannot be cloneable when its superclass is not cloneable, so cloning will be disabled for \", classname, \".\"\n+        )\n \n         public_methods[[\"clone\"]] <- NULL\n       }", "review": "Placeholder. Can be tweaked further.", "dataset": "codereview_new", "original_id": 2894, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\ngenerator_funs$new <- function(...) {\n    if (!identical(cloneable, inherit$cloneable)) {\n      if (inherit$cloneable) {\n        inherit[[\"public_methods\"]][[\"clone\"]] <- NULL\n      }\n\n      if (!inherit$cloneable) {\n        message(\"Subclass wants to allow cloning, but superclass has turned it off. Therefore, cloning will also be turned off for subclass.\")\n        public_methods[[\"clone\"]] <- NULL\n      }\n    }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 57.4469, "model_code": " generator_funs$new <- function(...) { \n if (!identical(cloneable, inherit$cloneable)) { \n if (inherit$cloneable) { \n inherit[[\"public_methods\"]][[\"clone\"]] <- NULL \n }", "model_em_trim": 0, "model_bleu_trim": 25.739514205256803}
{"id": 15, "old": " extension ReadingListPanel: NotificationThemeable {\n         refreshReadingList()\n     }\n }\n-\n-extension ReadingListPanel {\n-\n-}", "new": " extension ReadingListPanel: NotificationThemeable {\n         refreshReadingList()\n     }\n }", "review": "Empty extension?", "dataset": "codereview_new", "original_id": 279, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nextension ReadingListPanel: NotificationThemeable {\n    func refreshReadingList() {\n        // implementation goes here\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 36.26754, "model_code": " extension ReadingListPanel: NotificationThemeable { \n refreshReadingList() \n } \n }", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 16, "old": " final class TestSuitesProvider(\n     new MunitTestFinder(trees, symbolIndex, semanticdbs)\n   private val scalatestTestFinder =\n     new ScalatestTestFinder(trees, symbolIndex, semanticdbs)\n-  private val isExplorerEnabled = clientConfig.isTestExplorerProvider() &&\n     userConfig().testUserInterface == TestUserInterfaceKind.TestExplorer\n \n   override def isEnabled: Boolean = (clientConfig.isDebuggingProvider() &&", "new": " final class TestSuitesProvider(\n     new MunitTestFinder(trees, symbolIndex, semanticdbs)\n   private val scalatestTestFinder =\n     new ScalatestTestFinder(trees, symbolIndex, semanticdbs)\n+\n+  private def isExplorerEnabled = clientConfig.isTestExplorerProvider() &&\n     userConfig().testUserInterface == TestUserInterfaceKind.TestExplorer\n \n   override def isEnabled: Boolean = (clientConfig.isDebuggingProvider() &&", "review": "this needs to be a def because `userConfig` can change", "dataset": "codereview_new", "original_id": 3455, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfinal class TestSuitesProvider(\n    new MunitTestFinder(trees, symbolIndex, semanticdbs)\n  private val scalatestTestFinder =\n    new ScalatestTestFinder(trees, symbolIndex, semanticdbs)\n  private def isExplorerEnabled = clientConfig.isTestExplorerProvider() &&\n    userConfig().testUserInterface == TestUserInterfaceKind.TestExplorer\n\n  override def isEnabled: Boolean = (clientConfig.isDebuggingProvider() &&\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " final class TestSuitesProvider( \n new MunitTestFinder(trees, symbolIndex, semanticdbs) \n @VisibleForTesting \n private val scalatestTestFinder = \n new ScalatestTestFinder(trees, symbolIndex, semanticdbs) \n @VisibleForTesting \n private val isExplorerEnabled = clientConfig.isTestExplorerProvider() && \n userConfig().testUserInterface == TestUserInterfaceKind.TestExplorer \n @VisibleForTesting \n override def isEnabled: Boolean = (clientConfig.isDebuggingProvider() &&", "model_em_trim": 0, "model_bleu_trim": 81.23519741282699}
{"id": 17, "old": " DataWriterImpl::DataWriterImpl(\n     , qos_(&qos == &DATAWRITER_QOS_DEFAULT ? publisher_->get_default_datawriter_qos() : qos)\n     , listener_(listen)\n     , history_(get_topic_attributes(qos_, *topic_, type_), type_->m_typeSize, qos_.endpoint().history_memory_policy,\n-            [&](\n                 const InstanceHandle_t& handle) -> void\n             {\n                 if (nullptr != listener_)", "new": " DataWriterImpl::DataWriterImpl(\n     , qos_(&qos == &DATAWRITER_QOS_DEFAULT ? publisher_->get_default_datawriter_qos() : qos)\n     , listener_(listen)\n     , history_(get_topic_attributes(qos_, *topic_, type_), type_->m_typeSize, qos_.endpoint().history_memory_policy,\n+            [this](\n                 const InstanceHandle_t& handle) -> void\n             {\n                 if (nullptr != listener_)", "review": "```suggestion\r\n            [this](\r\n```", "dataset": "codereview_new", "original_id": 5948, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nDataWriterImpl::DataWriterImpl(\n    , qos_(&qos == &DATAWRITER_QOS_DEFAULT ? publisher_->get_default_datawriter_qos() : qos)\n    , listener_(listen)\n    , history_(get_topic_attributes(qos_, *topic_, type_), type_->m_typeSize, qos_.endpoint().history_memory_policy,\n            [this](const InstanceHandle_t& handle) -> void {\n                if (nullptr != listener_)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 100.0, "model_code": " DataWriterImpl::DataWriterImpl( \n , qos_(&qos == &DATAWRITER_QOS_DEFAULT ? publisher_->get_default_datawriter_qos() : qos) \n , listener_(listen) \n , history_(get_topic_attributes(qos_, *topic_, type_), type_->m_typeSize, qos_.endpoint().history_memory_policy, \n [this]( \n const InstanceHandle_t& handle) -> void \n { \n if (nullptr != listener_)", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Code Style Difference"}
{"id": 18, "old": " public fun <A> Iterable<A>.salign(\n /**\n  * aligns two structures and combine them with the given [Semigroup.combine]\n  */\n-@Deprecated(SemigroupDeprecation, ReplaceWith(\"salign(SG::combine, other)\"))\n public fun <A> Iterable<A>.salign(\n   SG: Semigroup<A>,\n   other: Iterable<A>", "new": " public fun <A> Iterable<A>.salign(\n /**\n  * aligns two structures and combine them with the given [Semigroup.combine]\n  */\n+@Deprecated(SemigroupDeprecation, ReplaceWith(\"salign(SG::combine, other)\", \"arrow.typeclasses.combine\"))\n public fun <A> Iterable<A>.salign(\n   SG: Semigroup<A>,\n   other: Iterable<A>", "review": "```suggestion\r\n@Deprecated(SemigroupDeprecation, ReplaceWith(\"salign(SG::combine, other)\", \"arrow.typeclasses.combine\"))\r\n```", "dataset": "codereview_new", "original_id": 1150, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\npublic fun <A> Iterable<A>.salign(\n/**\n * aligns two structures and combine them with the given [Semigroup.combine]\n */\n@Deprecated(SemigroupDeprecation, ReplaceWith(\"salign(SG::combine, other)\", \"arrow.typeclasses.combine\"))\npublic fun <A> Iterable<A>.salign(\n  SG: Semigroup<A>,\n  other: Iterable<A>\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " public fun <A> Iterable<A>.salign( \n /** \n * aligns two structures and combine them with the given [Semigroup.combine] \n */ \n @Deprecated(SemigroupDeprecation, ReplaceWith(\"salign(SG::combine, other)\", \"arrow.typeclasses.combine\")) \n public fun <A> Iterable<A>.salign( \n SG: Semigroup<A>, \n other: Iterable<A>", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 19, "old": " sealed class RendererViewHolder {\n }\n \n @ContributesBinding(ActivityScope::class)\n-@SingleInstanceIn(ActivityScope::class)\n class BrowserPrivacyDashboardRendererFactory @Inject constructor(\n     val moshi: Moshi\n ) : PrivacyDashboardRendererFactory {", "new": " sealed class RendererViewHolder {\n }\n \n @ContributesBinding(ActivityScope::class)\n class BrowserPrivacyDashboardRendererFactory @Inject constructor(\n     val moshi: Moshi\n ) : PrivacyDashboardRendererFactory {", "review": "Why a factory when we only have on renderer? Why not directly creating it and save a bunch of extra code?", "dataset": "codereview_new", "original_id": 2066, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-remove", "gpt_code": "\nsealed class RendererViewHolder {\n}\n\n@ContributesBinding(ActivityScope::class)\n@SingleInstanceIn(ActivityScope::class)\nclass BrowserPrivacyDashboardRenderer @Inject constructor(\n    val moshi: Moshi\n) : PrivacyDashboardRenderer {\n    // implementation of PrivacyDashboardRenderer interface\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 63.9187, "model_code": " sealed class RendererViewHolder { \n } \n class BrowserPrivacyDashboardRendererFactory { \n @Inject constructor( \n val moshi: Moshi \n ) : PrivacyDashboardRendererFactory {", "model_em_trim": 0, "model_bleu_trim": 56.86454904142219, "error_type_gpt": "Need Domain Knowledge"}
{"id": 20, "old": " final class BackupStatusCell: UITableViewCell {\n         backgroundColor = .clear\n         contentView.backgroundColor = .clear\n \n-        tintColor = SemanticColors.LabelsColor.textLabelSettingsCell\n         iconView.setTemplateIcon(.restore, size: .large)\n         iconView.contentMode = .center\n         iconView.translatesAutoresizingMaskIntoConstraints = false", "new": " final class BackupStatusCell: UITableViewCell {\n         backgroundColor = .clear\n         contentView.backgroundColor = .clear\n \n+        iconView.tintColor = SemanticColors.LabelsColor.textLabelSettingsCell\n         iconView.setTemplateIcon(.restore, size: .large)\n         iconView.contentMode = .center\n         iconView.translatesAutoresizingMaskIntoConstraints = false", "review": "Here I think you're setting the tintColor for the cell not the iconView. Please update it.", "dataset": "codereview_new", "original_id": 637, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfinal class BackupStatusCell: UITableViewCell {\n        backgroundColor = .clear\n        contentView.backgroundColor = .clear\n\n        SemanticColors.LabelsColor.textLabelSettingsCell\n        iconView.setTemplateIcon(.restore, size: .large)\n        iconView.tintColor = SemanticColors.LabelsColor.textLabelSettingsCell\n        iconView.contentMode = .center\n        iconView.translatesAutoresizingMaskIntoConstraints = false\n", "gpt_em_trim": 0, "gpt_bleu_trim": 86.9799, "model_code": " final class BackupStatusCell: UITableViewCell { \n backgroundColor = .clear \n contentView.backgroundColor = .clear \n iconView.setTemplateIcon(.restore, size: .large) \n iconView.contentMode = .center \n iconView.translatesAutoresizingMaskIntoConstraints = false", "model_em_trim": 0, "model_bleu_trim": 79.30652775301904, "error_type_gpt": "Code Style Difference"}
{"id": 21, "old": " class TLBundle(val params: TLBundleParameters) extends Record\n   private val optD = Some                (Flipped(Decoupled(new TLBundleD(params))))\n   private val optE = params.hasBCE.option(Decoupled(new TLBundleE(params)))\n \n-  def a: DecoupledIO[TLBundleA] = optA.getOrElse(WireInit(0.U.asTypeOf(Decoupled(new TLBundleA(params)))))\n-  def b: DecoupledIO[TLBundleB] = optB.getOrElse(WireInit(0.U.asTypeOf(Decoupled(new TLBundleB(params)))))\n-  def c: DecoupledIO[TLBundleC] = optC.getOrElse(WireInit(0.U.asTypeOf(Decoupled(new TLBundleC(params)))))\n-  def d: DecoupledIO[TLBundleD] = optD.getOrElse(WireInit(0.U.asTypeOf(Decoupled(new TLBundleD(params)))))\n-  def e: DecoupledIO[TLBundleE] = optE.getOrElse(WireInit(0.U.asTypeOf(Decoupled(new TLBundleE(params)))))\n \n   override def cloneType: this.type = (new TLBundle(params)).asInstanceOf[this.type]\n   val elements =", "new": " class TLBundle(val params: TLBundleParameters) extends Record\n   private val optD = Some                (Flipped(Decoupled(new TLBundleD(params))))\n   private val optE = params.hasBCE.option(Decoupled(new TLBundleE(params)))\n \n+  def a: DecoupledIO[TLBundleA] = optA.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleA(params)))))\n+  def b: DecoupledIO[TLBundleB] = optB.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleB(params)))))\n+  def c: DecoupledIO[TLBundleC] = optC.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleC(params)))))\n+  def d: DecoupledIO[TLBundleD] = optD.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleD(params)))))\n+  def e: DecoupledIO[TLBundleE] = optE.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleE(params)))))\n \n   override def cloneType: this.type = (new TLBundle(params)).asInstanceOf[this.type]\n   val elements =", "review": "```suggestion\r\n  def a: DecoupledIO[TLBundleA] = optA.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleA(params)))))\r\n  def b: DecoupledIO[TLBundleB] = optB.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleB(params)))))\r\n  def c: DecoupledIO[TLBundleC] = optC.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleC(params)))))\r\n  def d: DecoupledIO[TLBundleD] = optD.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleD(params)))))\r\n  def e: DecoupledIO[TLBundleE] = optE.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleE(params)))))\r\n```", "dataset": "codereview_new", "original_id": 3273, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass TLBundle(val params: TLBundleParameters) extends Record\n  private val optD = Some                (Flipped(Decoupled(new TLBundleD(params))))\n  private val optE = params.hasBCE.option(Decoupled(new TLBundleE(params)))\n\n  def a: DecoupledIO[TLBundleA] = optA.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleA(params)))))\n  def b: DecoupledIO[TLBundleB] = optB.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleB(params)))))\n  def c: DecoupledIO[TLBundleC] = optC.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleC(params)))))\n  def d: DecoupledIO[TLBundleD] = optD.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleD(params)))))\n  def e: DecoupledIO[TLBundleE] = optE.getOrElse(WireDefault(0.U.asTypeOf(Decoupled(new TLBundleE(params)))))\n\n  override def cloneType: this.type = (new TLBundle(params)).asInstanceOf[this.type]\n  val elements =\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " class TLBundle(val params: TLBundleParameters) extends Record \n private val optD = Some                (Flipped(Decoupled(new TLBundleD(params)))) \n private val optE = params.hasBCE.option(Decoupled(new TLBundleE(params)))", "model_em_trim": 0, "model_bleu_trim": 2.3517745856009107}
{"id": 22, "old": " snapshot_accept_hint <- function(variant, file, reset_output = TRUE) {\n snapshot_not_available <- function(message) {\n   cli::cli_inform(c(\n     \"{.strong Can't compare snapshot to reference when testing interactively.}\",\n-    i = \"Run {.run devtools::test()} or `testthat::test_file()` to see changes.\",\n     i = message\n   ))\n }", "new": " snapshot_accept_hint <- function(variant, file, reset_output = TRUE) {\n snapshot_not_available <- function(message) {\n   cli::cli_inform(c(\n     \"{.strong Can't compare snapshot to reference when testing interactively.}\",\n+    i = \"Run {.run devtools::test()} or {.code testthat::test_file()} to see changes.\",\n     i = message\n   ))\n }", "review": "I guess the second option is on purpose not `{.run}`, but it could be `{.code}`.", "dataset": "codereview_new", "original_id": 2532, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nsnapshot_accept_hint <- function(variant, file, reset_output = TRUE) {\nsnapshot_not_available <- function(message) {\n  cli::cli_inform(c(\n    \"{.strong Can't compare snapshot to reference when testing interactively.}\",\n    \"{.code Run devtools::test() or testthat::test_file() to see changes.}\",\n    message\n  ))\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 79.86342, "model_code": " snapshot_accept_hint <- function(variant, file, reset_output = TRUE) { \n snapshot_not_available <- function(message) { \n cli::cli_inform(c( \n \"{.strong Can't compare snapshot to reference when testing interactively.}\", \n i = \"Run {.code devtools::test()} or `testthat::test_file()` to see changes.\", \n i = message \n )) \n }", "model_em_trim": 0, "model_bleu_trim": 90.90182052511557, "error_type_gpt": "Need Domain Knowledge"}
{"id": 23, "old": " import (\n )\n \n // StepDelay is used so that transactions can disseminate properly\n-var StepDelay = time.Second\n \n // SyncDelay is used so that nodes have enough time to sync\n-var SyncDelay = time.Second * 2\n \n // P2pBootstrapDelay is used so that nodes have enough time to bootstrap\n var P2pBootstrapDelay = 5 * time.Second", "new": " import (\n )\n \n // StepDelay is used so that transactions can disseminate properly\n+var StepDelay = time.Millisecond * 180\n \n // SyncDelay is used so that nodes have enough time to sync\n+var SyncDelay = time.Second / 5\n \n // P2pBootstrapDelay is used so that nodes have enough time to bootstrap\n var P2pBootstrapDelay = 5 * time.Second", "review": "this is a large increase\r\nI think we need to investigate where the increase comes from.", "dataset": "codereview_new", "original_id": 7863, "relevance": "partial", "relevance_user1": "not", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "no code", "gpt_em_trim": 0, "gpt_bleu_trim": 1.6e-26, "model_code": " import ( \n ) \n // P2pBootstrapDelay is used so that nodes have enough time to bootstrap \n var P2pBootstrapDelay = 5 * time.Second", "model_em_trim": 0, "model_bleu_trim": 19.956206188065167}
{"id": 24, "old": "\n import org.apache.pulsar.common.util.collections.TripleLongPriorityQueue;\n \n @NotThreadSafe\n-public class TripleLongPriorityDelayedIndexQueue implements DelayedIndexQueue {\n \n     private final TripleLongPriorityQueue queue;\n ", "new": "\n import org.apache.pulsar.common.util.collections.TripleLongPriorityQueue;\n \n @NotThreadSafe\n+class TripleLongPriorityDelayedIndexQueue implements DelayedIndexQueue {\n \n     private final TripleLongPriorityQueue queue;\n ", "review": "I notice that this class is used to make `peek` and `pop` easier to use, and it is only used by `MutableBucket`, should it not exist as a separate public class?", "dataset": "codereview_new", "original_id": 7322, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "no code", "gpt_em_trim": 0, "gpt_bleu_trim": 1.88e-10, "model_code": " import org.apache.pulsar.common.util.collections.TripleLongPriorityQueue; \n @NotThreadSafe \n public class TripleLongPriorityDelayedIndexQueue implements DelayedIndexQueue { \n private final TripleLongPriorityQueue queue; \n", "model_em_trim": 0, "model_bleu_trim": 90.58983423366499, "error_type_gpt": "Need Domain Knowledge"}
{"id": 25, "old": " def _find_module(self, id: str, use_typeshed: bool) -> ModuleSearchResult:\n             # In namespace mode, register a potential namespace package\n             if self.options and self.options.namespace_packages:\n                 if (\n-                    fscache.exists_case(base_path, dir_prefix)\n                     and not fscache.isfile_case(base_path, dir_prefix)\n-                    and not has_init\n                 ):\n                     near_misses.append((base_path, dir_prefix))\n ", "new": " def _find_module(self, id: str, use_typeshed: bool) -> ModuleSearchResult:\n             # In namespace mode, register a potential namespace package\n             if self.options and self.options.namespace_packages:\n                 if (\n+                    not has_init\n+                    and fscache.exists_case(base_path, dir_prefix)\n                     and not fscache.isfile_case(base_path, dir_prefix)\n                 ):\n                     near_misses.append((base_path, dir_prefix))\n ", "review": "```suggestion\r\n                    not has_init\r\n                    and fscache.exists_case(base_path, dir_prefix)\r\n                    and not fscache.isfile_case(base_path, dir_prefix)\r\n```", "dataset": "codereview_new", "original_id": 8711, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ndef _find_module(self, id: str, use_typeshed: bool) -> ModuleSearchResult:\n            # In namespace mode, register a potential namespace package\n            if self.options and self.options.namespace_packages:\n                if (\n                    not has_init\n                    and fscache.exists_case(base_path, dir_prefix)\n                    and not fscache.isfile_case(base_path, dir_prefix)\n                ):\n                    near_misses.append((base_path, dir_prefix))\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " def _find_module(self, id: str, use_typeshed: bool) -> ModuleSearchResult: \n # In namespace mode, register a potential namespace package \n if self.options and self.options.namespace_packages: \n if ( \n not has_init \n and fscache.exists_case(base_path, dir_prefix) \n ): \n near_misses.append((base_path, dir_prefix)) \n", "model_em_trim": 0, "model_bleu_trim": 82.84197620019054}
{"id": 26, "old": " public sealed class CosmosFhirDataStore : IFhirDataStore, IProvideCapability\n             _modelInfoProvider = modelInfoProvider;\n         }\n \n-        public async Task<UpsertOutcome> UpsertAsync(ResourceWrapperExtended resource, CancellationToken cancellationToken)\n         {\n             return await UpsertAsync(resource.Wrapper, resource.WeakETag, resource.AllowCreate, resource.KeepHistory, cancellationToken, resource.RequireETagOnUpdate);\n         }", "new": " public sealed class CosmosFhirDataStore : IFhirDataStore, IProvideCapability\n             _modelInfoProvider = modelInfoProvider;\n         }\n \n+        public async Task<UpsertOutcome> UpsertAsync(ResourceWrapperOperation resource, CancellationToken cancellationToken)\n         {\n             return await UpsertAsync(resource.Wrapper, resource.WeakETag, resource.AllowCreate, resource.KeepHistory, cancellationToken, resource.RequireETagOnUpdate);\n         }", "review": "Low priority: add checks to avoid null references of ResourceWrapperExtended. ", "dataset": "codereview_new", "original_id": 5593, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\npublic sealed class CosmosFhirDataStore : IFhirDataStore, IProvideCapability\n{\n    private readonly IModelInfoProvider _modelInfoProvider;\n\n    public CosmosFhirDataStore(IModelInfoProvider modelInfoProvider)\n    {\n        _modelInfoProvider = modelInfoProvider ?? throw new ArgumentNullException(nameof(modelInfoProvider));\n    }\n\n    public async Task<UpsertOutcome> UpsertAsync(ResourceWrapperExtended resource, CancellationToken cancellationToken)\n    {\n        if (resource == null)\n        {\n            throw new ArgumentNullException(nameof(resource));\n        }\n\n        return await UpsertAsync(resource.Wrapper, resource.WeakETag, resource.AllowCreate, resource.KeepHistory, cancellationToken, resource.RequireETagOnUpdate);\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 48.59285, "model_code": " public sealed class CosmosFhirDataStore : IFhirDataStore, IProvideCapability \n _modelInfoProvider = modelInfoProvider; \n }", "model_em_trim": 0, "model_bleu_trim": 5.688823834610152}
{"id": 27, "old": " public List<FieldInfo> fields() {\n      * Returns the description information of the struct.\n      */\n     @JsonProperty\n     @JsonInclude(Include.NON_NULL)\n     @Nullable\n     public DescriptionInfo descriptionInfo() {", "new": " public List<FieldInfo> fields() {\n      * Returns the description information of the struct.\n      */\n     @JsonProperty\n+    @Override\n     @JsonInclude(Include.NON_NULL)\n     @Nullable\n     public DescriptionInfo descriptionInfo() {", "review": "```suggestion\r\n    @JsonProperty\r\n    @Override\r\n```", "dataset": "codereview_new", "original_id": 8238, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\npublic List<FieldInfo> fields() {\n     * Returns the description information of the struct.\n     */\n    @Override\n    @JsonProperty\n    @JsonInclude(Include.NON_NULL)\n    @Nullable\n    public DescriptionInfo descriptionInfo() {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 94.05844, "model_code": " public List<FieldInfo> fields() { \n * Returns the description information of the struct. \n */ \n @JsonInclude(Include.NON_NULL) \n @Nullable \n public DescriptionInfo descriptionInfo() {", "model_em_trim": 0, "model_bleu_trim": 88.07195790845002, "error_type_gpt": "Code Style Difference"}
{"id": 28, "old": " class ReviewerCustomFonts(context: Context) {\n     private fun getDominantFontStyle(context: Context, customFontsMap: Map<String?, AnkiFont>): String? {\n         if (mDominantFontStyle == null) {\n             mDominantFontStyle = getOverrideFontStyle(context, customFontsMap)\n-            if (mDominantFontStyle?.isEmpty() == true) {\n                 mDominantFontStyle = getDefaultFontStyle(context, customFontsMap)\n-                if (mDominantFontStyle?.isEmpty() == true) {\n                     mDominantFontStyle = themeFontStyle\n                 }\n             }", "new": " class ReviewerCustomFonts(context: Context) {\n     private fun getDominantFontStyle(context: Context, customFontsMap: Map<String?, AnkiFont>): String? {\n         if (mDominantFontStyle == null) {\n             mDominantFontStyle = getOverrideFontStyle(context, customFontsMap)\n+            if (mDominantFontStyle.isNullOrEmpty()) {\n                 mDominantFontStyle = getDefaultFontStyle(context, customFontsMap)\n+                if (mDominantFontStyle.isNullOrEmpty()) {\n                     mDominantFontStyle = themeFontStyle\n                 }\n             }", "review": "```suggestion\r\n            if (mDominantFontStyle.isNullOrEmpty()) {\r\n```", "dataset": "codereview_new", "original_id": 1074, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass ReviewerCustomFonts(context: Context) {\n    private fun getDominantFontStyle(context: Context, customFontsMap: Map<String?, AnkiFont>): String? {\n        if (mDominantFontStyle.isNullOrEmpty()) {\n            mDominantFontStyle = getOverrideFontStyle(context, customFontsMap)\n            if (mDominantFontStyle.isNullOrEmpty()) {\n                mDominantFontStyle = getDefaultFontStyle(context, customFontsMap)\n                if (mDominantFontStyle.isNullOrEmpty()) {\n                    mDominantFontStyle = themeFontStyle\n                }\n            }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 92.85218, "model_code": " class ReviewerCustomFonts(context: Context) { \n private fun getDominantFontStyle(context: Context, customFontsMap: Map<String?, AnkiFont>): String? { \n if (mDominantFontStyle == null) { \n mDominantFontStyle = getOverrideFontStyle(context, customFontsMap) \n if (mDominantFontStyle.isNullOrEmpty()) { \n mDominantFontStyle = getDefaultFontStyle(context, customFontsMap) \n if (mDominantFontStyle.isEmpty() == true) { \n mDominantFontStyle = themeFontStyle \n } \n }", "model_em_trim": 0, "model_bleu_trim": 91.42453726348256, "error_type_gpt": "Code Style Difference"}
{"id": 29, "old": " class BooleanPropertyNaming(config: Config = Config.empty) : Rule(config) {\n         val description = \"Boolean property name should match a $allowedPattern pattern.\"\n         return CodeSmell(\n             issue,\n-            Entity.from(declaration.nameIdentifier ?: declaration),\n             message = \"$description Actual name is $name\"\n         )\n     }", "new": " class BooleanPropertyNaming(config: Config = Config.empty) : Rule(config) {\n         val description = \"Boolean property name should match a $allowedPattern pattern.\"\n         return CodeSmell(\n             issue,\n+            Entity.atName(declaration),\n             message = \"$description Actual name is $name\"\n         )\n     }", "review": "Could this be statically checked in the new ruleset as a best practice? e.g. if the \"thing\" passed into `Entity.from` has a \"name\" (e.g. inherits from `PsiNameIdentifierOwner`), then `report(\"use '.nameIdentifier' instead\")`?", "dataset": "codereview_new", "original_id": 1243, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass BooleanPropertyNaming(config: Config = Config.empty) : Rule(config) {\n        val description = \"Boolean property name should match a $allowedPattern pattern.\"\n        val entity = declaration.nameIdentifier ?: declaration\n        val name = if (entity is PsiNameIdentifierOwner) entity.nameIdentifier?.text ?: entity.name else entity.toString()\n        return CodeSmell(\n            issue,\n            Entity.from(entity),\n            message = \"$description Actual name is $name\"\n        )\n    }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 55.47169, "model_code": " class BooleanPropertyNaming(config: Config = Config.empty) : Rule(config) { \n val description = \"Boolean property name should match a $allowedPattern pattern.\" \n return CodeSmell( \n issue, \n Entity.from(declaration.nameIdentifier), \n message = \"$description Actual name is $name\" \n ) \n }", "model_em_trim": 0, "model_bleu_trim": 90.01922975695913, "error_type_gpt": "Need Domain Knowledge"}
{"id": 30, "old": " def can_render(self):\n \t\treturn True\n \n \tdef render(self):\n-\t\taction = \"/login?redirect-to={}\".format(frappe.request.path)\n \t\tfrappe.local.message_title = _(\"Not Permitted\")\n \t\tfrappe.local.response[\"context\"] = dict(\n \t\t\tindicator_color=\"red\", primary_action=action, primary_label=_(\"Login\"), fullpage=True", "new": " def can_render(self):\n \t\treturn True\n \n \tdef render(self):\n+\t\taction = f\"/login?redirect-to={frappe.request.path}\"\n \t\tfrappe.local.message_title = _(\"Not Permitted\")\n \t\tfrappe.local.response[\"context\"] = dict(\n \t\t\tindicator_color=\"red\", primary_action=action, primary_label=_(\"Login\"), fullpage=True", "review": "```suggestion\r\n\t\taction = f\"/login?redirect-to={frappe.request.path}\"\r\n```", "dataset": "codereview_new", "original_id": 8810, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ndef can_render(self):\n\t\treturn True\n\n\tdef render(self):\n\t\taction = f\"/login?redirect-to={frappe.request.path}\"\n\t\tfrappe.local.message_title = _(\"Not Permitted\")\n\t\tfrappe.local.response[\"context\"] = dict(\n\t\t\tindicator_color=\"red\", primary_action=action, primary_label=_(\"Login\"), fullpage=True\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " def can_render(self): \n return True", "model_em_trim": 0, "model_bleu_trim": 0.06338892117716576}
{"id": 31, "old": " public override bool TryDetermineReturnType(SyntaxToken startToken, SemanticMode\n             while (!token.IsKind(SyntaxKind.None))\n             {\n                 if (overrideToken != default &&\n-                    overrideToken.Parent is not null &&\n-                    token.SpanStart < overrideToken.Parent.SpanStart)\n                 {\n                     break;\n                 }", "new": " public override bool TryDetermineReturnType(SyntaxToken startToken, SemanticMode\n             while (!token.IsKind(SyntaxKind.None))\n             {\n                 if (overrideToken != default &&\n+                    token.SpanStart < overrideToken.GetRequiredParent().SpanStart)\n                 {\n                     break;\n                 }", "review": "```suggestion\r\n                    token.SpanStart < overrideToken.GetRequiredParent().SpanStart)\r\n```", "dataset": "codereview_new", "original_id": 7555, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\npublic override bool TryDetermineReturnType(SyntaxToken startToken, SemanticModel semanticModel, CancellationToken cancellationToken, out TypeSyntax returnType)\n{\n    returnType = null;\n    var token = startToken;\n    var overrideToken = default(SyntaxToken);\n\n    while (!token.IsKind(SyntaxKind.None))\n    {\n        if (overrideToken != default &&\n            overrideToken.Parent is not null &&\n            token.SpanStart < overrideToken.GetRequiredParent().SpanStart)\n        {\n            break;\n        }\n\n        if (token.IsKind(SyntaxKind.OverrideKeyword))\n        {\n            overrideToken = token;\n        }\n        else if (token.IsKind(SyntaxKind.IdentifierToken))\n        {\n            var symbol = semanticModel.GetSymbolInfo(token, cancellationToken).Symbol;\n            if (symbol is not null && symbol.Kind == SymbolKind.Method)\n            {\n                returnType = ((IMethodSymbol)symbol).ReturnType.GenerateTypeSyntax();\n                break;\n            }\n        }\n\n        token = token.GetPreviousToken();\n    }\n\n    return returnType is not null;\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 79.57351, "model_code": " public override bool TryDetermineReturnType(SyntaxToken startToken, SemanticMode \n while (!token.IsKind(SyntaxKind.None)) \n { \n if (overrideToken != default) \n { \n break; \n }", "model_em_trim": 0, "model_bleu_trim": 64.94521226813102, "error_type_gpt": "Model fallacy"}
{"id": 32, "old": " static void verifyResponseBufs() {\n \n     @Test\n     void shouldReturnEmptyBodyOnHead() throws Exception {\n-        final BlockingWebClient client = WebClient.of(server.httpUri()).blocking();\n         final AggregatedHttpResponse res = client.head(\"/hello\");\n         assertThat(res.headers().contentLength()).isEqualTo(5);\n         assertThat(res.contentUtf8()).isEmpty();", "new": " static void verifyResponseBufs() {\n \n     @Test\n     void shouldReturnEmptyBodyOnHead() throws Exception {\n+        final BlockingWebClient client = server.blockingWebClient();\n         final AggregatedHttpResponse res = client.head(\"/hello\");\n         assertThat(res.headers().contentLength()).isEqualTo(5);\n         assertThat(res.contentUtf8()).isEmpty();", "review": "nit:\r\n```suggestion\r\n        final BlockingWebClient client = server.blockingWebClient();\r\n```", "dataset": "codereview_new", "original_id": 8167, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nstatic void verifyResponseBufs() {\n\n    @Test\n    void shouldReturnEmptyBodyOnHead() throws Exception {\n        final BlockingWebClient client = server.blockingWebClient();\n        final AggregatedHttpResponse res = client.head(\"/hello\");\n        assertThat(res.headers().contentLength()).isEqualTo(5);\n        assertThat(res.contentUtf8()).isEmpty();\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " static void verifyResponseBufs() { \n @Test \n void shouldReturnEmptyBodyOnHead() throws Exception { \n final BlockingWebClient client = server.blockingWebClient(); \n final AggregatedHttpResponse res = client.head(\"/hello\"); \n assertThat(res.headers().contentLength()).isEqualTo(5); \n assertThat(res.contentUtf8()).isEmpty();", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 33, "old": " class CallingActionsView: UIView {\n     private func setupViews() {\n         backgroundColor = .clear\n         topStackView.distribution = .equalSpacing\n-\n-//        topStackView.spacing = 16\n         verticalStackView.alignment = .fill\n         verticalStackView.spacing = 10\n         verticalStackView.isLayoutMarginsRelativeArrangement = true", "new": " class CallingActionsView: UIView {\n     private func setupViews() {\n         backgroundColor = .clear\n         topStackView.distribution = .equalSpacing\n         verticalStackView.alignment = .fill\n         verticalStackView.spacing = 10\n         verticalStackView.isLayoutMarginsRelativeArrangement = true", "review": "Do we need that spacing after all?", "dataset": "codereview_new", "original_id": 500, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-remove", "category_type_user1": "Documentation-remove", "category_type_user2": "Documentation-remove", "gpt_code": "\nclass CallingActionsView: UIView {\n    private func setupViews() {\n        backgroundColor = .clear\n        topStackView.distribution = .equalSpacing\n\n        verticalStackView.alignment = .fill\n        verticalStackView.spacing = 10\n        verticalStackView.isLayoutMarginsRelativeArrangement = true\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " class CallingActionsView: UIView { \n private func setupViews() { \n backgroundColor = .clear \n topStackView.distribution = .equalSpacing \n verticalStackView.alignment = .fill \n verticalStackView.spacing = 10 \n verticalStackView.isLayoutMarginsRelativeArrangement = true", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 34, "old": " data class Entity(\n     var title: String,\n     var url: String?,\n     var type: EntityType,\n-) {\n-    companion object {\n-        fun generateFolderId(index: Long): String {\n-            return \"folder$index\"\n-        }\n-\n-        fun generateFolderId(index: String): String {\n-            return \"folder$index\"\n-        }\n-\n-        fun generateFavoriteId(index: Long): String {\n-            return \"favorite$index\"\n-        }\n-\n-        fun generateBookmarkId(index: Long): String {\n-            return \"bookmark$index\"\n-        }\n-    }\n-}\n \n enum class EntityType {\n     BOOKMARK,", "new": " data class Entity(\n     var title: String,\n     var url: String?,\n     var type: EntityType,\n+)\n \n enum class EntityType {\n     BOOKMARK,", "review": "not fully understand what's this for.", "dataset": "codereview_new", "original_id": 2005, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\ndata class Entity(\n    var title: String,\n    var url: String?,\n    var type: EntityType\n) {\n    companion object {\n        fun generateFolderId(index: Long): String {\n            return \"folder$index\"\n        }\n\n        fun generateFavoriteId(index: Long): String {\n            return \"favorite$index\"\n        }\n\n        fun generateBookmarkId(index: Long): String {\n            return \"bookmark$index\"\n        }\n    }\n}\n\nenum class EntityType {\n    BOOKMARK\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 47.63559, "model_code": " data class Entity( \n var title: String, \n var url: String?, \n var type: EntityType) { \n enum class EntityType { \n BOOKMARK,", "model_em_trim": 0, "model_bleu_trim": 87.24671728062899, "error_type_gpt": "Need Domain Knowledge"}
{"id": 35, "old": " static int compiler_addcompare(struct compiler *c, location loc,\n     default:\n         Py_UNREACHABLE();\n     }\n-    /* cmp goes in top bits of the oparg, low bits are for the mask. */\n     ADDOP_I(c, loc, COMPARE_OP, cmp << 4);\n     return SUCCESS;\n }", "new": " static int compiler_addcompare(struct compiler *c, location loc,\n     default:\n         Py_UNREACHABLE();\n     }\n+    /* cmp goes in top bits of the oparg, while the low bits are used by specialized\n+     * versions of this opcode to store the comparison mask. */\n     ADDOP_I(c, loc, COMPARE_OP, cmp << 4);\n     return SUCCESS;\n }", "review": "```suggestion\r\n    /* cmp goes in top bits of the oparg, while the low bits are used by specialized\r\n     * versions of this opcode to store the comparison mask. */\r\n```", "dataset": "codereview_new", "original_id": 9380, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nstatic int compiler_addcompare(struct compiler *c, location loc,\n    default:\n        Py_UNREACHABLE();\n    }\n    /* cmp goes in top bits of the oparg, while the low bits are used by specialized\n     * versions of this opcode to store the comparison mask. */\n    ADDOP_I(c, loc, COMPARE_OP, (cmp << 4));\n    return SUCCESS;\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 94.50862, "model_code": " static int compiler_addcompare(struct compiler *c, location loc, \n default: \n Py_UNREACHABLE(); \n return SUCCESS; \n }", "model_em_trim": 0, "model_bleu_trim": 14.963911368619232, "error_type_gpt": "Code Style Difference"}
{"id": 36, "old": " class SlidingWindowAggIntegTest extends FeathrIntegTest {\n       new GenericRowWithSchema(Array(2, 5, 2), expectedSchema),\n       new GenericRowWithSchema(Array(3, 1, 1), expectedSchema))\n     val dfs = runLocalFeatureJoinForTest(featureJoinAsString, featureDefAsString, \"featuresWithFilterObs.avro.json\").data\n-    dfs.show()\n \n     validateRows(dfs.select(keyField, features: _*).collect().sortBy(row => row.getAs[Int](keyField)), expectedRows)\n   }", "new": " class SlidingWindowAggIntegTest extends FeathrIntegTest {\n       new GenericRowWithSchema(Array(2, 5, 2), expectedSchema),\n       new GenericRowWithSchema(Array(3, 1, 1), expectedSchema))\n     val dfs = runLocalFeatureJoinForTest(featureJoinAsString, featureDefAsString, \"featuresWithFilterObs.avro.json\").data\n \n     validateRows(dfs.select(keyField, features: _*).collect().sortBy(row => row.getAs[Int](keyField)), expectedRows)\n   }", "review": "nit: delete it.", "dataset": "codereview_new", "original_id": 3876, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nnew GenericRowWithSchema(Array(2, 5, 2), expectedSchema),\nnew GenericRowWithSchema(Array(3, 1, 1), expectedSchema))\nval dfs = runLocalFeatureJoinForTest(featureJoinAsString, featureDefAsString, \"featuresWithFilterObs.avro.json\").data\ndfs.show()\n\nvalidateRows(dfs.select(keyField, features: _*).collect().sortBy(row => row.getAs[Int](keyField)), expectedRows)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 91.68296, "model_code": " class SlidingWindowAggIntegTest extends FeathrIntegTest { \n new GenericRowWithSchema(Array(2, 5, 2), expectedSchema), \n new GenericRowWithSchema(Array(3, 1, 1), expectedSchema)) \n validateRows(dfs.select(keyField, features: _*).collect().sortBy(row => row.getAs[Int](keyField)), expectedRows) \n }", "model_em_trim": 0, "model_bleu_trim": 75.43758714984867, "error_type_gpt": "Unclear Location"}
{"id": 37, "old": " class MaestroDriverService {\n         while (!Thread.interrupted()) {\n             Thread.sleep(100)\n         }\n-        ToastAccessibilityListener.stop()\n     }\n \n }", "new": " class MaestroDriverService {\n         while (!Thread.interrupted()) {\n             Thread.sleep(100)\n         }\n     }\n \n }", "review": "What could be best place to stop this listener? I observed this line is not called. Any other better options to stop the listener?", "dataset": "codereview_new", "original_id": 1748, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nclass MaestroDriverService {\n        private boolean isRunning = true;\n\n        public void stopService() {\n            isRunning = false;\n            ToastAccessibilityListener.stop();\n        }\n\n        public void startService() {\n            while (isRunning) {\n                try {\n                    Thread.sleep(100);\n                } catch (InterruptedException e) {\n                    // Handle exception\n                }\n            }\n        }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 31.3737, "model_code": " class MaestroDriverService { \n ToastAccessibilityListener.stop() \n } \n }", "model_em_trim": 0, "model_bleu_trim": 12.632278750496795, "error_type_gpt": "Unclear Location"}
{"id": 38, "old": " INSERT INTO public.\"role\" (name, description, priv_level)\n VALUES('trouter', 'Limited role for Traffic Router calls to Traffic Ops', 10);\n \n INSERT INTO public.role_capability (role_id, cap_name)\n-    VALUES ((select id from role where name='trouter'), UNNEST('{CDN:READ, DELIVERY-SERVICE:READ, DNS-SEC:READ, FEDERATION:READ, STEERING:READ, FEDERATION-RESOLVER:READ, DS-SECURITY-KEY:READ}'::text[]))\n-        ON CONFLICT DO NOTHING;\n\\ No newline at end of file\n\\ No newline at end of file", "new": " INSERT INTO public.\"role\" (name, description, priv_level)\n VALUES('trouter', 'Limited role for Traffic Router calls to Traffic Ops', 10);\n \n INSERT INTO public.role_capability (role_id, cap_name)\n\\ No newline at end of file\n+    VALUES (\n+        (SELECT id FROM role WHERE name='trouter'),\n+        UNNEST(ARRAY[\n+            'CDN:READ',\n+            'DELIVERY-SERVICE:READ',\n+            'DNS-SEC:READ',\n+            'FEDERATION:READ',\n+            'STEERING:READ',\n+            'FEDERATION-RESOLVER:READ',\n+            'DS-SECURITY-KEY:READ']\n+        )\n+    );\n\\ No newline at end of file", "review": "In order to be POSIX-compliant, every file should end with a newline.", "dataset": "codereview_new", "original_id": 9517, "relevance": "partial", "relevance_user1": "perfect", "relevance_user2": "not", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nINSERT INTO public.\"role\" (name, description, priv_level)\nVALUES('trouter', 'Limited role for Traffic Router calls to Traffic Ops', 10);\n\nINSERT INTO public.role_capability (role_id, cap_name)\n    VALUES ((select id from role where name='trouter'), UNNEST('{CDN:READ, DELIVERY-SERVICE:READ, DNS-SEC:READ, FEDERATION:READ, STEERING:READ, FEDERATION-RESOLVER:READ, DS-SECURITY-KEY:READ}'::text[]))\n        ON CONFLICT DO NOTHING;\n\n", "gpt_em_trim": 0, "gpt_bleu_trim": 65.53908, "model_code": " INSERT INTO public.\"role\" (name, description, priv_level) \n VALUES('trouter', 'Limited role for Traffic Router calls to Traffic Ops', 10); \n } \n No newline at end of file", "model_em_trim": 0, "model_bleu_trim": 11.399627294235938}
{"id": 39, "old": " import org.assertj.core.api.Assertions.assertThat\n import org.junit.jupiter.api.Nested\n import org.junit.jupiter.api.Test\n \n-private const val THRESHOLD = 4\n-private val defaultConfig = \"threshold\" to THRESHOLD\n-private val staticDeclarationsConfig = TestConfig(defaultConfig, \"includeStaticDeclarations\" to true)\n-private val privateDeclarationsConfig = TestConfig(defaultConfig, \"includePrivateDeclarations\" to true)\n-private val ignoreOverloadedConfig = TestConfig(defaultConfig, \"ignoreOverloaded\" to true)\n \n class ComplexInterfaceSpec {\n \n-    private val subject = ComplexInterface(TestConfig(defaultConfig))\n \n     @Nested\n     inner class `ComplexInterface rule positives` {", "new": " import org.assertj.core.api.Assertions.assertThat\n import org.junit.jupiter.api.Nested\n import org.junit.jupiter.api.Test\n \n+private val defaultThreshold = \"threshold\" to 4\n+private val staticDeclarationsConfig = TestConfig(defaultThreshold, \"includeStaticDeclarations\" to true)\n+private val privateDeclarationsConfig = TestConfig(defaultThreshold, \"includePrivateDeclarations\" to true)\n+private val ignoreOverloadedConfig = TestConfig(defaultThreshold, \"ignoreOverloaded\" to true)\n \n class ComplexInterfaceSpec {\n \n+    private val subject = ComplexInterface(TestConfig(defaultThreshold))\n \n     @Nested\n     inner class `ComplexInterface rule positives` {", "review": "Unexpected spacing before \",\"\n\n[Show more details](https://github.com/detekt/detekt/security/code-scanning/7115)", "dataset": "codereview_new", "original_id": 1203, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nimport org.assertj.core.api.Assertions.assertThat\nimport org.junit.jupiter.api.Nested\nimport org.junit.jupiter.api.Test\n\nprivate const val THRESHOLD = 4\nprivate val defaultConfig = \"threshold\" to THRESHOLD\nprivate val staticDeclarationsConfig = TestConfig(defaultConfig, \"includeStaticDeclarations\" to true)\nprivate val privateDeclarationsConfig = TestConfig(defaultConfig, \"includePrivateDeclarations\" to true)\nprivate val ignoreOverloadedConfig = TestConfig(defaultConfig, \"ignoreOverloaded\" to true)\n\nclass ComplexInterfaceSpec {\n\n    private val subject = ComplexInterface(TestConfig(defaultConfig))\n\n    @Nested\n    inner class `ComplexInterface rule positives` {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 81.17766, "model_code": " import org.assertj.core.api.Assertions.assertThat \n import org.junit.jupiter.api.Nested \n import org.junit.jupiter.api.Test \n class ComplexInterfaceSpec { \n private val subject = ComplexInterface(TestConfig(DEFAULT_CONFIG)) \n @Nested \n inner class `ComplexInterface rule positives` {", "model_em_trim": 0, "model_bleu_trim": 38.95037472651453}
{"id": 40, "old": " class TmdbShowDetailToTiviShow : Mapper<TmdbShowDetail, TiviShow> {\n         summary = from.overview,\n         homepage = from.homepage,\n         network = from.networks.firstOrNull()?.name,\n-        networkLogoPath = null, // FIXME from.networks.firstOrNull()?.logo_path,\n     )\n }", "new": " class TmdbShowDetailToTiviShow : Mapper<TmdbShowDetail, TiviShow> {\n         summary = from.overview,\n         homepage = from.homepage,\n         network = from.networks.firstOrNull()?.name,\n+        networkLogoPath = from.networks.firstOrNull()?.logoPath,\n     )\n }", "review": "Fixed upstream: https://github.com/MoviebaseApp/tmdb-api/pull/16", "dataset": "codereview_new", "original_id": 1133, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-and-code", "category_type_user1": "Feature-modify", "category_type_user2": "Documentation-and-code", "gpt_code": "\nclass TmdbShowDetailToTiviShow : Mapper<TmdbShowDetail, TiviShow> {\n        summary = from.overview,\n        homepage = from.homepage,\n        network = from.networks.firstOrNull()?.name,\n        networkLogoPath = from.networks.firstOrNull()?.logo_path ?: null,\n    )\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 86.11191, "model_code": " class TmdbShowDetailToTiviShow : Mapper<TmdbShowDetail, TiviShow> { \n summary = from.overview, \n homepage = from.homepage, \n network = from.networks.first().name, \n networkLogoPath = null, \n ) \n }", "model_em_trim": 0, "model_bleu_trim": 65.45236017320933}
{"id": 41, "old": " modify_in <- function(.x, .where, .f, ...) {\n assign_in <- function(x, where, value) {\n   n <- length(where)\n   if (n == 0) {\n-    cli::cli_abort(\"{.arg where} must contain at least one element\")\n   } else if (n > 1) {\n     old <- pluck(x, where[[1]], .default = list())\n     if (!is_zap(value) || !identical(old, list())) {", "new": " modify_in <- function(.x, .where, .f, ...) {\n assign_in <- function(x, where, value) {\n   n <- length(where)\n   if (n == 0) {\n+    cli::cli_abort(\"{.arg where} must contain at least one element.\", arg = \"where\")\n   } else if (n > 1) {\n     old <- pluck(x, where[[1]], .default = list())\n     if (!is_zap(value) || !identical(old, list())) {", "review": "```suggestion\r\n    cli::cli_abort(\"{.arg where} must contain at least one element.\", arg = \"where\")\r\n```", "dataset": "codereview_new", "original_id": 2302, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nmodify_in <- function(.x, .where, .f, ...) {\n  assign_in <- function(x, where, value) {\n    n <- length(where)\n    if (n == 0) {\n      cli::cli_abort(\"{.arg} must contain at least one element.\", arg = \"where\")\n    } else if (n > 1) {\n      old <- pluck(x, where[[1]], .default = list())\n      if (!is_zap(value) || !identical(old, list())) {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 98.06386, "model_code": " modify_in <- function(.x, .where, .f, ...) { \n assign_in <- function(x, where, value) { \n n <- length(where) \n if (n == 0) { \n cli::cli_abort(\"{.arg where} must contain at least one element.\", arg = \"where\") \n } else if (n > 1) { \n old <- pluck(x, where[[1]], .default = list()) \n if (!is_zap(value) || !identical(old, list())) {", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Model fallacy"}
{"id": 42, "old": " public static function setupBeforeClass(): void\n     {\n         parent::setupBeforeClass();\n         \\Yii::import('application.core.plugins.dateFunctions.EMFunctions', true);\n-        self::importSurvey(\\Yii::app()->getConfig('rootdir') . '/tests/unit/plugins/surveys/limesurvey_survey_854771.lss');\n     }\n \n     /**", "new": " public static function setupBeforeClass(): void\n     {\n         parent::setupBeforeClass();\n         \\Yii::import('application.core.plugins.dateFunctions.EMFunctions', true);\n+        self::importSurvey(self::$surveysFolder . '/limesurvey_survey_854771.lss');\n+        \\Yii::app()->session['LEMsid'] = self::$surveyId;\n     }\n \n     /**", "review": "Import ? Not activate https://github.com/LimeSurvey/LimeSurvey/blob/4e5e6d2f3e2c17ce4bfdae9d86e4c9ef5eacc72a/tests/TestBaseClass.php#L147 ?", "dataset": "codereview_new", "original_id": 6356, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\npublic static function setupBeforeClass(): void\n{\n    parent::setupBeforeClass();\n    require_once __DIR__ . '/../../../../../application/core/plugins/dateFunctions/EMFunctions.php';\n    self::importSurvey(\\Yii::app()->getConfig('rootdir') . '/tests/unit/plugins/surveys/limesurvey_survey_854771.lss');\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 42.9765, "model_code": " public static function setupBeforeClass(): void \n { \n parent::setupBeforeClass(); \n } \n /**", "model_em_trim": 0, "model_bleu_trim": 4.134434165719679}
{"id": 43, "old": " func (handler *keysHandler) GetAssociatedPid(pkBytes []byte) core.PeerID {\n \n // IsOriginalPublicKeyOfTheNode returns true if the provided public key bytes are the original ones used by the node\n func (handler *keysHandler) IsOriginalPublicKeyOfTheNode(pkBytes []byte) bool {\n-\treturn false\n }\n \n // UpdatePublicKeyLiveness update the provided public key liveness if the provided pid is not managed by the current node", "new": " func (handler *keysHandler) GetAssociatedPid(pkBytes []byte) core.PeerID {\n \n // IsOriginalPublicKeyOfTheNode returns true if the provided public key bytes are the original ones used by the node\n func (handler *keysHandler) IsOriginalPublicKeyOfTheNode(pkBytes []byte) bool {\n+\treturn true\n }\n \n // UpdatePublicKeyLiveness update the provided public key liveness if the provided pid is not managed by the current node", "review": "is not managed? or is managed?", "dataset": "codereview_new", "original_id": 7865, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfunc (handler *keysHandler) GetAssociatedPid(pkBytes []byte) core.PeerID {\n\n// IsOriginalPublicKeyOfTheNode returns true if the provided public key bytes are the original ones used by the node\nfunc (handler *keysHandler) IsOriginalPublicKeyOfTheNode(pkBytes []byte) bool {\n\treturn false\n}\n\n// UpdatePublicKeyLiveness updates the provided public key liveness if the provided pid is managed by the current node, otherwise it returns an error\nfunc (handler *keysHandler) UpdatePublicKeyLiveness(pid core.PeerID, liveness int) error {\n\tif handler.isManaged(pid) {\n\t\thandler.publicKeys[pid].liveness = liveness\n\t\treturn nil\n\t}\n\treturn fmt.Errorf(\"provided pid is not managed by the current node\")\n}\n\n// isManaged returns true if the provided pid is managed by the current node\nfunc (handler *keysHandler) isManaged(pid core.PeerID) bool {\n\t_, ok := handler.publicKeys[pid]\n\treturn ok\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 83.86191, "model_code": " func (handler *keysHandler) GetAssociatedPid(pkBytes []byte) core.PeerID { \n // UpdatePublicKeyLiveness update the provided public key liveness if the provided pid is not managed by the current node", "model_em_trim": 0, "model_bleu_trim": 37.49618112947965}
{"id": 44, "old": " trait SCollectionMatchers extends EqInstances {\n    * SCollection assertion only applied to the specified window, running the checker only on the\n    * early pane for each key.\n    */\n-  def inEarlyPane[T: ClassTag, B: ClassTag](\n     window: BoundedWindow\n   )(matcher: MatcherBuilder[T]): Matcher[T] =\n     matcher match {", "new": " trait SCollectionMatchers extends EqInstances {\n    * SCollection assertion only applied to the specified window, running the checker only on the\n    * early pane for each key.\n    */\n+  def inEarlyPane[T](\n     window: BoundedWindow\n   )(matcher: MatcherBuilder[T]): Matcher[T] =\n     matcher match {", "review": "I believe we can simplify this with following:\r\n```suggestion\r\n  def inEarlyPane[T](\r\n```", "dataset": "codereview_new", "original_id": 3329, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ntrait SCollectionMatchers extends EqInstances {\n   * SCollection assertion only applied to the specified window, running the checker only on the\n   * early pane for each key.\n   */\n  def inEarlyPane[T: ClassTag](\n    window: BoundedWindow\n  )(matcher: MatcherBuilder[T]): Matcher[T] =\n    matcher match {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 93.94275, "model_code": " trait SCollectionMatchers extends EqInstances { \n * SCollection assertion only applied to the specified window, running the checker only on the \n * early pane for each key. \n * \n * @param T: ClassTag \n * @param B: ClassTag \n */ \n def inEarlyPane[T]( \n window: BoundedWindow \n )(matcher: MatcherBuilder[T]): Matcher[T] = \n matcher match {", "model_em_trim": 0, "model_bleu_trim": 80.11402409898338, "error_type_gpt": "Model fallacy"}
{"id": 45, "old": " default Validator<V> getDefaultValidator() {\n      *      *&#47;\n      *     &#64;Override\n      *     public Validator getDefaultValidator() {\n-     *          return (value, valueContext) -> clientSideValid ? ValidationResult.ok()\n      *                 : ValidationResult.error(\"Invalid date format\");\n      *     }\n      *", "new": " default Validator<V> getDefaultValidator() {\n      *      *&#47;\n      *     &#64;Override\n      *     public Validator getDefaultValidator() {\n+     *          return (value, valueContext) -&gt; clientSideValid ? ValidationResult.ok()\n      *                 : ValidationResult.error(\"Invalid date format\");\n      *     }\n      *", "review": "```suggestion\r\n     *          return (value, valueContext) -&gt; clientSideValid ? ValidationResult.ok()\r\n```", "dataset": "codereview_new", "original_id": 9872, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ndefault Validator<V> getDefaultValidator() {\n    @Override\n    public Validator getDefaultValidator() {\n        return (value, valueContext) -> {\n            if (clientSideValid) {\n                return ValidationResult.ok();\n            } else {\n                return ValidationResult.error(\"Invalid date format\");\n            }\n        };\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 51.27404, "model_code": " default Validator<V> getDefaultValidator() { \n *      *&#47; \n *     &#64;Override \n *     public Validator getDefaultValidator() { \n *          return (value, valueContext) -&gt; clientSideValid ? ValidationResult.ok() : ValidationResult.error(\"Invalid date format\"); \n *     } \n *", "model_em_trim": 0, "model_bleu_trim": 95.9077701463126, "error_type_gpt": "Model fallacy"}
{"id": 46, "old": " class Release extends mbEntity.Release {\n     this.packagingID = ko.observable(data.packagingID);\n     this.barcode = new Barcode(data.barcode);\n     this.comment = ko.observable(data.comment);\n-    const annotationText = data.latest_annotation?.text\n-      ? data.latest_annotation.text\n-      : '';\n     this.annotation = ko.observable(annotationText);\n     this.annotation.original = ko.observable(annotationText);\n ", "new": " class Release extends mbEntity.Release {\n     this.packagingID = ko.observable(data.packagingID);\n     this.barcode = new Barcode(data.barcode);\n     this.comment = ko.observable(data.comment);\n+    const annotationText = data.latest_annotation?.text ?? '';\n     this.annotation = ko.observable(annotationText);\n     this.annotation.original = ko.observable(annotationText);\n ", "review": "```suggestion\r\n    const annotationText = data.latest_annotation?.text ?? '';\r\n```\r\nCould be shortened a bit since this will always be a string, null or undefined. Thanks for finding and fixing this, by the way. (I should really try to set up my own local development server soon, just need to find a partition with enough free space and a bit of time...)", "dataset": "codereview_new", "original_id": 2990, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass Release extends mbEntity.Release {\n    this.packagingID = ko.observable(data.packagingID);\n    this.barcode = new Barcode(data.barcode);\n    this.comment = ko.observable(data.comment);\n    this.annotation = ko.observable(data.latest_annotation?.text ?? '');\n    this.annotation.original = ko.observable(data.latest_annotation?.text ?? '');\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 81.00552, "model_code": " class Release extends mbEntity.Release { \n this.packagingID = ko.observable(data.packagingID); \n this.barcode = new Barcode(data.barcode); \n this.comment = ko.observable(data.comment); \n this.annotation = ko.observable(data.latest_annotation?.text ?? ''); \n this.annotation.original = ko.observable(annotationText); \n", "model_em_trim": 0, "model_bleu_trim": 89.45299152635387, "error_type_gpt": "Unclear Location"}
{"id": 47, "old": " def type_check_second_pass(self) -> bool:\n         self.time_spent_us += time_spent_us(t0)\n         return result\n \n-    def detect_partially_defined_vars(self, type_map: Dict[Expression, Type]) -> None:\n         assert self.tree is not None, \"Internal error: method must be called on parsed file only\"\n         manager = self.manager\n         if manager.errors.is_error_code_enabled(codes.PARTIALLY_DEFINED):", "new": " def type_check_second_pass(self) -> bool:\n         self.time_spent_us += time_spent_us(t0)\n         return result\n \n+    def detect_partially_defined_vars(self, type_map: dict[Expression, Type]) -> None:\n         assert self.tree is not None, \"Internal error: method must be called on parsed file only\"\n         manager = self.manager\n         if manager.errors.is_error_code_enabled(codes.PARTIALLY_DEFINED):", "review": "```suggestion\r\n    def detect_partially_defined_vars(self, type_map: dict[Expression, Type]) -> None:\r\n```", "dataset": "codereview_new", "original_id": 8722, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\ndef detect_partially_defined_vars(self, type_map: dict[Expression, Type]) -> None:\n    assert self.tree is not None, \"Internal error: method must be called on parsed file only\"\n    manager = self.manager\n    if manager.errors.is_error_code_enabled(codes.PARTIALLY_DEFINED):\n", "gpt_em_trim": 0, "gpt_bleu_trim": 62.7662, "model_code": " def type_check_second_pass(self) -> bool: \n self.time_spent_us += time_spent_us(t0) \n return result", "model_em_trim": 0, "model_bleu_trim": 12.781864481060753, "error_type_gpt": "Insignificant Omission"}
{"id": 48, "old": " class Compilers(\n           pc.semanticTokens(vFile, capableTypes.asJava, capableModifiers.asJava)\n             .asScala\n             .map { plist =>\n-              // Thread.sleep(5000) // for debug\n-              scribe.info(\"Result from token : \" + plist.size().toString())\n               new SemanticTokens(plist)\n             }\n         }", "new": " class Compilers(\n           pc.semanticTokens(vFile, capableTypes.asJava, capableModifiers.asJava)\n             .asScala\n             .map { plist =>\n               new SemanticTokens(plist)\n             }\n         }", "review": "```suggestion\r\n```\r\nBefore merge.", "dataset": "codereview_new", "original_id": 3467, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nclass Compilers(\n          pc.semanticTokens(vFile, capableTypes.asJava, capableModifiers.asJava)\n            .asScala\n            .map { plist =>\n              scribe.info(\"Result from token : \" + plist.size().toString())\n              new SemanticTokens(plist)\n            }\n        }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 56.82469, "model_code": " class Compilers( \n pc.semanticTokens(vFile, capableTypes.asJava, capableModifiers.asJava) \n .asScala \n .map { plist => new SemanticTokens(plist) } \n }", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 49, "old": " var RootCmd = &cobra.Command{\n \t\t\treturn err\n \t\t}\n \n \t\tif cfg.DBURLNext != \"\" {\n \t\t\terr = doMigrations(cfg.DBURLNext)\n \t\t\tif err != nil {\n \t\t\t\treturn errors.Wrap(err, \"nextdb\")\n \t\t\t}\n-\t\t}\n \n-\t\tvar db *sql.DB\n-\t\tif cfg.DBURLNext != \"\" {\n \t\t\tmgr, err := swo.NewManager(swo.Config{OldDBURL: cfg.DBURL, NewDBURL: cfg.DBURLNext, CanExec: !cfg.APIOnly, Logger: cfg.Logger})\n \t\t\tif err != nil {\n \t\t\t\treturn errors.Wrap(err, \"init switchover handler\")", "new": " var RootCmd = &cobra.Command{\n \t\t\treturn err\n \t\t}\n \n+\t\tvar db *sql.DB\n \t\tif cfg.DBURLNext != \"\" {\n \t\t\terr = doMigrations(cfg.DBURLNext)\n \t\t\tif err != nil {\n \t\t\t\treturn errors.Wrap(err, \"nextdb\")\n \t\t\t}\n \n \t\t\tmgr, err := swo.NewManager(swo.Config{OldDBURL: cfg.DBURL, NewDBURL: cfg.DBURLNext, CanExec: !cfg.APIOnly, Logger: cfg.Logger})\n \t\t\tif err != nil {\n \t\t\t\treturn errors.Wrap(err, \"init switchover handler\")", "review": "can these 2 if statements be combined since they are doing the same check?", "dataset": "codereview_new", "original_id": 4764, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nvar RootCmd = &cobra.Command{\n    // ...\n    if cfg.DBURLNext != \"\" {\n        err = doMigrations(cfg.DBURLNext)\n        if err != nil {\n            return errors.Wrap(err, \"nextdb\")\n        }\n\n        mgr, err := swo.NewManager(swo.Config{OldDBURL: cfg.DBURL, NewDBURL: cfg.DBURLNext, CanExec: !cfg.APIOnly, Logger: cfg.Logger})\n        if err != nil {\n            return errors.Wrap(err, \"init switchover handler\")\n        }\n    }\n    // ...\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 87.61069, "model_code": " var RootCmd = &cobra.Command{ \n return err \n } \n var db *sql.DB \n if cfg.DBURLNext != \"\" { \n err = doMigrations(cfg.DBURLNext) \n if err != nil { \n return errors.Wrap(err, \"nextdb\") \n } \n } else { \n mgr, err := swo.NewManager(swo.Config{OldDBURL: cfg.DBURL, NewDBURL: cfg.DBURLNext, CanExec: !cfg.APIOnly, Logger: cfg.Logger}) \n if err != nil { \n return errors.Wrap(err, \"init switchover handler\")", "model_em_trim": 0, "model_bleu_trim": 95.97841924259353, "error_type_gpt": "Need Domain Knowledge"}
{"id": 50, "old": " class DumpReader(base.ReaderBase):\n     to represent the unit cell. Lengths *A*, *B*, *C* are in the MDAnalysis\n     length unit (\u00c5), and angles are in degrees.\n \n-    .. versionchanges:: 2.4.0\n        Now imports velocities and forces, translates the box to the origin,\n        and optionally unwraps trajectories with image flags upon loading.\n     .. versionchanged:: 2.2.0", "new": " class DumpReader(base.ReaderBase):\n     to represent the unit cell. Lengths *A*, *B*, *C* are in the MDAnalysis\n     length unit (\u00c5), and angles are in degrees.\n \n+    .. versionchanged:: 2.4.0\n        Now imports velocities and forces, translates the box to the origin,\n        and optionally unwraps trajectories with image flags upon loading.\n     .. versionchanged:: 2.2.0", "review": "```suggestion\r\n    .. versionchanged:: 2.4.0\r\n```", "dataset": "codereview_new", "original_id": 4416, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "no code", "gpt_em_trim": 0, "gpt_bleu_trim": 8.33e-88, "model_code": " class DumpReader(base.ReaderBase): \n to represent the unit cell. Lengths *A*, *B*, *C* are in the MDAnalysis \n length unit (\u00c5), and angles are in degrees. \n .. versionchanged:: 2.4.0", "model_em_trim": 0, "model_bleu_trim": 53.65253274094535, "error_type_gpt": "Model fallacy"}
{"id": 51, "old": " class MaskSpeechbubbleCornersFrameLayout @JvmOverloads constructor(\n     context: Context,\n     attrs: AttributeSet? = null,\n     defStyleAttr: Int = 0\n-) :\n-    FrameLayout(context, attrs, defStyleAttr) {\n \n     override fun dispatchDraw(canvas: Canvas) {\n         val path = Path()", "new": " class MaskSpeechbubbleCornersFrameLayout @JvmOverloads constructor(\n     context: Context,\n     attrs: AttributeSet? = null,\n     defStyleAttr: Int = 0\n+) : FrameLayout(context, attrs, defStyleAttr) {\n \n     override fun dispatchDraw(canvas: Canvas) {\n         val path = Path()", "review": "```suggestion\r\n) : FrameLayout(context, attrs, defStyleAttr) {\r\n```", "dataset": "codereview_new", "original_id": 1951, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\nclass MaskSpeechbubbleCornersFrameLayout @JvmOverloads constructor(\n    context: Context,\n    attrs: AttributeSet? = null,\n    defStyleAttr: Int = 0\n) : FrameLayout(context, attrs, defStyleAttr) {\n\n    override fun dispatchDraw(canvas: Canvas) {\n        val path = Path()\n        // code for drawing path\n        super.dispatchDraw(canvas)\n    }\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " class MaskSpeechbubbleCornersFrameLayout @JvmOverloads constructor( \n context: Context, \n attrs: AttributeSet? = null, \n defStyleAttr: Int = 0 \n ) : FrameLayout(context, attrs) { \n override fun dispatchDraw(canvas: Canvas) { \n val path = Path()", "model_em_trim": 0, "model_bleu_trim": 92.58269845870123}
{"id": 52, "old": " static LogicalResult setMaliMatmulConfig(linalg::LinalgOp op,\n   const int subgroupSize = limits.getSubgroupSize();\n   const std::array<int64_t, 2> workgroupXY = {subgroupSize / 2, 2};\n   std::array<int64_t, 3> threadMNK;\n-  Type elementType = op.getDpsInputOperand(0)\n-                         ->get()\n-                         .getType()\n-                         .cast<ShapedType>()\n-                         .getElementType();\n   if (elementType.getIntOrFloatBitWidth() == 16) {\n     threadMNK = {2, 8, 8};\n   } else if (elementType.isInteger(8)) {\n-    threadMNK = {2, 4, 4};\n   } else {\n     threadMNK = {6, 4, 4};\n   }", "new": " static LogicalResult setMaliMatmulConfig(linalg::LinalgOp op,\n   const int subgroupSize = limits.getSubgroupSize();\n   const std::array<int64_t, 2> workgroupXY = {subgroupSize / 2, 2};\n   std::array<int64_t, 3> threadMNK;\n+  Type inputType = op.getDpsInputOperand(0)->get().getType();\n+  Type elementType = inputType.cast<ShapedType>().getElementType();\n   if (elementType.getIntOrFloatBitWidth() == 16) {\n     threadMNK = {2, 8, 8};\n   } else if (elementType.isInteger(8)) {\n+    threadMNK = {4, 4, 4};\n   } else {\n     threadMNK = {6, 4, 4};\n   }", "review": "Nit: can we use two local variables for this long chain of method calls? The formatting here is not very nice. :) ", "dataset": "codereview_new", "original_id": 9425, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nstatic LogicalResult setMaliMatmulConfig(linalg::LinalgOp op) {\n  const int subgroupSize = limits.getSubgroupSize();\n  const std::array<int64_t, 2> workgroupXY = {subgroupSize / 2, 2};\n  std::array<int64_t, 3> threadMNK;\n  auto inputOperand = op.getDpsInputOperand(0)->get();\n  auto shapedType = inputOperand.getType().cast<ShapedType>();\n  Type elementType = shapedType.getElementType();\n  if (elementType.getIntOrFloatBitWidth() == 16) {\n    threadMNK = {2, 8, 8};\n  } else if (elementType.isInteger(8)) {\n    threadMNK = {2, 4, 4};\n  } else {\n    threadMNK = {6, 4, 4};\n  }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 86.95958, "model_code": " static LogicalResult setMaliMatmulConfig(linalg::LinalgOp op, \n const int subgroupSize = limits.getSubgroupSize(); \n const std::array<int64_t, 2> workgroupXY = {subgroupSize / 2, 2}; \n Type elementType = op.getDpsInputOperand(0) \n ->get() \n .getType() \n .cast<ShapedType>() \n .getElementType(); \n if (elementType.getIntOrFloatBitWidth() == 16) { \n threadMNK = {2, 8, 8}; \n } else if (elementType.isInteger(8)) { \n threadMNK = {2, 4, 4}; \n } else { \n threadMNK = {6, 4, 4}; \n }", "model_em_trim": 0, "model_bleu_trim": 83.39483117499837}
{"id": 53, "old": " final class FunctionsAPITests: XCTestCase {\n     }\n \n     let callableCodable = Functions.functions()\n-      .httpsCallable(\"codable\", requestAs: Message.self, resposneAs: Response.self)\n     let callableCodable2 = Functions.functions()\n-      .httpsCalalble(url, requestAs: Message.self, responseAs: Response.self)\n     let message = Message(hello: \"hello\", world: \"world\")\n     callableCodable.call(message) { result in\n       switch result {\n       case let .success(response):\n         let _: Response = response\n       case let .failure(error):\n-        // ...\n       }\n     }\n ", "new": " final class FunctionsAPITests: XCTestCase {\n     }\n \n     let callableCodable = Functions.functions()\n+      .httpsCallable(\"codable\", requestAs: Message.self, responseAs: Response.self)\n     let callableCodable2 = Functions.functions()\n+      .httpsCallable(url, requestAs: Message.self, responseAs: Response.self)\n     let message = Message(hello: \"hello\", world: \"world\")\n     callableCodable.call(message) { result in\n       switch result {\n       case let .success(response):\n         let _: Response = response\n       case let .failure(error):\n+        ()\n       }\n     }\n ", "review": "typo: `responseAs`", "dataset": "codereview_new", "original_id": 919, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfinal class FunctionsAPITests: XCTestCase {\n}\n\nlet callableCodable = Functions.functions()\n  .httpsCallable(\"codable\", requestAs: Message.self, responseAs: Response.self)\nlet callableCodable2 = Functions.functions()\n  .httpsCallable(url, requestAs: Message.self, responseAs: Response.self)\nlet message = Message(hello: \"hello\", world: \"world\")\ncallableCodable.call(message) { result in\n  switch result {\n  case let .success(response):\n    let _: Response = response\n  case let .failure(error):\n    // ...\n  }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 94.61629, "model_code": " final class FunctionsAPITests: XCTestCase { \n } \n let callableCodable = Functions.functions() \n .httpsCallable(\"codable\", requestAs: Message.self, responseAs: Response.self) \n let callableCodable2 = Functions.functions() \n .httpsCalalble(url, requestAs: Message.self, responseAs: Response.self) \n let message = Message(hello: \"hello\", world: \"world\") \n callableCodable.call(message) { \n switch result { \n case let .success(response): \n let _: Response = response \n case let .failure(error): \n // ... \n } \n } \n }", "model_em_trim": 0, "model_bleu_trim": 90.18042233129329}
{"id": 54, "old": " LINTERS_TO_USE <- list(\n     , \"any_is_na_linter\"     = lintr::any_is_na_linter()\n     , \"assignment\"           = lintr::assignment_linter()\n     , \"braces\"               = lintr::brace_linter()\n     , \"commas\"               = lintr::commas_linter()\n     , \"equals_na\"            = lintr::equals_na_linter()\n     , \"function_left\"        = lintr::function_left_parentheses_linter()", "new": " LINTERS_TO_USE <- list(\n     , \"any_is_na_linter\"     = lintr::any_is_na_linter()\n     , \"assignment\"           = lintr::assignment_linter()\n     , \"braces\"               = lintr::brace_linter()\n+    , \"class_equals_linter\"  = lintr::class_equals_linter()\n     , \"commas\"               = lintr::commas_linter()\n     , \"equals_na\"            = lintr::equals_na_linter()\n     , \"function_left\"        = lintr::function_left_parentheses_linter()", "review": "Use alphabetical order please.\r\n```suggestion\r\n    , \"class_equals\"         = lintr::class_equals_linter()\r\n    , \"commas\"               = lintr::commas_linter()\r\n```", "dataset": "codereview_new", "original_id": 6128, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\nLINTERS_TO_USE <- list(\n    , \"any_is_na_linter\"     = lintr::any_is_na_linter()\n    , \"assignment\"           = lintr::assignment_linter()\n    , \"braces\"               = lintr::brace_linter()\n    , \"class_equals\"         = lintr::class_equals_linter()\n    , \"commas\"               = lintr::commas_linter()\n    , \"equals_na\"            = lintr::equals_na_linter()\n    , \"function_left\"        = lintr::function_left_parentheses_linter()\n", "gpt_em_trim": 0, "gpt_bleu_trim": 97.20509, "model_code": " LINTERS_TO_USE <- list( \n , \"any_is_na_linter\"     = lintr::any_is_na_linter() \n , \"assignment\"           = lintr::assignment_linter() \n , \"class_equals\"         = lintr::class_equals_linter() \n , \"commas\"               = lintr::commas_linter() \n , \"equals_na\"            = lintr::equals_na_linter() \n , \"function_left\"        = lintr::function_left_parentheses_linter()", "model_em_trim": 0, "model_bleu_trim": 86.26419947740193, "error_type_gpt": "Reasonable Improvement"}
{"id": 55, "old": " func (monitor *heartbeatV2Monitor) parseMessage(pid core.PeerID, message interfa\n \t\treturn nil, fmt.Errorf(\"%w, messageAge %v\", heartbeat.ErrShouldSkipValidator, messageAge)\n \t}\n \n-\tpkHexString, err := monitor.pubKeyConverter.Encode(heartbeatV2.GetPubkey())\n \tif err != nil {\n \t\treturn nil, err\n \t}\n \n \tpubKeyHeartbeat := &data.PubKeyHeartbeat{\n-\t\tPublicKey:            pkHexString,\n \t\tTimeStamp:            messageTime,\n \t\tIsActive:             monitor.isActive(messageAge),\n \t\tReceivedShardID:      monitor.shardId,", "new": " func (monitor *heartbeatV2Monitor) parseMessage(pid core.PeerID, message interfa\n \t\treturn nil, fmt.Errorf(\"%w, messageAge %v\", heartbeat.ErrShouldSkipValidator, messageAge)\n \t}\n \n+\tencodedPubKey, err := monitor.pubKeyConverter.Encode(heartbeatV2.GetPubkey())\n \tif err != nil {\n \t\treturn nil, err\n \t}\n \n \tpubKeyHeartbeat := &data.PubKeyHeartbeat{\n+\t\tPublicKey:            encodedPubKey,\n \t\tTimeStamp:            messageTime,\n \t\tIsActive:             monitor.isActive(messageAge),\n \t\tReceivedShardID:      monitor.shardId,", "review": "can be renamed to `encodedPubKey`", "dataset": "codereview_new", "original_id": 7778, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\nfunc (monitor *heartbeatV2Monitor) parseMessage(pid core.PeerID, message interface{}) (*data.PubKeyHeartbeat, error) {\n\tmessageTime, messageAge, err := monitor.extractMessageTimeAndAge(message)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif messageAge > monitor.maxAge {\n\t\treturn nil, fmt.Errorf(\"%w, messageAge %v\", heartbeat.ErrShouldSkipValidator, messageAge)\n\t}\n\n\tencodedPubKey, err := monitor.pubKeyConverter.Encode(heartbeatV2.GetPubkey())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpubKeyHeartbeat := &data.PubKeyHeartbeat{\n\t\tPublicKey:            encodedPubKey,\n\t\tTimeStamp:            messageTime,\n\t\tIsActive:             monitor.isActive(messageAge),\n\t\tReceivedShardID:      monitor.shardId,\n", "gpt_em_trim": 0, "gpt_bleu_trim": 83.46666, "model_code": " func (monitor *heartbeatV2Monitor) parseMessage(pid core.PeerID, message interfa \n return nil, fmt.Errorf(\"%w, messageAge %v\", heartbeat.ErrShouldSkipValidator, messageAge) \n } \n pubKeyHeartbeat := &data.PubKeyHeartbeat{ \n PublicKey:            monitor.pubKeyConverter.Encode(heartbeatV2.GetPubKey()), \n TimeStamp:            messageTime, \n IsActive:             monitor.isActive(messageAge), \n ReceivedShardID:      monitor.shardId,", "model_em_trim": 0, "model_bleu_trim": 77.135614871952, "error_type_gpt": "Code Style Difference"}
{"id": 56, "old": "\n #' Report the use of undesirable functions, e.g. [base::return()], [base::options()], or\n #' [base::sapply()] and suggest an alternative.\n #'\n-#' @param fun Named character vector, where the names are the names of the undesirable functions, and the values are\n-#'   the text for the alternative function to use (or `NA`). Defaults to\n #'   [default_undesirable_functions]. To make small customizations to this list,\n #'   use [modify_defaults()].\n #' @param symbol_is_undesirable Whether to consider the use of an undesirable function name as a symbol undesirable", "new": "\n #' Report the use of undesirable functions, e.g. [base::return()], [base::options()], or\n #' [base::sapply()] and suggest an alternative.\n #'\n+#' @param fun Named character vector. `names(fun)` correspond to undesirable functions,\n+#'   while the values give a description of why the function is undesirable.\n+#'   If `NA`, no alternative is specified in the lint message. Defaults to\n #'   [default_undesirable_functions]. To make small customizations to this list,\n #'   use [modify_defaults()].\n #' @param symbol_is_undesirable Whether to consider the use of an undesirable function name as a symbol undesirable", "review": "This is no longer correct. The values are additional hints, since they're added to the lint message as-is.", "dataset": "codereview_new", "original_id": 2515, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-modify", "gpt_code": "\n#' Report the use of undesirable functions, e.g. [base::return()], [base::options()], or\n#' [base::sapply()] and suggest an alternative.\n#'\n#' @param fun Named character vector, where the names are the names of the undesirable functions, and the values are\n#'   additional hints to be added to the lint message. Defaults to\n#'   [default_undesirable_functions]. To make small customizations to this list,\n#'   use [modify_defaults()].\n#' @param symbol_is_undesirable Whether to consider the use of an undesirable function name as a symbol undesirable\n", "gpt_em_trim": 0, "gpt_bleu_trim": 75.91898, "model_code": " #' Report the use of undesirable functions, e.g. [base::return()], [base::options()], or \n #' [base::sapply()] and suggest an alternative. \n #' \n #' @param fun Named character vector, where the names are the names of the undesirable functions, and the values are \n #'   the text for the alternative function to use (or `NA`). Defaults to \n #'   [default_undesirable_functions]. To make small customizations to this list, \n #'   use [modify_defaults()]. \n #' @param symbol_is_undesirable Whether to consider the use of an undesirable function name as a symbol undesirable", "model_em_trim": 0, "model_bleu_trim": 75.82771205916234, "error_type_gpt": "Need Domain Knowledge"}
{"id": 57, "old": " module ClassMethods\n         # @example Mongoize the object.\n         #   Regexp.mongoize(/\\A[abc]/)\n         #\n-        # @param [ Regexp | String ] object The object to mongoize.\n         #\n         # @return [ Regexp | nil ] The object mongoized or nil.\n         def mongoize(object)", "new": " module ClassMethods\n         # @example Mongoize the object.\n         #   Regexp.mongoize(/\\A[abc]/)\n         #\n+        # @param [ Object ] object The object to mongoize.\n         #\n         # @return [ Regexp | nil ] The object mongoized or nil.\n         def mongoize(object)", "review": "This method actually accepts arbitrary input as of MONGOID-5222", "dataset": "codereview_new", "original_id": 4816, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nmodule ClassMethods\n  # @example Mongoize the object.\n  #   Regexp.mongoize(/\\A[abc]/)\n  #\n  # @param [ Regexp | String | Object ] object The object to mongoize.\n  #\n  # @return [ Regexp | nil ] The object mongoized or nil.\n  def mongoize(object)\n    case object\n    when Regexp\n      object\n    when String\n      Regexp.new(object)\n    else\n      nil\n    end\n  end\nend\n", "gpt_em_trim": 0, "gpt_bleu_trim": 90.5067, "model_code": " module ClassMethods \n # @example Mongoize the object. \n #   Regexp.mongoize(/\\A[abc]/) \n # @param [ Regexp | String ] object The object to mongoize. \n # \n # @return [ Regexp | nil ] The object mongoized or nil. \n def mongoize(object)", "model_em_trim": 0, "model_bleu_trim": 90.37018168371624, "error_type_gpt": "Unclear Location"}
{"id": 58, "old": " class CapitalConnectionsFinder(private val civInfo: CivilizationInfo) {\n             return\n \n         val bfs = BFS(cityToConnectFrom.getCenterTile()) {\n-              it.getOwner()?.let { owner -> civInfo.hasOpenBordersTo(owner) } ?: true && (it.isCityCenter() || tileFilter(it)) }\n         bfs.stepToEnd()\n         val reachedCities = openBordersCivCities.filter {\n             bfs.hasReachedTile(it.getCenterTile()) && cityFilter(it)", "new": " class CapitalConnectionsFinder(private val civInfo: CivilizationInfo) {\n             return\n \n         val bfs = BFS(cityToConnectFrom.getCenterTile()) {\n+              val owner = it.getOwner()\n+              (it.isCityCenter() || tileFilter(it)) && (owner == null || civInfo.hasOpenBordersTo(owner))\n+        }\n         bfs.stepToEnd()\n         val reachedCities = openBordersCivCities.filter {\n             bfs.hasReachedTile(it.getCenterTile()) && cityFilter(it)", "review": "`{ -> Boolean? } ?: true` I'd write as `!= false`, matter of taste. But `?.let` should always be checked for alternatives with better readability...\r\n```kotlin\r\n        val bfs = BFS(cityToConnectFrom.getCenterTile()) {\r\n            val owner = it.getOwner() ?: return@BFS tileFilter(it)\r\n            civInfo.hasOpenBordersTo(owner) && (it.isCityCenter() || tileFilter(it))\r\n        }\r\n```\r\nshould be equivalent as long as the assumption that tiles with isCityCenter() true always have an owner holds.", "dataset": "codereview_new", "original_id": 1438, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass CapitalConnectionsFinder(private val civInfo: CivilizationInfo) {\n    val bfs = BFS(cityToConnectFrom.getCenterTile()) {\n        val owner = it.getOwner()\n        if (owner == null) {\n            tileFilter(it)\n        } else {\n            civInfo.hasOpenBordersTo(owner) && (it.isCityCenter() || tileFilter(it))\n        }\n    }\n    bfs.stepToEnd()\n    val reachedCities = openBordersCivCities.filter {\n        bfs.hasReachedTile(it.getCenterTile()) && cityFilter(it)\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 79.33929, "model_code": " class CapitalConnectionsFinder(private val civInfo: CivilizationInfo) { \n return \n val bfs = BFS(cityToConnectFrom.getCenterTile()) { \n it.getOwner()?.let { owner -> civInfo.hasOpenBordersTo(owner) } : true && (it.isCityCenter() || tileFilter(it)) } \n bfs.stepToEnd() \n val reachedCities = openBordersCivCities.filter { \n bfs.hasReachedTile(it.getCenterTile()) && cityFilter(it)", "model_em_trim": 0, "model_bleu_trim": 81.35160335546433}
{"id": 59, "old": " class LocalLeaderEndPointTest {\n \n     assertEquals(expected, result)\n \n-    // Check missing epoch\n     result = endPoint.fetchEpochEndOffsets(Map(\n       topicPartition -> new OffsetForLeaderPartition()\n         .setPartition(topicPartition.partition)", "new": " class LocalLeaderEndPointTest {\n \n     assertEquals(expected, result)\n \n+    // Check missing epoch: 3, we expect the API to return (leader_epoch=0, end_offset=3).\n+    result = endPoint.fetchEpochEndOffsets(Map(\n+      topicPartition -> new OffsetForLeaderPartition()\n+        .setPartition(topicPartition.partition)\n+        .setLeaderEpoch(3)))\n+\n+    expected = Map(\n+      topicPartition -> new EpochEndOffset()\n+        .setPartition(topicPartition.partition)\n+        .setErrorCode(Errors.NONE.code)\n+        .setLeaderEpoch(0)\n+        .setEndOffset(3L))\n+\n+    assertEquals(expected, result)\n+\n+    // Check missing epoch: 5, we expect the API to return (leader_epoch=-1, end_offset=-1)\n     result = endPoint.fetchEpochEndOffsets(Map(\n       topicPartition -> new OffsetForLeaderPartition()\n         .setPartition(topicPartition.partition)", "review": "Could we add another test for the missing epoch in the middle like 3?", "dataset": "codereview_new", "original_id": 8269, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\nclass LocalLeaderEndPointTest {\n\n    assertEquals(expected, result)\n\n    // Check missing epoch at beginning\n    result = endPoint.fetchEpochEndOffsets(Map(\n      topicPartition -> new OffsetForLeaderPartition()\n        .setPartition(topicPartition.partition)\n        .setEpoch(1)\n    ))\n\n    assertEquals(expected, result)\n\n    // Check missing epoch in the middle\n    result = endPoint.fetchEpochEndOffsets(Map(\n      topicPartition -> new OffsetForLeaderPartition()\n        .setPartition(topicPartition.partition)\n    ))\n\n    assertEquals(expected, result)\n\n    // Check missing epoch at end\n    result = endPoint.fetchEpochEndOffsets(Map(\n      topicPartition -> new OffsetForLeaderPartition()\n        .setPartition(topicPartition.partition)\n        .setEpoch(3)\n    ))\n\n    assertEquals(expected, result)\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 49.13264, "model_code": " class LocalLeaderEndPointTest { \n assertEquals(expected, result) \n \n result = endPoint.fetchEpochEndOffsets(Map( \n topicPartition -> new OffsetForLeaderPartition() \n .setPartition(topicPartition.partition)", "model_em_trim": 0, "model_bleu_trim": 1.6758660175587181, "error_type_gpt": "Need Domain Knowledge"}
{"id": 60, "old": " public function testGetResults(): void\n         $obj->process($results);\n         $results->expects($this->once())->method('getTopicRecommendations')\n             ->will($this->returnValue(false));\n-        $this->assertFalse($obj->getresults());\n     }\n \n     /**", "new": " public function testGetResults(): void\n         $obj->process($results);\n         $results->expects($this->once())->method('getTopicRecommendations')\n             ->will($this->returnValue(false));\n+        $this->assertFalse($obj->getResults());\n     }\n \n     /**", "review": "Another `getresults` that should be `getResults`.", "dataset": "codereview_new", "original_id": 6650, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\npublic function testGetResults(): void\n        $obj->process($results);\n        $results->expects($this->once())->method('getTopicRecommendations')\n            ->will($this->returnValue(false));\n        $this->assertFalse($obj->getResults());\n    }\n\n    /**\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " public function testGetResults(): void \n $obj->process($results); \n $results->expects($this->once())->method('getResults') \n ->will($this->returnValue(false)); \n $this->assertFalse($obj->getResults()); \n } \n /**", "model_em_trim": 0, "model_bleu_trim": 96.45993475832401}
{"id": 61, "old": " public TTransport request(FContext context, byte[] payload) throws TTransportExc\n         byte[] response = makeRequest(context, payload);\n \n         TTransport responseTransport = null;\n-        if(response != null) {\n             TConfiguration responseConfig =\n                 TConfigurationBuilder.custom().setMaxMessageSize(responseSizeLimit).build();\n             responseTransport = new TMemoryInputTransport(responseConfig, response);", "new": " public TTransport request(FContext context, byte[] payload) throws TTransportExc\n         byte[] response = makeRequest(context, payload);\n \n         TTransport responseTransport = null;\n+        if (response != null) {\n             TConfiguration responseConfig =\n                 TConfigurationBuilder.custom().setMaxMessageSize(responseSizeLimit).build();\n             responseTransport = new TMemoryInputTransport(responseConfig, response);", "review": "```suggestion\r\n        if (response != null) {\r\n```", "dataset": "codereview_new", "original_id": 5151, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\npublic TTransport request(FContext context, byte[] payload) throws TTransportExc\n        byte[] response = makeRequest(context, payload);\n\n        TTransport responseTransport = null;\n        if (response != null && response.length > 0) {\n            TConfiguration responseConfig =\n                TConfigurationBuilder.custom().setMaxMessageSize(responseSizeLimit).build();\n            responseTransport = new TMemoryInputTransport(responseConfig, response);\n", "gpt_em_trim": 0, "gpt_bleu_trim": 88.36248, "model_code": " public TTransport request(FContext context, byte[] payload) throws TTransportExc \n byte[] response = makeRequest(context, payload); \n TTransport responseTransport = null; \n if (response != null) { \n TConfiguration responseConfig = \n TConfigurationBuilder.custom().setMaxMessageSize(responseSizeLimit).build(); \n responseTransport = new TMemoryInputTransport(responseConfig, response);", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Model fallacy"}
{"id": 62, "old": " default void importTable(String tableName, String importDir)\n    *          A set of directories containing the files copied by distcp from exportTable\n    * @since 2.1.0\n    */\n-  void importTable(String tableName, ImportConfiguration ic, Set<String> importDirs)\n       throws TableExistsException, AccumuloException, AccumuloSecurityException;\n \n   /**", "new": " default void importTable(String tableName, String importDir)\n    *          A set of directories containing the files copied by distcp from exportTable\n    * @since 2.1.0\n    */\n+  void importTable(String tableName, Set<String> importDirs, ImportConfiguration ic)\n       throws TableExistsException, AccumuloException, AccumuloSecurityException;\n \n   /**", "review": "Conceptually ImportConfiguration allows expressing one or more optional arguments.  Could place the two required arguments first and the optional ones last.  This is just personal preference, please ignore if it does not resonate.\r\n\r\n```suggestion\r\n  void importTable(String tableName, Set<String> importDirs, ImportConfiguration ic)\r\n```", "dataset": "codereview_new", "original_id": 4527, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-swap", "category_type_user1": "Refactoring-swap", "category_type_user2": "Refactoring-swap", "gpt_code": "\nvoid importTable(String tableName, Set<String> importDirs, ImportConfiguration ic)\n      throws TableExistsException, AccumuloException, AccumuloSecurityException;\n", "gpt_em_trim": 0, "gpt_bleu_trim": 20.90426, "model_code": " default void importTable(String tableName, String importDir) \n *          A set of directories containing the files copied by distcp from exportTable \n * @since 2.1.0 \n * @throws TableExistsException, AccumuloException, AccumuloSecurityException; \n /**", "model_em_trim": 0, "model_bleu_trim": 66.83861129301384, "error_type_gpt": "Insignificant Omission"}
{"id": 63, "old": " import (\n var reverseProxyMetrics = struct {\n \tinit             sync.Once\n \tupstreamsHealthy *prometheus.GaugeVec\n-}{\n-\tinit: sync.Once{},\n-}\n \n func initReverseProxyMetrics() {\n \tconst ns, sub = \"caddy\", \"reverseproxy\"", "new": " import (\n var reverseProxyMetrics = struct {\n \tinit             sync.Once\n \tupstreamsHealthy *prometheus.GaugeVec\n+}{}\n \n func initReverseProxyMetrics() {\n \tconst ns, sub = \"caddy\", \"reverseproxy\"", "review": "```suggestion\r\n\tconst ns, sub = \"caddy\", \"reverse_proxy\"\r\n```\r\n\r\nTechnically, the module name is `http.handlers.reverse_proxy` -- not sure how much we care to have namespace parity here?", "dataset": "codereview_new", "original_id": 5225, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nimport (\nvar reverseProxyMetrics = struct {\n\tinit             sync.Once\n\tupstreamsHealthy *prometheus.GaugeVec\n}{\n\tinit: sync.Once{},\n}\n\nfunc initReverseProxyMetrics() {\n\tconst ns, sub = \"http.handlers\", \"reverse_proxy\"\n", "gpt_em_trim": 0, "gpt_bleu_trim": 62.53126, "model_code": " import ( \n var reverseProxyMetrics = struct { \n init sync.Once \n upstreamsHealthy *prometheus.GaugeVec \n }{ \n init: sync.Once{}, \n } \n func initReverseProxyMetrics() { \n const ns, sub = \"caddy\", \"reverseproxy\"", "model_em_trim": 0, "model_bleu_trim": 78.47193902639101}
{"id": 64, "old": " namespace AI\n             Troop * unitToSwap = heroArmy.GetSlowestTroop();\n             if ( unitToSwap ) {\n                 // We need to compare a strength of troops excluding hero's stats.\n-                const double troopsStrength = Troops( heroArmy ).GetStrength();\n \n                 const double significanceRatio = isFigtherHero ? 20.0 : 10.0;\n                 if ( unitToSwap->GetStrength() > troopsStrength / significanceRatio ) {", "new": " namespace AI\n             Troop * unitToSwap = heroArmy.GetSlowestTroop();\n             if ( unitToSwap ) {\n                 // We need to compare a strength of troops excluding hero's stats.\n+                const double troopsStrength = Troops( heroArmy.getTroops() ).GetStrength();\n \n                 const double significanceRatio = isFigtherHero ? 20.0 : 10.0;\n                 if ( unitToSwap->GetStrength() > troopsStrength / significanceRatio ) {", "review": ":warning: **cppcoreguidelines\\-slicing** :warning:\nslicing object from type `` Army `` to `` Troops `` discards 24 bytes of state", "dataset": "codereview_new", "original_id": 4642, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nnamespace AI\n            Troop * unitToSwap = heroArmy.GetSlowestTroop();\n            if ( unitToSwap ) {\n                // We need to compare a strength of troops excluding hero's stats.\n                const Troops troops(heroArmy);\n                const double troopsStrength = troops.GetStrength();\n\n                const double significanceRatio = isFigtherHero ? 20.0 : 10.0;\n                if ( unitToSwap->GetStrength() > troopsStrength / significanceRatio ) {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 89.83383, "model_code": " namespace AI \n Troop * unitToSwap = heroArmy.GetSlowestTroop(); \n if ( unitToSwap ) { \n const double significanceRatio = isFigtherHero ? 20.0 : 10.0; \n if ( unitToSwap->GetStrength() > troopsStrength / significanceRatio ) {", "model_em_trim": 0, "model_bleu_trim": 47.108360867237764, "error_type_gpt": "Need Domain Knowledge"}
{"id": 65, "old": " std::string py_fetch_error() {\n   // build error text\n   std::ostringstream oss;\n \n   for (Py_ssize_t i = 0, n = PyList_Size(formatted); i < n; i++)\n     oss << as_std_string(PyList_GetItem(formatted, i));\n \n   std::string error = oss.str();\n \n-  int max_msg_len(Rf_asInteger(Rf_GetOption1(Rf_install(\"warning.length\"))));\n   if (error.size() > max_msg_len) {\n     // R has a modest byte size limit for error messages, default 1000, user\n     // adjustable up to 8170. Error messages beyond the limit are silently", "new": " std::string py_fetch_error() {\n   // build error text\n   std::ostringstream oss;\n \n+  // PyList_GetItem() returns a borrowed reference, no need to decref.\n   for (Py_ssize_t i = 0, n = PyList_Size(formatted); i < n; i++)\n     oss << as_std_string(PyList_GetItem(formatted, i));\n \n   std::string error = oss.str();\n \n+  SEXP max_msg_len_s = PROTECT(Rf_GetOption1(Rf_install(\"warning.length\")));\n+  int max_msg_len(Rf_asInteger(max_msg_len_s));\n+  UNPROTECT(1);\n+\n   if (error.size() > max_msg_len) {\n     // R has a modest byte size limit for error messages, default 1000, user\n     // adjustable up to 8170. Error messages beyond the limit are silently", "review": "nit: good practice to document as a reminder places where we're using Python APIs which provide borrowed references", "dataset": "codereview_new", "original_id": 2210, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\n// This function uses Python APIs which provide borrowed references\nstd::string py_fetch_error() {\n  // build error text\n  std::ostringstream oss;\n\n  for (Py_ssize_t i = 0, n = PyList_Size(formatted); i < n; i++)\n    oss << as_std_string(PyList_GetItem(formatted, i));\n\n  std::string error = oss.str();\n\n  int max_msg_len(Rf_asInteger(Rf_GetOption1(Rf_install(\"warning.length\"))));\n  if (error.size() > max_msg_len) {\n    // R has a modest byte size limit for error messages, default 1000, user\n    // adjustable up to 8170. Error messages beyond the limit are silently\n", "gpt_em_trim": 0, "gpt_bleu_trim": 76.60743, "model_code": " std::string py_fetch_error() { \n // build error text \n std::ostringstream oss; \n int max_msg_len = Rf_asInteger(Rf_GetOption1(Rf_install(\"warning.length\"))); \n for (Py_ssize_t i = 0, n = PyList_Size(formatted); i < n; i++) \n oss << as_std_string(PyList_GetItem(formatted, i)); \n std::string error = oss.str(); \n if (error.size() > max_msg_len) { \n // R has a modest byte size limit for error messages, default 1000, user \n // adjustable up to 8170. Error messages beyond the limit are silently", "model_em_trim": 0, "model_bleu_trim": 73.30257348509441}
{"id": 66, "old": " unite.dtplyr_step <- function(data, col, ..., sep = \"_\", remove = TRUE, na.rm =\n     .cols <- data$vars\n   } else {\n     sim_data <- simulate_vars(data)\n-    .cols <- names(tidyselect::eval_select(expr(c(!!!dots)), sim_data))\n   }\n \n   out <- mutate(ungroup(data), !!.col := paste(!!!syms(.cols), sep = sep))", "new": " unite.dtplyr_step <- function(data, col, ..., sep = \"_\", remove = TRUE, na.rm =\n     .cols <- data$vars\n   } else {\n     sim_data <- simulate_vars(data)\n+    locs <- tidyselect::eval_select(expr(c(x, y)), sim_data, allow_rename = FALSE)\n+    .cols <- data$vars[locs]\n   }\n \n   out <- mutate(ungroup(data), !!.col := paste(!!!syms(.cols), sep = sep))", "review": "This should probably use `allow_rename = FALSE`, e.g. something like\r\n\r\n```r\r\nlocs <- tidyselect::eval_select(expr(c(x, y)), sim_data, allow_rename = FALSE)\r\n.cols <- data$vars[locs]\r\n```\r\n\r\nOtherwise, to prevent this error (which also happens for normal dataframes)\r\n\r\n```r\r\ndf %>% unite(\"z\", c(a = x, y), remove = FALSE)\r\n#> Error in paste(a, y, sep = \"_\") : object 'a' not found\r\n```\r\n\r\nNote that the `allow_rename = FALSE` does not really do anything here, using the locations is the relevant part here. But I think `allow_rename = FALSE` communicates the intent better.", "dataset": "codereview_new", "original_id": 2608, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nunite.dtplyr_step <- function(data, col, ..., sep = \"_\", remove = TRUE, na.rm = FALSE) {\n  sim_data <- simulate_vars(data)\n  locs <- tidyselect::eval_select(expr(c(!!!dots)), sim_data, allow_rename = FALSE)\n  .cols <- data$vars[locs]\n  out <- mutate(ungroup(data), !!.col := paste(!!!syms(.cols), sep = sep))\n  if (remove) {\n    out <- select(out, -one_of(.cols))\n  }\n  out\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 77.19478, "model_code": " unite.dtplyr_step <- function(data, col, ..., sep = \"_\", remove = TRUE, na.rm = \n .cols <- data$vars \n } else { \n sim_data <- simulate_vars(data, allow_rename = FALSE) \n .cols <- names(tidyselect::eval_select(expr(c(!!!dots)), sim_data)) \n } \n out <- mutate(ungroup(data), !!.col := paste(!!!syms(.cols), sep = sep))", "model_em_trim": 0, "model_bleu_trim": 85.58368803058215, "error_type_gpt": "Unclear Changes"}
{"id": 67, "old": " def setup\n     @finished_regular_event = ActiveDecorator::Decorator.instance.decorate(regular_events(:regular_event8))\n   end\n \n-  test 'holding_cycles' do\n     assert_equal '\u6bce\u9031\u65e5\u66dc\u65e5', @regular_event.holding_cycles\n   end\n \n-  test 'next_holding_date' do\n     travel_to Time.zone.local(2022, 6, 1, 0, 0, 0) do\n       assert_equal '\u6b21\u56de\u306e\u958b\u50ac\u65e5\u306f 2022\u5e7406\u670805\u65e5 \u3067\u3059', @regular_event.next_holding_date\n     end", "new": " def setup\n     @finished_regular_event = ActiveDecorator::Decorator.instance.decorate(regular_events(:regular_event8))\n   end\n \n+  test '#holding_cycles' do\n     assert_equal '\u6bce\u9031\u65e5\u66dc\u65e5', @regular_event.holding_cycles\n   end\n \n+  test '#next_holding_date' do\n     travel_to Time.zone.local(2022, 6, 1, 0, 0, 0) do\n       assert_equal '\u6b21\u56de\u306e\u958b\u50ac\u65e5\u306f 2022\u5e7406\u670805\u65e5 \u3067\u3059', @regular_event.next_holding_date\n     end", "review": "```suggestion\r\n  test '#holding_cycles' do\r\n```\r\n\r\nmodel\u3084decorator\u306a\u3069\u30e1\u30bd\u30c3\u30c9\u540d\u305d\u306e\u307e\u307e\u306e\u30c6\u30b9\u30c8\u306e\u5834\u5408\u306f\u30af\u30e9\u30b9\u30e1\u30bd\u30c3\u30c9\u304b\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u30e1\u30bd\u30c3\u30c9\u304c\u308f\u304b\u308b\u3088\u3046\u306b\u3053\u3046\u3044\u3046\u611f\u3058\u306b\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u3042\u308a\u304c\u305f\u3044\u3067\u3059\u301c\r\n\r\n\uff08user_decorator\u3082\u3053\u3046\u306a\u3063\u3066\u306f\u3044\u306a\u3044\u306e\u3067\u3059\u304c\u3001\u5225Issue\u3067\u5bfe\u5fdc\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\uff09", "dataset": "codereview_new", "original_id": 7192, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n  test '#holding_cycles instance method' do\n    assert_equal '\u6bce\u9031\u65e5\u66dc\u65e5', @regular_event.holding_cycles\n  end\n\n  test '#next_holding_date instance method' do\n    travel_to Time.zone.local(2022, 6, 1, 0, 0, 0) do\n      assert_equal '\u6b21\u56de\u306e\u958b\u50ac\u65e5\u306f 2022\u5e7406\u670805\u65e5 \u3067\u3059', @regular_event.next_holding_date\n    end\n  end\n", "gpt_em_trim": 0, "gpt_bleu_trim": 67.568, "model_code": " def setup \n @finished_regular_event = ActiveDecorator::Decorator.instance.decorate(regular_events(:regular_event8)) \n end", "model_em_trim": 0, "model_bleu_trim": 7.939393227707822}
{"id": 68, "old": " internal class AbstractYamlConfigValidatorSpec {\n         assertThat(validator.validationSettings.checkExhaustiveness).isEqualTo(configValue)\n     }\n \n-    private class SettingsCapturingValidatorAbstract() : AbstractYamlConfigValidator() {\n         lateinit var validationSettings: ValidationSettings\n         override fun validate(\n             configToValidate: YamlConfig,", "new": " internal class AbstractYamlConfigValidatorSpec {\n         assertThat(validator.validationSettings.checkExhaustiveness).isEqualTo(configValue)\n     }\n \n+    private class SettingsCapturingValidatorAbstract : AbstractYamlConfigValidator() {\n         lateinit var validationSettings: ValidationSettings\n         override fun validate(\n             configToValidate: YamlConfig,", "review": "## Empty block of code detected. As they serve no purpose they should be removed.\n\nAn empty default constructor can be removed.\n\n[Show more details](https://github.com/detekt/detekt/security/code-scanning/5952)", "dataset": "codereview_new", "original_id": 1271, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-remove", "gpt_code": "\ninternal class AbstractYamlConfigValidatorSpec {\n        assertThat(validator.validationSettings.checkExhaustiveness).isEqualTo(configValue)\n    }\n\n    private class SettingsCapturingValidatorAbstract : AbstractYamlConfigValidator() {\n        lateinit var validationSettings: ValidationSettings\n        override fun validate(\n            configToValidate: YamlConfig,\n            validationSettings: ValidationSettings\n        ): ValidationResult {\n            this.validationSettings = validationSettings\n            return super.validate(configToValidate, validationSettings)\n        }\n    }\n", "gpt_em_trim": 1, "gpt_bleu_trim": 92.72419, "model_code": " internal class AbstractYamlConfigValidatorSpec { \n assertThat(validator.validationSettings.checkExhaustiveness).isEqualTo(configValue) \n } \n \n private class SettingsCapturingValidatorAbstract : AbstractYamlConfigValidatorSpec { \n lateinit var validationSettings: ValidationSettings \n override fun validate( \n configToValidate: YamlConfig,", "model_em_trim": 0, "model_bleu_trim": 88.20850105454649}
{"id": 69, "old": "\n FINISHED = _(\"Installation finished\")\n PENDING = _(\"Update available\")\n CHECKING = _(\"Checking for updates\")\n-CANCELLED = _(\"Cancelled\")\n \n INSTALL_INFO_MESSAGES = {\n-    DOWNLOADING_INSTALLER: _(\"Downloading Spyder latest \"\n-                             \"release installer executable\"),\n-    INSTALLING: _(\"Installing Spyder latest release\"),\n     FINISHED: _(\"Spyder update installation finished\"),\n-    PENDING: _(\"An update is pending\"),\n-    CHECKING: _(\"Checking for updates\"),\n-    CANCELLED: _(\"Update cancelled\")\n }\n \n ", "new": "\n FINISHED = _(\"Installation finished\")\n PENDING = _(\"Update available\")\n CHECKING = _(\"Checking for updates\")\n+CANCELLED = _(\"Cancelled update\")\n \n INSTALL_INFO_MESSAGES = {\n+    DOWNLOADING_INSTALLER: _(\"Downloading latest Spyder update\"),\n+    INSTALLING: _(\"Installing Spyder update\"),\n     FINISHED: _(\"Spyder update installation finished\"),\n+    PENDING: _(\"Spyder update available to download\"),\n+    CHECKING: _(\"Checking for Spyder updates\"),\n+    CANCELLED: _(\"Spyder update cancelled\")\n }\n \n ", "review": "Making this as a seperate comment so its easier to find on this long PR\r\n\r\n```suggestion\r\nCANCELLED = _(\"Cancelled update\")\r\n\r\nINSTALL_INFO_MESSAGES = {\r\n    DOWNLOADING_INSTALLER: _(\"Downloading latest Spyder update\"),\r\n    INSTALLING: _(\"Installing Spyder update\"),\r\n    FINISHED: _(\"Spyder update installation finished\"),\r\n    PENDING: _(\"Spyder update available to download\"),\r\n    CHECKING: _(\"Checking for Spyder updates\"),\r\n    CANCELLED: _(\"Spyder update cancelled\")\r\n```", "dataset": "codereview_new", "original_id": 9161, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-modify", "gpt_code": "\nCANCELLED_UPDATE = _(\"Cancelled update\")\n\nINSTALL_INFO_MESSAGES = {\n    DOWNLOADING_INSTALLER: _(\"Downloading latest Spyder update\"),\n    INSTALLING: _(\"Installing Spyder update\"),\n    FINISHED: _(\"Spyder update installation finished\"),\n    PENDING: _(\"Spyder update available to download\"),\n    CHECKING: _(\"Checking for Spyder updates\"),\n    CANCELLED_UPDATE: _(\"Spyder update cancelled\")\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 72.79785, "model_code": " FINISHED = _(\"Installation finished\") \n PENDING = _(\"Update available\") \n RETURNED = _(\"Spyder update installation finished\")", "model_em_trim": 0, "model_bleu_trim": 4.839355002301722, "error_type_gpt": "Model fallacy"}
{"id": 70, "old": " object ResolvedAst {\n \n   case class Op(sym: Symbol.OpSym, spec: ResolvedAst.Spec) extends Declaration\n \n-  // MATT make Declaration object\n-  // MATT or maybe kill this if unneeded\n   sealed trait Declaration\n \n   sealed trait Expression {", "new": " object ResolvedAst {\n \n   case class Op(sym: Symbol.OpSym, spec: ResolvedAst.Spec) extends Declaration\n \n+  // TODO NS-REFACTOR make Declaration object\n   sealed trait Declaration\n \n   sealed trait Expression {", "review": "Add TODO NS-REFACTOR ", "dataset": "codereview_new", "original_id": 3720, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nobject ResolvedAst {\n\n  case class Op(sym: Symbol.OpSym, spec: ResolvedAst.Spec) extends Declaration\n\n  // TODO NS-REFACTOR: Consider removing Declaration object if unneeded\n  sealed trait Declaration\n\n  sealed trait Expression {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 80.48449, "model_code": " object ResolvedAst { \n case class Op(sym: Symbol.OpSym, spec: ResolvedAst.Spec) extends Declaration \n // TODO: MATT make Declaration object \n // MATT or maybe kill this if unneeded \n sealed trait Declaration \n sealed trait Expression {", "model_em_trim": 0, "model_bleu_trim": 68.02421213130039, "error_type_gpt": "Need Domain Knowledge"}
{"id": 71, "old": " test_that(\"commented_code_linter can detect operators in comments and lint corre\n   test_ops <- c(\n     \"+\", \"=\", \"==\", \"!=\", \"<=\", \">=\", \"<-\", \"<<-\", \"<\", \">\", \"->\",\n     \"->>\", \"%%\", \"%||%\", \"/\", \"^\", \"*\", \"**\", \"|\", \"||\", \"&\", \"&&\",\n-    \"%>%\", \"%<>%\", \"%T>%\", \"%anything%\"\n   )\n \n   for (op in test_ops) {", "new": " test_that(\"commented_code_linter can detect operators in comments and lint corre\n   test_ops <- c(\n     \"+\", \"=\", \"==\", \"!=\", \"<=\", \">=\", \"<-\", \"<<-\", \"<\", \">\", \"->\",\n     \"->>\", \"%%\", \"%||%\", \"/\", \"^\", \"*\", \"**\", \"|\", \"||\", \"&\", \"&&\",\n+    \"%>%\", \"%<>%\", \"%T>%\", \"%$%\", \"%!>%\", \"%anything%\"\n   )\n \n   for (op in test_ops) {", "review": "From magrittr", "dataset": "codereview_new", "original_id": 2370, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ntest_that(\"commented_code_linter can detect operators in comments and lint correctly\", {\n  test_ops <- c(\n    \"+\", \"=\", \"==\", \"!=\", \"<=\", \">=\", \"<-\", \"<<-\", \"<\", \">\", \"->\",\n    \"->>\", \"%%\", \"%||%\", \"/\", \"^\", \"*\", \"**\", \"|\", \"||\", \"&\", \"&&\",\n    \"%>%\", \"%<>%\", \"%T>%\", \"%anything%\"\n  )\n\n  for (op in test_ops) {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 91.56617, "model_code": " test_that(\"commented_code_linter can detect operators in comments and lint corre \n test_ops <- c( \n \"+\", \"=\", \"==\", \"!=\", \"<=\", \">=\", \"<-\", \"<<-\", \"<\", \">\", \"->\", \n \"->>\", \"/\", \"^\", \"*\", \"**\", \"|\", \"||\", \"&\", \"&&\", \n \"%>%\", \"%<>%\", \"%T>%\", \"%anything%\" \n ) \n for (op in test_ops) {", "model_em_trim": 0, "model_bleu_trim": 85.01540910514973, "error_type_gpt": "Need Domain Knowledge"}
{"id": 72, "old": " class KafkaServer(\n                 case None =>\n                   info(s\"Broker registration for controller $controllerId is not available in the metadata cache\")\n               }\n-            case _ =>\n               info(\"No zk controller present in the metadata cache\")\n           }\n ", "new": " class KafkaServer(\n                 case None =>\n                   info(s\"Broker registration for controller $controllerId is not available in the metadata cache\")\n               }\n+            case Some(_: KRaftCachedControllerId) | None =>\n               info(\"No zk controller present in the metadata cache\")\n           }\n ", "review": "this one confuses me. Isn't `case None` clearer than `case _` ?", "dataset": "codereview_new", "original_id": 8354, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass KafkaServer(\n                case None =>\n                  info(s\"Broker registration for controller $controllerId is not available in the metadata cache\")\n              }\n            case None =>\n              info(\"No zk controller present in the metadata cache\")\n          }\n\n", "gpt_em_trim": 0, "gpt_bleu_trim": 82.90272, "model_code": " class KafkaServer( \n case None => \n info(s\"Broker registration for controller $controllerId is not available in the metadata cache\") \n default: \n info(s\"No zk controller present in the metadata cache\") \n } \n } \n", "model_em_trim": 0, "model_bleu_trim": 71.33292542514921, "error_type_gpt": "Need Domain Knowledge"}
{"id": 73, "old": " def _bfs_with_marks(G, start_node, check_set):\n                 # memoize where we visited so far\n                 visited[neighbr] = None\n \n-                # mark the node in Z' and do not continue\n                 # along that path\n                 if neighbr in check_set:\n                     marked.add(neighbr)", "new": " def _bfs_with_marks(G, start_node, check_set):\n                 # memoize where we visited so far\n                 visited[neighbr] = None\n \n+                # mark the node in 'check_set' and do not continue\n                 # along that path\n                 if neighbr in check_set:\n                     marked.add(neighbr)", "review": "```suggestion\r\n                # mark the node in 'check_set' and do not continue\r\n```", "dataset": "codereview_new", "original_id": 5684, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\ndef _bfs_with_marks(G, start_node, check_set):\n                # memoize where we visited so far\n                visited[neighbr] = None\n\n                # mark the node in check_set and do not continue\n                # along that path\n                if neighbr in check_set:\n                    marked.add(neighbr)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 91.88322, "model_code": " def _bfs_with_marks(G, start_node, check_set): \n # memoize where we visited so far \n visited[neighbr] = None \n # mark the node in 'check_set' and do not continue \n # along that path \n if neighbr in check_set: \n marked.add(neighbr)", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Code Style Difference"}
{"id": 74, "old": " def setup(self):\n \n         goto_cursor_action = self.create_action(\n             DebuggerWidgetActions.GotoCursor,\n-            text=_(\"Show in editor\"),\n             icon=self.create_icon('fromcursor'),\n             triggered=self.goto_current_step,\n             register_shortcut=True", "new": " def setup(self):\n \n         goto_cursor_action = self.create_action(\n             DebuggerWidgetActions.GotoCursor,\n+            text=_(\"Show in the editor the file and line where the debugger \"\n+                   \"is placed\"),\n             icon=self.create_icon('fromcursor'),\n             triggered=self.goto_current_step,\n             register_shortcut=True", "review": "```suggestion\r\n            text=_(\"Show in the editor the file and line where the debugger \"\r\n                   \"is placed\"),\r\n```\r\n\r\nMake this a bit more verbose so that it's easier to understand what it does.", "dataset": "codereview_new", "original_id": 9130, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\ndef setup(self):\n\n        goto_cursor_action = self.create_action(\n            DebuggerWidgetActions.GotoCursor,\n            text=_(\"Show the file and line where the debugger is currently placed in the editor\"),\n            icon=self.create_icon('fromcursor'),\n            triggered=self.goto_current_step,\n            register_shortcut=True\n", "gpt_em_trim": 0, "gpt_bleu_trim": 88.70557, "model_code": " def setup(self): \n goto_cursor_action = self.create_action( \n DebuggerWidgetActions.GotoCursor, \n text=_(\"Show in the editor\"), \n icon=self.create_icon('fromcursor'), \n triggered=self.goto_current_step, \n register_shortcut=True", "model_em_trim": 0, "model_bleu_trim": 81.88734428657867, "error_type_gpt": "Model fallacy"}
{"id": 75, "old": "\n package de.westnordost.streetcomplete.quests.seating\n \n-enum class Seating {\n-    NO,\n-    ONLY_INDOOR,\n-    ONLY_OUTDOOR,\n-    INDOOR_AND_OUTDOOR,\n }", "new": "\n package de.westnordost.streetcomplete.quests.seating\n \n+enum class Seating(val hasOutdoorSeating: Boolean, val hasIndoorSeating: Boolean) {\n+    NO(false, false),\n+    ONLY_INDOOR(false, true),\n+    ONLY_OUTDOOR(true, false),\n+    INDOOR_AND_OUTDOOR(true, true),\n }", "review": "You can give some extra attributes to enum members, which can simplify the logic in `applyAnswerTo` (see my next comment):\r\n\r\n```suggestion\r\nenum class Seating(val hasOutdoorSeating: Boolean, val hasIndoorSeating: Boolean) {\r\n    NO(false, false),\r\n    ONLY_INDOOR(false, true),\r\n    ONLY_OUTDOOR(true, false),\r\n    INDOOR_AND_OUTDOOR(true, true),\r\n}\r\n```", "dataset": "codereview_new", "original_id": 1926, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nenum class Seating(val hasOutdoorSeating: Boolean, val hasIndoorSeating: Boolean) {\n    NO(false, false),\n    ONLY_INDOOR(false, true),\n    ONLY_OUTDOOR(true, false),\n    INDOOR_AND_OUTDOOR(true, true),\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 82.80521, "model_code": " package de.westnordost.streetcomplete.quests.seating", "model_em_trim": 0, "model_bleu_trim": 0.8850576089052096, "error_type_gpt": "Insignificant Omission"}
{"id": 76, "old": " public protocol PledgeViewModelInputs {\n   func riskMessagingViewControllerDismissed(isApplePay: Bool)\n   func scaFlowCompleted(with result: StripePaymentHandlerActionStatusType, error: Error?)\n   func shippingRuleSelected(_ shippingRule: ShippingRule)\n-\n   func stripeTokenCreated(token: String?, error: Error?) -> PKPaymentAuthorizationStatus\n   func submitButtonTapped()\n   func termsOfUseTapped(with: HelpType)", "new": " public protocol PledgeViewModelInputs {\n   func riskMessagingViewControllerDismissed(isApplePay: Bool)\n   func scaFlowCompleted(with result: StripePaymentHandlerActionStatusType, error: Error?)\n   func shippingRuleSelected(_ shippingRule: ShippingRule)\n   func stripeTokenCreated(token: String?, error: Error?) -> PKPaymentAuthorizationStatus\n   func submitButtonTapped()\n   func termsOfUseTapped(with: HelpType)", "review": "can probably remove this newline, format might do it, if not don't worry about it.", "dataset": "codereview_new", "original_id": 339, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\npublic protocol PledgeViewModelInputs {\n  func riskMessagingViewControllerDismissed(isApplePay: Bool)\n  func scaFlowCompleted(with result: StripePaymentHandlerActionStatusType, error: Error?)\n  func shippingRuleSelected(_ shippingRule: ShippingRule)\n  func stripeTokenCreated(token: String?, error: Error?) -> PKPaymentAuthorizationStatus\n  func submitButtonTapped()\n  func termsOfUseTapped(with: HelpType)\n}\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " public protocol PledgeViewModelInputs { \n func riskMessagingViewControllerDismissed(isApplePay: Bool) \n func scaFlowCompleted(with result: StripePaymentHandlerActionStatusType, error: Error) \n func shippingRuleSelected(_ shippingRule: ShippingRule) \n func stripeTokenCreated(token: String?, error: Error) -> PKPaymentAuthorizationStatus \n func submitButtonTapped() \n func termsOfUseTapped(with: HelpType)", "model_em_trim": 0, "model_bleu_trim": 91.3184357233855}
{"id": 77, "old": " MiniAccumuloConfigImpl initialize() {\n       // since there is a small amount of memory, check more frequently for majc... setting may not\n       // be needed in 1.5\n       mergeProp(Property.TSERV_MAJC_DELAY.getKey(), \"3\");\n-      @SuppressWarnings(\"deprecation\")\n-      Property generalClasspaths = Property.GENERAL_CLASSPATHS;\n-      mergeProp(generalClasspaths.getKey(), libDir.getAbsolutePath() + \"/[^.].*[.]jar\");\n       mergeProp(Property.GC_CYCLE_DELAY.getKey(), \"4s\");\n       mergeProp(Property.GC_CYCLE_START.getKey(), \"0s\");\n       mergePropWithRandomPort(Property.MANAGER_CLIENTPORT.getKey());", "new": " MiniAccumuloConfigImpl initialize() {\n       // since there is a small amount of memory, check more frequently for majc... setting may not\n       // be needed in 1.5\n       mergeProp(Property.TSERV_MAJC_DELAY.getKey(), \"3\");\n       mergeProp(Property.GC_CYCLE_DELAY.getKey(), \"4s\");\n       mergeProp(Property.GC_CYCLE_START.getKey(), \"0s\");\n       mergePropWithRandomPort(Property.MANAGER_CLIENTPORT.getKey());", "review": "This property can be removed also", "dataset": "codereview_new", "original_id": 4481, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-and-code", "category_type_user1": "Feature-remove", "category_type_user2": "Documentation-and-code", "gpt_code": "\nMiniAccumuloConfigImpl initialize() {\n      // since there is a small amount of memory, check more frequently for majc... setting may not\n      // be needed in 1.5\n      mergeProp(Property.TSERV_MAJC_DELAY.getKey(), \"3\");\n      @SuppressWarnings(\"deprecation\")\n      Property generalClasspaths = Property.GENERAL_CLASSPATHS;\n      mergeProp(generalClasspaths.getKey(), libDir.getAbsolutePath() + \"/[^.].*[.]jar\");\n      mergeProp(Property.GC_CYCLE_DELAY.getKey(), \"4s\");\n      mergeProp(Property.GC_CYCLE_START.getKey(), \"0s\");\n      mergePropWithRandomPort(Property.MANAGER_CLIENTPORT.getKey());\n      // Remove the following line as per code review\n      // mergeProp(Property.SOME_PROPERTY_TO_BE_REMOVED.getKey(), \"some_value\");\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 69.84811, "model_code": " MiniAccumuloConfigImpl initialize() { \n // since there is a small amount of memory, check more frequently for majc... setting may not \n // be needed in 1.5 \n mergeProp(Property.TSERV_MAJC_DELAY.getKey(), \"3s\"); \n mergeProp(Property.GENERAL_CLASSPATHS, libDir.getAbsolutePath() + \"/[^.].*[.]jar\"); \n mergeProp(Property.GC_CYCLE_DELAY.getKey(), \"4s\"); \n mergeProp(Property.GC_CYCLE_START.getKey(), \"0s\"); \n mergePropWithRandomPort(Property.MANAGER_CLIENTPORT.getKey());", "model_em_trim": 0, "model_bleu_trim": 76.34805731673062, "error_type_gpt": "Need Domain Knowledge"}
{"id": 78, "old": " func setupStmt(db *sql.DB, stmt **sql.Stmt, sql string) {\n \tvar err error\n \tif *stmt, err = db.Prepare(sql); err != nil {\n \t\tlog.Logger().Error(fmt.Sprintf(\"Failed to prepare statement '%s': %s\", sql, err))\n-\t\ttime.Sleep(5 * time.Second)\n-\t\tgo setupStmt(db, stmt, sql)\n \t}\n }\n ", "new": " func setupStmt(db *sql.DB, stmt **sql.Stmt, sql string) {\n \tvar err error\n \tif *stmt, err = db.Prepare(sql); err != nil {\n \t\tlog.Logger().Error(fmt.Sprintf(\"Failed to prepare statement '%s': %s\", sql, err))\n+\t\tgo func() {\n+\t\t\tvar err error\n+\t\t\tfor {\n+\t\t\t\ttime.Sleep(5 * time.Second)\n+\t\t\t\tif *stmt, err = db.Prepare(sql); err == nil {\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\n+\t\t\t\tlog.Logger().Error(fmt.Sprintf(\"Failed to prepare statement '%s': %s\", sql, err))\n+\t\t\t}\n+\t\t}()\n \t}\n }\n ", "review": "Maybe a better approach would be to use a WaitGroup in RadiusStatements::Setup.\r\nSo we can wait until all of the statements succeed before moving forward.\r\n", "dataset": "codereview_new", "original_id": 2931, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfunc setupStmt(db *sql.DB, stmt **sql.Stmt, sql string, wg *sync.WaitGroup) {\n\tvar err error\n\tif *stmt, err = db.Prepare(sql); err != nil {\n\t\tlog.Logger().Error(fmt.Sprintf(\"Failed to prepare statement '%s': %s\", sql, err))\n\t\ttime.Sleep(5 * time.Second)\n\t\tgo setupStmt(db, stmt, sql, wg)\n\t} else {\n\t\twg.Done()\n\t}\n}\n\nfunc (r *RadiusStatements) Setup(db *sql.DB) error {\n\tvar wg sync.WaitGroup\n\twg.Add(3)\n\n\tgo setupStmt(db, &r.insertRadacctStmt, insertRadacctSQL, &wg)\n\tgo setupStmt(db, &r.insertRadcheckStmt, insertRadcheckSQL, &wg)\n\tgo setupStmt(db, &r.insertRadreplyStmt, insertRadreplySQL, &wg)\n\n\twg.Wait()\n\n\treturn nil\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 49.24955, "model_code": " func setupStmt(db *sql.DB, stmt **sql.Stmt, sql string) { \n var err error \n if *stmt, err = db.Prepare(sql); err != nil { \n log.Logger().Error(fmt.Sprintf(\"Failed to prepare statement '%s': %s\", sql, err)) \n time.Sleep(2 * time.Second) \n go setupStmt(db, stmt, sql) \n } \n } \n", "model_em_trim": 0, "model_bleu_trim": 48.51667940945997, "error_type_gpt": "Need Domain Knowledge"}
{"id": 79, "old": "\n // DigestExtensions.swift - Copyright 2022 SwifterSwift\n \n-#if canImport(CoreLocation)\n import CryptoKit\n \n @available(iOS 13.0, macOS 10.15, watchOS 6.0, tvOS 13.0, *)", "new": "\n // DigestExtensions.swift - Copyright 2022 SwifterSwift\n \n+#if canImport(CryptoKit)\n import CryptoKit\n \n @available(iOS 13.0, macOS 10.15, watchOS 6.0, tvOS 13.0, *)", "review": "Wouldn't that be CriptoKit?\r\n```suggestion\r\n#if canImport(CryptoKit)\r\n```", "dataset": "codereview_new", "original_id": 22, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\n// DigestExtensions.swift - Copyright 2022 SwifterSwift\n\n#if canImport(CryptoKit)\nimport CryptoKit\n\n@available(iOS 13.0, macOS 10.15, watchOS 6.0, tvOS 13.0, *)\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " // DigestExtensions.swift - Copyright 2022 SwifterSwift \n #if canImport(CryptoKit) \n @available(iOS 13.0, macOS 10.15, watchOS 6.0, tvOS 13.0, *)", "model_em_trim": 0, "model_bleu_trim": 91.50007466174905}
{"id": 80, "old": " void Interface::StatusWindow::TimerEventProcessing()\n \n void Interface::StatusWindow::RedrawStatusIfNeeded( const uint32_t progressValue )\n {\n     turn_progress = progressValue;\n \n     interface.Redraw( REDRAW_STATUS );\n \n     if ( Game::validateAnimationDelay( Game::MAPS_DELAY ) ) {\n-        // Process events if any before rendering a frame. For instance, updating a mouse cursor position.\n-        LocalEvent::Get().HandleEvents( false );\n-\n         uint32_t & frame = Game::MapsAnimationFrame();\n         ++frame;\n \n-        interface.GetGameArea().SetRedraw();\n-\n-        interface.Redraw();\n         fheroes2::Display::instance().render();\n     }\n }", "new": " void Interface::StatusWindow::TimerEventProcessing()\n \n void Interface::StatusWindow::RedrawStatusIfNeeded( const uint32_t progressValue )\n {\n+    // Process events if any before rendering a frame. For instance, updating a mouse cursor position.\n+    LocalEvent::Get().HandleEvents( false );\n+\n     turn_progress = progressValue;\n \n     interface.Redraw( REDRAW_STATUS );\n \n     if ( Game::validateAnimationDelay( Game::MAPS_DELAY ) ) {\n         uint32_t & frame = Game::MapsAnimationFrame();\n         ++frame;\n \n+        interface.Redraw( REDRAW_GAMEAREA );\n         fheroes2::Display::instance().render();\n     }\n }", "review": "This can be simplified to `interface.Redraw( REDRAW_GAMEAREA );`, there is not need for separate `SetRedraw()` call. Or we can do the following:\r\n\r\n```cpp\r\ninterface.SetRedraw( REDRAW_STATUS );\r\n\r\nif ( Game::validateAnimationDelay( Game::MAPS_DELAY ) ) {\r\n[...]\r\n  interface.SetRedraw( REDRAW_GAMEAREA ); // Or interface.GetGameArea().SetRedraw(), which is effectively the same\r\n}\r\n\r\ninterface.Redraw();\r\nfheroes2::Display::instance().render();\r\n```\r\n\r\nWhat do you think?", "dataset": "codereview_new", "original_id": 4610, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-and-code", "category_type_user1": "Feature-modify", "category_type_user2": "Documentation-and-code", "gpt_code": "\nvoid Interface::StatusWindow::TimerEventProcessing()\n\nvoid Interface::StatusWindow::RedrawStatusIfNeeded( const uint32_t progressValue )\n{\n    turn_progress = progressValue;\n\n    interface.SetRedraw( REDRAW_STATUS );\n\n    if ( Game::validateAnimationDelay( Game::MAPS_DELAY ) ) {\n        // Process events if any before rendering a frame. For instance, updating a mouse cursor position.\n        LocalEvent::Get().HandleEvents( false );\n\n        uint32_t & frame = Game::MapsAnimationFrame();\n        ++frame;\n\n        interface.SetRedraw( REDRAW_GAMEAREA );\n\n    }\n\n    interface.Redraw();\n    fheroes2::Display::instance().render();\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 89.78752, "model_code": " void Interface::StatusWindow::TimerEventProcessing() \n { \n if ( Game::validateAnimationDelay( Game::MAPS_DELAY ) ) { \n // Process events if any before rendering a frame. For instance, updating a mouse cursor position. \n LocalEvent::Get().HandleEvents( false ); \n fheroes2::Display::instance().render(); \n } \n }", "model_em_trim": 0, "model_bleu_trim": 46.20944970130947}
{"id": 81, "old": "\n #'\n #' @param .f A function to modify, specified in one of the following ways:\n #'   * A named function, e.g. `mean`.\n-#'   * An anonymous function, e.g. `function(x) x + 1`.\n-#'   * A lambda function, e.g. `\\(a) a + 1`.\n #'   * A formula, e.g. `~ .x + 1`. Only appropriate for simple tasks.\n #' @param otherwise Default value to use when an error occurs.\n #' @param quiet Hide errors (`TRUE`, the default), or display them", "new": "\n #'\n #' @param .f A function to modify, specified in one of the following ways:\n #'   * A named function, e.g. `mean`.\n+#'   * An anonymous function, e.g. `\\(x) x + 1` or `function(x) x + 1`.\n #'   * A formula, e.g. `~ .x + 1`. Only appropriate for simple tasks.\n #' @param otherwise Default value to use when an error occurs.\n #' @param quiet Hide errors (`TRUE`, the default), or display them", "review": "Just so that we don't imply that the shorthand variant needs different naming conventions:\r\n\r\n```suggestion\r\n#'   * A lambda function, e.g. `\\(x) x + 1`.\r\n```", "dataset": "codereview_new", "original_id": 2338, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n#'\n#' @param .f A function to modify, specified in one of the following ways:\n#'   * A named function, e.g. `mean`.\n#'   * An anonymous function, e.g. `function(x) x + 1`.\n#'   * A lambda function, e.g. `\\(x) x + 1`.\n#'   * A formula, e.g. `~ .x + 1`. Only appropriate for simple tasks.\n#' @param otherwise Default value to use when an error occurs.\n#' @param quiet Hide errors (`TRUE`, the default), or display them\n", "gpt_em_trim": 0, "gpt_bleu_trim": 89.93444, "model_code": " #' \n #' @param .f A function to modify, specified in one of the following ways: \n #'   * An anonymous function, e.g. `function(x) x + 1`. \n #'   * A lambda function, e.g. `\\(a) a + 1`. \n #'   * A formula, e.g. `~ .x + 1`. Only appropriate for simple tasks. \n #' @param otherwise Default value to use when an error occurs. \n #' @param quiet Hide errors (`TRUE`, the default), or display them", "model_em_trim": 0, "model_bleu_trim": 89.96693600459312, "error_type_gpt": "Need Domain Knowledge"}
{"id": 82, "old": " object RegField\n     val numFullBytes = reg.getWidth/8\n     val numPartialBytes  = if ((reg.getWidth % 8) > 0) 1 else 0\n     val numPadBytes = numBytes - numFullBytes - numPartialBytes\n-    val pad = reg | 0.U( (8*numBytes).W)\n     val oldBytes = VecInit.tabulate(numBytes) { i => pad(8*(i+1)-1, 8*i) }\n     val newBytes = WireDefault(oldBytes)\n     val valids = WireDefault(VecInit.fill(numBytes) { false.B })", "new": " object RegField\n     val numFullBytes = reg.getWidth/8\n     val numPartialBytes  = if ((reg.getWidth % 8) > 0) 1 else 0\n     val numPadBytes = numBytes - numFullBytes - numPartialBytes\n+    val pad = reg | 0.U((8*numBytes).W)\n     val oldBytes = VecInit.tabulate(numBytes) { i => pad(8*(i+1)-1, 8*i) }\n     val newBytes = WireDefault(oldBytes)\n     val valids = WireDefault(VecInit.fill(numBytes) { false.B })", "review": "```suggestion\r\n    val pad = reg | 0.U((8*numBytes).W)\r\n```", "dataset": "codereview_new", "original_id": 3281, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nobject RegField\n    val numFullBytes = reg.getWidth/8\n    val numPartialBytes  = if ((reg.getWidth % 8) > 0) 1 else 0\n    val numPadBytes = numBytes - numFullBytes - numPartialBytes\n    val pad = reg.pad((8*numBytes).W)\n    val oldBytes = VecInit.tabulate(numBytes) { i => pad(8*(i+1)-1, 8*i) }\n    val newBytes = WireDefault(oldBytes)\n    val valids = WireDefault(VecInit.fill(numBytes) { false.B })\n", "gpt_em_trim": 0, "gpt_bleu_trim": 95.05137, "model_code": " object RegField \n val numFullBytes = reg.getWidth/8 \n val numPartialBytes  = if ((reg.getWidth % 8) > 0) 1 else 0 \n val numPadBytes = numBytes - numFullBytes - numPartialBytes \n val pad = reg | 0.U((8*numBytes).W) \n val oldBytes = VecInit.tabulate(numBytes) { i => pad(8*(i+1)-1, 8*i) } \n val newBytes = WireDefault(oldBytes) \n val valids = WireDefault(VecInit.fill(numBytes) { false.B })", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Model fallacy"}
{"id": 83, "old": "\n #'  remove the original date variables by default. Set `keep_original_cols`\n #'  to `FALSE` to remove them.\n #'\n-#'  See [step_time()] if you want to calculate features that are small than\n #'  days.\n #'\n #'  # Tidying", "new": "\n #'  remove the original date variables by default. Set `keep_original_cols`\n #'  to `FALSE` to remove them.\n #'\n+#'  See [step_time()] if you want to calculate features that are smaller than\n #'  days.\n #'\n #'  # Tidying", "review": "```suggestion\r\n#'  See [step_time()] if you want to calculate features that are smaller than\r\n#'  days.\r\n```\r\n\r\nThen redocument", "dataset": "codereview_new", "original_id": 2769, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "\n\n#'  remove the original date variables by default. Set `keep_original_cols`\n#'  to `FALSE` to remove them.\n#'\n#'  See [step_time()] if you want to calculate features that are smaller than\n#'  days.\n#'\n#'  # Tidying\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " #'  remove the original date variables by default. Set `keep_original_cols` \n #'  to `FALSE` to remove them. \n #' \n #'  See [step_time()] if you want to calculate features that are smaller than \n #'  days. \n #' \n #'  # Tidying", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 84, "old": " private function getTokenFromSession(string $sessionKey): ?TokenInterface\n         return $token;\n     }\n \n-    private function validatePreviewLink(FrontendPreviewToken $token): bool\n     {\n         $id = $token->getPreviewLinkId();\n ", "new": " private function getTokenFromSession(string $sessionKey): ?TokenInterface\n         return $token;\n     }\n \n+    private function isValidPreviewLink(FrontendPreviewToken $token): bool\n     {\n         $id = $token->getPreviewLinkId();\n ", "review": "This method should probably be called `isValidPreviewLink`.", "dataset": "codereview_new", "original_id": 4117, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\nprivate function getTokenFromSession(string $sessionKey): ?TokenInterface\n{\n    return $token;\n}\n\nprivate function isValidPreviewLink(FrontendPreviewToken $token): bool\n{\n    $id = $token->getPreviewLinkId();\n    // Add validation logic here\n    return true;\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 93.34976, "model_code": " private function getTokenFromSession(string $sessionKey): ?TokenInterface \n return $token; \n } \n \n private function isValidPreviewLink(FrontendPreviewToken $token): bool \n { \n $id = $token->getPreviewLinkId(); \n", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Reasonable Improvement"}
{"id": 85, "old": " class ServiceBuilder extends chromium.ServiceBuilder {\n       console.log(\n         ` The ChromeDriver could not be found on the current PATH, trying Selenium Manager`\n       )\n-      exe = driverLocation(Browser.CHROME)\n-    }\n \n-    if (!exe) {\n-      throw Error(`Unable to obtain driver using Selenium Manager`)\n     }\n     super(exe)\n   }\n }", "new": " class ServiceBuilder extends chromium.ServiceBuilder {\n       console.log(\n         ` The ChromeDriver could not be found on the current PATH, trying Selenium Manager`\n       )\n \n+      try {\n+        exe = driverLocation(Browser.CHROME)\n+      } catch (e) {\n+        throw Error(\n+          `The ChromeDriver could not be found on the current PATH.\n+        Please download the latest version of the ChromeDriver\n+        from http://chromedriver.storage.googleapis.com/index.html\n+        and ensure it can be found on your PATH.`\n+        )\n+      }\n     }\n+\n     super(exe)\n   }\n }", "review": "we want to throw/catch this error so that people don't even know we tried using the Selenium Manager.", "dataset": "codereview_new", "original_id": 7117, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass ServiceBuilder extends chromium.ServiceBuilder {\n    try {\n        exe = driverLocation(Browser.CHROME)\n    } catch (error) {\n        throw new Error(`Unable to obtain driver using Selenium Manager`)\n    }\n    super(exe)\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 16.62772, "model_code": " class ServiceBuilder extends chromium.ServiceBuilder { \n console.log( \n ` The ChromeDriver could not be found on the current PATH, trying Selenium Manager` \n ) \n exe = driverLocation(Browser.CHROME) \n } \n super(exe) \n } \n }", "model_em_trim": 0, "model_bleu_trim": 24.15788729632266, "error_type_gpt": "Need Domain Knowledge"}
{"id": 86, "old": " public class MigrationControlManager {\n         zkMigrationState = new TimelineObject<>(snapshotRegistry, ZkMigrationState.NONE);\n     }\n \n-    public ZkMigrationState zkMigrationState() {\n         return zkMigrationState.get();\n     }\n ", "new": " public class MigrationControlManager {\n         zkMigrationState = new TimelineObject<>(snapshotRegistry, ZkMigrationState.NONE);\n     }\n \n+    ZkMigrationState zkMigrationState() {\n         return zkMigrationState.get();\n     }\n ", "review": "does this need to be a public function or can it be package-private", "dataset": "codereview_new", "original_id": 8355, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\npublic class MigrationControlManager {\n        private final TimelineObject<ZkMigrationState> zkMigrationState;\n\n        MigrationControlManager(SnapshotRegistry snapshotRegistry) {\n            zkMigrationState = new TimelineObject<>(snapshotRegistry, ZkMigrationState.NONE);\n        }\n\n        ZkMigrationState getZkMigrationState() {\n            return zkMigrationState.get();\n        }\n    }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 75.65283, "model_code": " public class MigrationControlManager { \n zkMigrationState = new TimelineObject<>(snapshotRegistry, ZkMigrationState.NONE); \n }", "model_em_trim": 0, "model_bleu_trim": 52.20457767610161, "error_type_gpt": "Unclear Location"}
{"id": 87, "old": " void FixAmoebaBiTorsion::init()\n   // error check that PairAmoeba or PairHiippo exist\n \n   pair = nullptr;\n-  pair = force->pair_match(\"amoeba\",1,0);\n-  if (!pair) pair = force->pair_match(\"amoeba/gpu\",1,0);\n-  if (!pair) pair = force->pair_match(\"hippo\",1,0);\n-  if (!pair) pair = force->pair_match(\"hippo/gpu\",1,0);\n \n   if (!pair)\n     error->all(FLERR,\"Cannot use fix amoeba/bitorsion w/out pair amoeba/hippo\");", "new": " void FixAmoebaBiTorsion::init()\n   // error check that PairAmoeba or PairHiippo exist\n \n   pair = nullptr;\n+  pair = force->pair_match(\"^amoeba\",0,0);\n+  if (!pair) pair = force->pair_match(\"^hippo\",0,0);\n \n   if (!pair)\n     error->all(FLERR,\"Cannot use fix amoeba/bitorsion w/out pair amoeba/hippo\");", "review": "This section could also be done as:\r\n```\r\npair = force->pair_match(\"^amoeba\",0,0);\r\nif (!pair) pair = force->pair_match(\"^hippo\",0,0);\r\n```", "dataset": "codereview_new", "original_id": 6951, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nvoid FixAmoebaBiTorsion::init()\n  // error check that PairAmoeba or PairHiippo exist\n\n  pair = nullptr;\n  pair = force->pair_match(\"^amoeba\",0,0);\n  if (!pair) pair = force->pair_match(\"^hippo\",0,0);\n\n  if (!pair)\n    error->all(FLERR,\"Cannot use fix amoeba/bitorsion w/out pair amoeba/hippo\");\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " void FixAmoebaBiTorsion::init() \n // error check that PairAmoeba or PairHiippo exist \n if (!pair) \n error->all(FLERR,\"Cannot use fix amoeba/bitorsion w/out pair amoeba/hippo\");", "model_em_trim": 0, "model_bleu_trim": 34.747327837331845}
{"id": 88, "old": " static const char *help_msg_c[] = {\n \t\"c\", \" [string]\", \"compare a plain with escaped chars string\",\n \t\"c*\", \" [string]\", \"same as above, but printing r2 commands instead\",\n \t\"c1\", \" [addr]\", \"compare byte at addr with current offset\",\n-\t\"c2\", \"* [value]\", \"compare word at offset with given value\",\n-\t\"c4\", \"* [value]\", \"compare doubleword at offset with given value\",\n-\t\"c8\", \"* [value]\", \"compare quadword at offset with given value\",\n \t\"cat\", \" [file]\", \"show contents of file (see pwd, ls)\",\n \t\"cc\", \" [at]\", \"compares in two hexdump columns of block size\",\n \t\"ccc\", \" [at]\", \"same as above, but only showing different lines\",", "new": " static const char *help_msg_c[] = {\n \t\"c\", \" [string]\", \"compare a plain with escaped chars string\",\n \t\"c*\", \" [string]\", \"same as above, but printing r2 commands instead\",\n \t\"c1\", \" [addr]\", \"compare byte at addr with current offset\",\n+\t\"c2\", \"[*] [value]\", \"compare word at offset with given value\",\n+\t\"c4\", \"[*] [value]\", \"compare doubleword at offset with given value\",\n+\t\"c8\", \"[*] [value]\", \"compare quadword at offset with given value\",\n \t\"cat\", \" [file]\", \"show contents of file (see pwd, ls)\",\n \t\"cc\", \" [at]\", \"compares in two hexdump columns of block size\",\n \t\"ccc\", \" [at]\", \"same as above, but only showing different lines\",", "review": "the * is optional\r\n```suggestion\r\n\t\"c2\", \"[*] [value]\", \"compare word at offset with given value\",\r\n```", "dataset": "codereview_new", "original_id": 8018, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nstatic const char *help_msg_c[] = {\n\t\"c\", \" [string]\", \"compare a plain with escaped chars string\",\n\t\"c*\", \" [string]\", \"same as above, but printing r2 commands instead\",\n\t\"c1\", \" [addr]\", \"compare byte at addr with current offset\",\n\t\"c2\", \"[*] [value]\", \"compare word at offset with given value\",\n\t\"c4\", \"* [value]\", \"compare doubleword at offset with given value\",\n\t\"c8\", \"* [value]\", \"compare quadword at offset with given value\",\n\t\"cat\", \" [file]\", \"show contents of file (see pwd, ls)\",\n\t\"cc\", \" [at]\", \"compares in two hexdump columns of block size\",\n\t\"ccc\", \" [at]\", \"same as above, but only showing different lines\",\n", "gpt_em_trim": 0, "gpt_bleu_trim": 95.93971, "model_code": " static const char *help_msg_c[] = { \n \"c\", \" [string]\", \"compare a plain with escaped chars string\", \n \"c*\", \" [string]\", \"same as above, but printing r2 commands instead\", \n \"cat\", \" [file]\", \"show contents of file (see pwd, ls)\", \n \"cc\", \" [at]\", \"compares in two hexdump columns of block size\", \n \"ccc\", \" [at]\", \"same as above, but only showing different lines\",", "model_em_trim": 0, "model_bleu_trim": 48.50141390581555, "error_type_gpt": "Need Domain Knowledge"}
{"id": 89, "old": " func getTerraformProviderTypes(ctx context.Context, k8sClient client.Client) ([]\n \tlegacyDefs := &v1beta1.ComponentDefinitionList{}\n \terr := k8sClient.List(ctx, legacyDefs, client.InNamespace(types.DefaultKubeVelaNS),\n \t\tclient.MatchingLabels{definition.UserPrefix + definition.DefinitionType: types.TerraformProvider})\n-\tif err != nil && kerrors.IsNotFound(err) {\n \t\treturn nil, err\n \t}\n \tdefs := &v1beta1.ComponentDefinitionList{}\n \terr = k8sClient.List(ctx, defs, client.InNamespace(types.DefaultKubeVelaNS),\n \t\tclient.MatchingLabels{definition.DefinitionType: types.TerraformProvider})\n-\tif err != nil {\n \t\treturn nil, err\n \t}\n ", "new": " func getTerraformProviderTypes(ctx context.Context, k8sClient client.Client) ([]\n \tlegacyDefs := &v1beta1.ComponentDefinitionList{}\n \terr := k8sClient.List(ctx, legacyDefs, client.InNamespace(types.DefaultKubeVelaNS),\n \t\tclient.MatchingLabels{definition.UserPrefix + definition.DefinitionType: types.TerraformProvider})\n+\tif err != nil && !kerrors.IsNotFound(err) {\n \t\treturn nil, err\n \t}\n \tdefs := &v1beta1.ComponentDefinitionList{}\n \terr = k8sClient.List(ctx, defs, client.InNamespace(types.DefaultKubeVelaNS),\n \t\tclient.MatchingLabels{definition.DefinitionType: types.TerraformProvider})\n+\tif err != nil && !kerrors.IsNotFound(err) {\n \t\treturn nil, err\n \t}\n ", "review": "```suggestion\r\n\tif err != nil && !kerrors.IsNotFound(err) {\r\n```", "dataset": "codereview_new", "original_id": 7022, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfunc getTerraformProviderTypes(ctx context.Context, k8sClient client.Client) ([]*v1beta1.ComponentDefinition, error) {\n\tdefs := &v1beta1.ComponentDefinitionList{}\n\terr := k8sClient.List(ctx, defs, client.InNamespace(types.DefaultKubeVelaNS),\n\t\tclient.MatchingLabels{definition.DefinitionType: types.TerraformProvider})\n\tif err != nil && !kerrors.IsNotFound(err) {\n\t\treturn nil, err\n\t}\n\tif len(defs.Items) == 0 {\n\t\tlegacyDefs := &v1beta1.ComponentDefinitionList{}\n\t\terr = k8sClient.List(ctx, legacyDefs, client.InNamespace(types.DefaultKubeVelaNS),\n\t\t\tclient.MatchingLabels{definition.UserPrefix + definition.DefinitionType: types.TerraformProvider})\n\t\tif err != nil && !kerrors.IsNotFound(err) {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn legacyDefs.Items, nil\n\t}\n\treturn defs.Items, nil\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 86.33721, "model_code": " func getTerraformProviderTypes(ctx context.Context, k8sClient client.Client) ([] \n legacyDefs := &v1beta1.ComponentDefinitionList{} \n err := k8sClient.List(ctx, legacyDefs, client.InNamespace(types.DefaultKubeVelaNS), \n client.MatchingLabels{definition.UserPrefix + definition.DefinitionType: types.TerraformProvider}) \n if err != nil && !kerrors.IsNotFound(err) { \n return nil, err \n } \n defs := &v1beta1.ComponentDefinitionList{} \n err = k8sClient.List(ctx, defs, client.InNamespace(types.DefaultKubeVelaNS), \n client.MatchingLabels{definition.DefinitionType: types.TerraformProvider}) \n if err != nil { \n return nil, err \n }", "model_em_trim": 0, "model_bleu_trim": 92.48953662733746, "error_type_gpt": "Need Domain Knowledge"}
{"id": 90, "old": " internal object UncivServer {\n         args.forEach { arg ->\n             when {\n                 arg.startsWith(\"-port=\") -> {\n-                    try {\n-                        with(arg.removePrefix(\"-port=\").toInt()) {\n-                            if (this !in 1.rangeTo(65535)) println(\"'port' must in 1-65535\")\n-                            else serverPort = this\n-                        }\n-                    } catch (e: NumberFormatException) {\n-                        println(\"'port' must be a positive integer\")\n-                    }\n                 }\n             }\n         }", "new": " internal object UncivServer {\n         args.forEach { arg ->\n             when {\n                 arg.startsWith(\"-port=\") -> {\n+                    val port = arg.removePrefix(\"-port=\").toIntOrNull() ?: 0\n+                    if (port in 1.rangeTo(65535)) serverPort = port\n+                    else println(\"'port' must be between 1 and 65535\")\n                 }\n             }\n         }", "review": "If we already know that it might not be a number, we can use toIntOrNull and get rid of try/catch", "dataset": "codereview_new", "original_id": 1442, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ninternal object UncivServer {\n        args.forEach { arg ->\n            when {\n                arg.startsWith(\"-port=\") -> {\n                    arg.removePrefix(\"-port=\").toIntOrNull()?.let { port ->\n                        if (port !in 1.rangeTo(65535)) println(\"'port' must in 1-65535\")\n                        else serverPort = port\n                    } ?: println(\"'port' must be a positive integer\")\n                }\n            }\n        }\n    }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 62.50065, "model_code": " internal object UncivServer { \n args.forEach { arg -> \n when { \n arg.startsWith(\"-port=\") -> { \n if (this != in 1.rangeTo(65535)) println(\"'port' must in 1-65535\") \n else serverPort = this \n } \n } \n }", "model_em_trim": 0, "model_bleu_trim": 52.503696509423534, "error_type_gpt": "Need Domain Knowledge"}
{"id": 91, "old": " void model::setup_distconv()\n     }\n   }\n   if (m_comm->am_world_master()) {\n-    std::cout << \"\\nDistconv-enabled layers:\\n\\t\" << dc_enabled.str() << std::endl << std::endl;\n-    std::cout << \"Distconv-disabled layers:\\n\\t\" << dc_disabled.str() << std::endl;\n-    std::stringstream ss;\n-    print_parallel_strategy_header(ss);\n-    std::cout << \"\\nParallel Strategy description - \" << ss.str() << std::endl;\n   }\n   setup_distributions();\n   print_distributions();", "new": " void model::setup_distconv()\n     }\n   }\n   if (m_comm->am_world_master()) {\n+    std::cout << \"\\nDistconv-enabled layers:\\n\\t\" << dc_enabled.str() << \"\\n\\n\"\n+              << \"Distconv-disabled layers:\\n\\t\" << dc_disabled.str() << \"\\n\\n\"\n+              << \"Parallel Strategy description - \";\n+    print_parallel_strategy_header(std::cout);\n+    endl(std::cout);\n   }\n   setup_distributions();\n   print_distributions();", "review": "```suggestion\r\n    std::cout << \"\\nDistconv-enabled layers:\\n\\t\" << dc_enabled.str() << \"\\n\\n\"\r\n              << \"Distconv-disabled layers:\\n\\t\" << dc_disabled.str() << \"\\n\\n\"\r\n              << \"Parallel Strategy description - \";\r\n    print_parallel_strategy_header(std::cout);\r\n    endl(std::cout);\r\n```", "dataset": "codereview_new", "original_id": 5148, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nvoid model::setup_distconv()\n    }\n  }\n  if (m_comm->am_world_master()) {\n    std::cout << \"\\nDistconv-enabled layers:\\n\\t\" << dc_enabled.str() << \"\\n\\n\"\n              << \"Distconv-disabled layers:\\n\\t\" << dc_disabled.str() << \"\\n\\n\"\n              << \"Parallel Strategy description - \";\n    print_parallel_strategy_header(std::cout);\n    std::endl(std::cout);\n  }\n  setup_distributions();\n  print_distributions();\n", "gpt_em_trim": 0, "gpt_bleu_trim": 96.73677, "model_code": " void model::setup_distconv() \n } \n } \n setup_distributions(); \n print_distributions();", "model_em_trim": 0, "model_bleu_trim": 0.8695199372502179, "error_type_gpt": "Reasonable Improvement"}
{"id": 92, "old": " func (f *HotStuffFactory) CreateModules(\n \tpayloads storage.ClusterPayloads,\n \tupdater module.Finalizer,\n ) (*consensus.HotstuffModules, module.HotstuffMetrics, error) {\n-\n-\tlog := f.createLogger(cluster)\n-\n \t// setup metrics/logging with the new chain ID\n \tmetrics := f.createMetrics(cluster.ChainID())\n \tnotifier := pubsub.NewDistributor()\n \tfinalizationDistributor := pubsub.NewFinalizationDistributor()", "new": " func (f *HotStuffFactory) CreateModules(\n \tpayloads storage.ClusterPayloads,\n \tupdater module.Finalizer,\n ) (*consensus.HotstuffModules, module.HotstuffMetrics, error) {\n \t// setup metrics/logging with the new chain ID\n+\tlog := f.createLogger(cluster)\n \tmetrics := f.createMetrics(cluster.ChainID())\n \tnotifier := pubsub.NewDistributor()\n \tfinalizationDistributor := pubsub.NewFinalizationDistributor()", "review": "```suggestion\r\n\t// setup metrics/logging with the new chain ID\r\n\tlog := f.createLogger(cluster)\r\n```", "dataset": "codereview_new", "original_id": 5436, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-swap", "category_type_user1": "Refactoring-swap", "category_type_user2": "Refactoring-swap", "gpt_code": "\nfunc (f *HotStuffFactory) CreateModules(\n\tpayloads storage.ClusterPayloads,\n\tupdater module.Finalizer,\n) (*consensus.HotstuffModules, module.HotstuffMetrics, error) {\n\n\t// setup logging with the new chain ID\n\tlog := f.createLogger(cluster)\n\n\t// setup metrics with the new chain ID\n\tmetrics := f.createMetrics(cluster.ChainID())\n\tnotifier := pubsub.NewDistributor()\n\tfinalizationDistributor := pubsub.NewFinalizationDistributor()\n", "gpt_em_trim": 0, "gpt_bleu_trim": 87.59868, "model_code": " func (f *HotStuffFactory) CreateModules( \n payloads storage.ClusterPayloads, \n updater module.Finalizer, \n ) (*consensus.HotstuffModules, module.HotstuffMetrics, error) { \n notifier := pubsub.NewDistributor() \n finalizationDistributor := pubsub.NewFinalizationDistributor()", "model_em_trim": 0, "model_bleu_trim": 49.33855616736291, "error_type_gpt": "Unclear Location"}
{"id": 93, "old": " import de.westnordost.streetcomplete.ktx.containsAnyKey\n     when (map[\"tourism\"]) {\n         \"information\" -> return R.drawable.ic_pin_information\n     }\n-    if ((map.containsKey(\"addr:housenumber\") || map.containsKey(\"addr:housename\"))\n         && !map.containsAnyKey(\"amenity\", \"craft\", \"leisure\", \"office\", \"shop\", \"tourism\")){\n         return R.drawable.ic_none\n     }", "new": " import de.westnordost.streetcomplete.ktx.containsAnyKey\n     when (map[\"tourism\"]) {\n         \"information\" -> return R.drawable.ic_pin_information\n     }\n+    if (map.containsKey(\"addr:housenumber\") || map.containsKey(\"addr:housename\") ||\n+            map.containsKey(\"addr:conscriptionnumber\") || map.containsKey(\"addr:streetnumber\")\n         && !map.containsAnyKey(\"amenity\", \"craft\", \"leisure\", \"office\", \"shop\", \"tourism\")){\n         return R.drawable.ic_none\n     }", "review": "maybe add \"healthcare\" to that list. (It is in the list for AddOpeningHours, AddWheelchairAccessBusiness, etc too)", "dataset": "codereview_new", "original_id": 1919, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nimport de.westnordost.streetcomplete.ktx.containsAnyKey\n    when (map[\"tourism\"]) {\n        \"information\" -> return R.drawable.ic_pin_information\n    }\n    if ((map.containsKey(\"addr:housenumber\") || map.containsKey(\"addr:housename\"))\n        && !map.containsAnyKey(\"amenity\", \"craft\", \"leisure\", \"office\", \"shop\", \"tourism\", \"healthcare\")){\n        return R.drawable.ic_none\n    }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 78.97004, "model_code": " import de.westnordost.streetcomplete.ktx.containsAnyKey \n when (map[\"tourism\"]) { \n \"information\" -> return R.drawable.ic_pin_information \n } \n if ((map.containsKey(\"addr:housenumber\") || map.containsKey(\"addr:housename\")) \n && !map.containsAnyKey(\"amenity\", \"craft\", \"leisure\", \"office\", \"shop\", \"tourism\", \"healthcare\")){ \n return R.drawable.ic_none \n }", "model_em_trim": 0, "model_bleu_trim": 78.97003823557182}
{"id": 94, "old": " class Connection : BaseAsyncTask<Connection.Payload, Any, Connection.Payload>()\n          * @return the original payload\n          */\n         private fun genericError(data: Payload): Payload {\n-            data.apply {\n                 success = false\n                 resultType = GENERIC_ERROR\n                 result = arrayOfNulls(0)\n             }\n-            return data\n         }\n \n         @Synchronized // #7108: AsyncTask", "new": " class Connection : BaseAsyncTask<Connection.Payload, Any, Connection.Payload>()\n          * @return the original payload\n          */\n         private fun genericError(data: Payload): Payload {\n+            return data.apply {\n                 success = false\n                 resultType = GENERIC_ERROR\n                 result = arrayOfNulls(0)\n             }\n         }\n \n         @Synchronized // #7108: AsyncTask", "review": "should be able to `return data.apply {`", "dataset": "codereview_new", "original_id": 1111, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass Connection : BaseAsyncTask<Connection.Payload, Any, Connection.Payload>() {\n        /**\n         * Updates the payload with generic error information.\n         *\n         * @param data the payload to update\n         * @return the updated payload\n         */\n        private fun genericError(data: Payload): Payload {\n            return data.apply {\n                success = false\n                resultType = GENERIC_ERROR\n                result = arrayOfNulls(0)\n            }\n        }\n\n        @Synchronized // #7108: AsyncTask\n", "gpt_em_trim": 0, "gpt_bleu_trim": 69.76923, "model_code": " class Connection : BaseAsyncTask<Connection.Payload, Any, Connection.Payload>() \n * @return the original payload \n */ \n @Synchronized // #7108: AsyncTask", "model_em_trim": 0, "model_bleu_trim": 37.140963459258, "error_type_gpt": "Unclear Changes"}
{"id": 95, "old": " func (d *Driver) Start() error {\n \t\t\t\"-nic\", fmt.Sprintf(\"user,model=virtio,hostfwd=tcp::%d-:22,hostfwd=tcp::%d-:2376,hostname=%s\", d.SSHPort, d.EnginePort, d.GetMachineName()),\n \t\t)\n \tcase \"socket\":\n-\t\tstartCmd = append(startCmd,\n-\t\t\t\"-nic\", fmt.Sprintf(\"socket,model=virtio,sock=%s\", d.NetworkSocket),\n-\t\t)\n \tcase \"tap\":\n \t\tstartCmd = append(startCmd,\n \t\t\t\"-nic\", fmt.Sprintf(\"tap,model=virtio,ifname=%s,script=no,downscript=no\", d.NetworkInterface),", "new": " func (d *Driver) Start() error {\n \t\t\t\"-nic\", fmt.Sprintf(\"user,model=virtio,hostfwd=tcp::%d-:22,hostfwd=tcp::%d-:2376,hostname=%s\", d.SSHPort, d.EnginePort, d.GetMachineName()),\n \t\t)\n \tcase \"socket\":\n+\t\t// TODO: finalize actual socket_vmnet network flags.\n+\t\treturn errors.New(\"qemu socket_vmnet network flags are not yet implemented\")\n \tcase \"tap\":\n \t\tstartCmd = append(startCmd,\n \t\t\t\"-nic\", fmt.Sprintf(\"tap,model=virtio,ifname=%s,script=no,downscript=no\", d.NetworkInterface),", "review": "Hi @AkihiroSuda, in your experience, do these parameters for the `-nic` flag look correct for socket_vmnet with minikube socket network?", "dataset": "codereview_new", "original_id": 7961, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\nfunc (d *Driver) Start() error {\n\tswitch d.NetworkMode {\n\tcase \"user\":\n\t\tstartCmd = append(startCmd,\n\t\t\t\"-nic\", fmt.Sprintf(\"user,model=virtio,hostfwd=tcp::%d-:22,hostfwd=tcp::%d-:2376,hostname=%s\", d.SSHPort, d.EnginePort, d.GetMachineName()),\n\t\t)\n\tcase \"socket\":\n\t\tstartCmd = append(startCmd,\n\t\t\t\"-nic\", fmt.Sprintf(\"socket,model=virtio,sock=%s\", d.NetworkSocket),\n\t\t\t\"-netdev\", fmt.Sprintf(\"socket,id=net0,listen=%s\", d.NetworkSocket),\n\t\t\t\"-device\", \"virtio-net-pci,netdev=net0\",\n\t\t)\n\tcase \"tap\":\n\t\tstartCmd = append(startCmd,\n\t\t\t\"-nic\", fmt.Sprintf(\"tap,model=virtio,ifname=%s,script=no,downscript=no\", d.NetworkInterface),\n\t\t)\n\t}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 57.77971, "model_code": " func (d *Driver) Start() error { \n \"-nic\", fmt.Sprintf(\"user,model=virtio,hostfwd=tcp::%d-:22,hostfwd=tcp::%d-:2376,hostname=%s\", d.SSHPort, d.EnginePort, d.GetMachineName()), \n ) \n case \"tap\": \n startCmd = append(startCmd, \n \"-nic\", fmt.Sprintf(\"tap,model=virtio,ifname=%s,script=no,downscript=no\", d.NetworkInterface),", "model_em_trim": 0, "model_bleu_trim": 74.33337300538264, "error_type_gpt": "Need Domain Knowledge"}
{"id": 96, "old": " func (sender *peerAuthenticationSender) execute() (error, bool) {\n \n \thardforkPayload, isTriggered := sender.getHardforkPayload()\n \tpayload := &heartbeat.Payload{\n-\t\tTimestamp:          time.Now().Unix(),\n-\t\tHardforkMessage:    string(hardforkPayload),\n-\t\tNumTrieNodesSynced: 0, // sent through heartbeat v2 message\n \t}\n \tpayloadBytes, err := sender.marshaller.Marshal(payload)\n \tif err != nil {", "new": " func (sender *peerAuthenticationSender) execute() (error, bool) {\n \n \thardforkPayload, isTriggered := sender.getHardforkPayload()\n \tpayload := &heartbeat.Payload{\n+\t\tTimestamp:       time.Now().Unix(),\n+\t\tHardforkMessage: string(hardforkPayload),\n \t}\n \tpayloadBytes, err := sender.marshaller.Marshal(payload)\n \tif err != nil {", "review": "no need for this", "dataset": "codereview_new", "original_id": 7817, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nfunc (sender *peerAuthenticationSender) execute() (error, bool) {\n\n\thardforkPayload, isTriggered := sender.getHardforkPayload()\n\tpayload := &heartbeat.Payload{\n\t\tTimestamp:          time.Now().Unix(),\n\t\tHardforkMessage:    string(hardforkPayload),\n\t\tNumTrieNodesSynced: 0, // sent through heartbeat v2 message\n\t}\n\tpayloadBytes, err := sender.marshaller.Marshal(payload)\n\tif err != nil {\n\t\t// handle error\n\t}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 84.77444, "model_code": " func (sender *peerAuthenticationSender) execute() (error, bool) { \n hardforkPayload, isTriggered := sender.getHardforkPayload() \n payload := &heartbeat.Payload{ \n Timestamp:          time.Now().Unix(), \n HardforkMessage:    string(hardforkPayload), \n NumTrieNodesSynced: 0, \n } \n payloadBytes, err := sender.marshaller.Marshal(payload) \n if err != nil {", "model_em_trim": 0, "model_bleu_trim": 93.66728766079402, "error_type_gpt": "Need Domain Knowledge"}
{"id": 97, "old": " static inline RBinWasmSection *sections_first_custom_name(RBinWasmObj *bin) {\n \tr_list_foreach (bin->g_sections, iter, sec) {\n \t\tif (sec->id == R_BIN_WASM_SECTION_CUSTOM && sec->size > 6) {\n \t\t\tut8 _tmp[CUST_NAME_START_LEN] = {0};\n-\t\t\tr_buf_read_at (buf, sec->offset, _tmp, CUST_NAME_START_LEN);\n-\t\t\tif (!memcmp (CUST_NAME_START, _tmp, CUST_NAME_START_LEN)) {\n-\t\t\t\treturn sec;\n \t\t\t}\n \t\t}\n \t}", "new": " static inline RBinWasmSection *sections_first_custom_name(RBinWasmObj *bin) {\n \tr_list_foreach (bin->g_sections, iter, sec) {\n \t\tif (sec->id == R_BIN_WASM_SECTION_CUSTOM && sec->size > 6) {\n \t\t\tut8 _tmp[CUST_NAME_START_LEN] = {0};\n+\t\t\tif (r_buf_read_at (buf, sec->offset, _tmp, CUST_NAME_START_LEN) > 0) {\n+\t\t\t\tif (!memcmp (CUST_NAME_START, _tmp, CUST_NAME_START_LEN)) {\n+\t\t\t\t\treturn sec;\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \t}", "review": "```suggestion\r\n\t\t\t\tif (!memcmp (CUST_NAME_START, _tmp, CUST_NAME_START_LEN)) {\r\n```", "dataset": "codereview_new", "original_id": 8021, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "diff\nstatic inline RBinWasmSection *sections_first_custom_name(RBinWasmObj *bin) {\n    r_list_foreach (bin->g_sections, iter, sec) {\n        if (sec->id == R_BIN_WASM_SECTION_CUSTOM && sec->size > 6) {\n            ut8 _tmp[CUST_NAME_START_LEN] = {0};\n            r_buf_read_at (buf, sec->offset, _tmp, CUST_NAME_START_LEN);\n-           if (!memcmp (CUST_NAME_START, _tmp, CUST_NAME_START_LEN)) {\n+           if (strncmp(CUST_NAME_START, (char *)_tmp, CUST_NAME_START_LEN) == 0) {\n                return sec;\n            }\n        }\n    }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 79.60978, "model_code": " static inline RBinWasmSection *sections_first_custom_name(RBinWasmObj *bin) { \n r_list_foreach (bin->g_sections, iter, sec) { \n if (sec->id == R_BIN_WASM_SECTION_CUSTOM && sec->size > 6) { \n ut8 *tmp; \n r_buf_read_at (buf, sec->offset, _tmp, CUST_NAME_START_LEN); \n if (!memcmp (CUST_NAME_START, _tmp, CUST_NAME_START_LEN)) { \n return sec; \n } \n } \n }", "model_em_trim": 0, "model_bleu_trim": 81.5432601820556}
{"id": 98, "old": " fun KotlinType.fqNameOrNull(): FqName? {\n     return TypeUtils.getClassDescriptor(this)?.fqNameOrNull()\n }\n \n @Suppress(\"ReturnCount\")\n fun KtExpression.getDataFlowAwareTypes(\n     bindingContext: BindingContext,", "new": " fun KotlinType.fqNameOrNull(): FqName? {\n     return TypeUtils.getClassDescriptor(this)?.fqNameOrNull()\n }\n \n+/**\n+ * Returns types considering data flow.\n+ *\n+ * For Example, for `s` in `print(s)` below, [BindingContext.getType] returns String?, but this function returns String.\n+ *\n+ * ```kotlin\n+ * fun foo(s: String?) {\n+ *     if (s != null) {\n+ *         println(s) // s is String (smart cast from String?)\n+ *     }\n+ * }\n+ * ```\n+ */\n @Suppress(\"ReturnCount\")\n fun KtExpression.getDataFlowAwareTypes(\n     bindingContext: BindingContext,", "review": "Can we add a comment on this function with a couple of example of what it does?", "dataset": "codereview_new", "original_id": 1247, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-add", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-add", "gpt_code": "\n/**\n * Returns the fully qualified name of the class descriptor or null if it doesn't exist.\n * Example: If the class descriptor is for the class \"com.example.MyClass\", the fully qualified name would be \"com.example.MyClass\".\n */\nfun KotlinType.fqNameOrNull(): FqName? {\n    return TypeUtils.getClassDescriptor(this)?.fqNameOrNull()\n}\n\n@Suppress(\"ReturnCount\")\nfun KtExpression.getDataFlowAwareTypes(\n    bindingContext: BindingContext,\n", "gpt_em_trim": 0, "gpt_bleu_trim": 6.38e-57, "model_code": " fun KotlinType.fqNameOrNull(): FqName? { \n return TypeUtils.getClassDescriptor(this)?.fqNameOrNull() \n } \n } \n @Suppress(\"ReturnCount\") \n fun KtExpression.getDataFlowAwareTypes( \n bindingContext: BindingContext,", "model_em_trim": 0, "model_bleu_trim": 9.50527445062507, "error_type_gpt": "Need Domain Knowledge"}
{"id": 99, "old": " func (dbCtx *DatabaseContext) AddDatabaseLogContext(ctx context.Context) context\n \treturn ctx\n }\n \n-// GetSingleCollectionID returns a collectionID. This is a shim for single collections.\n func (dbCtx *DatabaseContext) GetSingleCollectionID() (uint32, error) {\n \tcollection, err := base.AsCollection(dbCtx.Bucket)\n \tif err != nil {", "new": " func (dbCtx *DatabaseContext) AddDatabaseLogContext(ctx context.Context) context\n \treturn ctx\n }\n \n+// GetSingleCollectionID returns a collectionID. This is a temporary shim for single collections, and will be removed when a database can support multiple collecitons.\n func (dbCtx *DatabaseContext) GetSingleCollectionID() (uint32, error) {\n \tcollection, err := base.AsCollection(dbCtx.Bucket)\n \tif err != nil {", "review": "Is this temporary and will be removed?  Might be good to indicate in a comment if so.", "dataset": "codereview_new", "original_id": 9247, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n// AddDatabaseLogContext adds database log context to the given context.\n// This function is temporary and will be removed in future versions.\nfunc (dbCtx *DatabaseContext) AddDatabaseLogContext(ctx context.Context) context.Context {\n\treturn ctx\n}\n\n// GetSingleCollectionID returns a collectionID. This is a shim for single collections.\nfunc (dbCtx *DatabaseContext) GetSingleCollectionID() (uint32, error) {\n\tcollection, err := base.AsCollection(dbCtx.Bucket)\n\tif err != nil {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 76.98609, "model_code": " func (dbCtx *DatabaseContext) AddDatabaseLogContext(ctx context.Context) context \n return ctx \n } \n \n // GetSingleCollectionID returns a collectionID. This is a shim for single collections. \n func (dbCtx *DatabaseContext) GetSingleCollectionID() (uint32, error) { \n collection, err := base.AsCollection(dbCtx.Bucket) \n if err != nil {", "model_em_trim": 0, "model_bleu_trim": 78.489044217589, "error_type_gpt": "Need Domain Knowledge"}
{"id": 100, "old": " def __build_tool_cmds(self, benchmark_case: BenchmarkCase,\n \n     cmds: List[Any] = run_module_utils.build_linux_wrapper_cmds_for_device_spec(\n         run_config.target_device_spec)\n-    cmds += [tool_path]\n \n     module_path = iree_artifacts.get_module_path(\n         run_config.module_generation_config,", "new": " def __build_tool_cmds(self, benchmark_case: BenchmarkCase,\n \n     cmds: List[Any] = run_module_utils.build_linux_wrapper_cmds_for_device_spec(\n         run_config.target_device_spec)\n+    cmds.append(tool_path)\n \n     module_path = iree_artifacts.get_module_path(\n         run_config.module_generation_config,", "review": "append?\r\n\r\n```suggestion\r\n    cmds.append(tool_path)\r\n```\r\n\r\n(same below)", "dataset": "codereview_new", "original_id": 9487, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ndef __build_tool_cmds(self, benchmark_case: BenchmarkCase,\n\n    cmds: List[Any] = run_module_utils.build_linux_wrapper_cmds_for_device_spec(\n        run_config.target_device_spec)\n    cmds.append(tool_path)\n\n    module_path = iree_artifacts.get_module_path(\n        run_config.module_generation_config,\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " def __build_tool_cmds(self, benchmark_case: BenchmarkCase, \n cmds: List[Any] = run_module_utils.build_linux_wrapper_cmds_for_device_spec( \n run_config.target_device_spec) \n module_path = iree_artifacts.get_module_path( \n run_config.module_generation_config,", "model_em_trim": 0, "model_bleu_trim": 89.47589693668468}
{"id": 101, "old": "\n-# avoid infinite loop\n # rubocop:disable Layout/ArrayAlignment\n describe DistributionPdf do\n   let(:distribution) { FactoryBot.create(:distribution) }", "new": "\n+# avoid Rubocop failing with an infinite loop when it checks this cop\n # rubocop:disable Layout/ArrayAlignment\n describe DistributionPdf do\n   let(:distribution) { FactoryBot.create(:distribution) }", "review": "I'am not exactly sure what this is referring to", "dataset": "codereview_new", "original_id": 4039, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n# This code block creates a new instance of the DistributionPdf class and sets the distribution variable to a new instance of the FactoryBot distribution object.\ndescribe DistributionPdf do\n  let(:distribution) { FactoryBot.create(:distribution) }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 38.58759, "model_code": " describe DistributionPdf do \n let(:distribution) { FactoryBot.create(:distribution) }", "model_em_trim": 0, "model_bleu_trim": 21.58150833986898, "error_type_gpt": "Need Domain Knowledge"}
{"id": 102, "old": " import TuistGraph\n import XcodeProj\n \n protocol BuildRulesGenerating: AnyObject {\n-    func generateBuildRules(\n-        target: Target,\n-        pbxTarget: PBXTarget,\n-        pbxproj: PBXProj\n-    ) throws\n }\n \n final class BuildRulesGenerator: BuildRulesGenerating {", "new": " import TuistGraph\n import XcodeProj\n \n protocol BuildRulesGenerating: AnyObject {\n+    func generateBuildRules(target: Target, pbxTarget: PBXTarget, pbxproj: PBXProj) throws\n }\n \n final class BuildRulesGenerator: BuildRulesGenerating {", "review": "```suggestion\r\n    func generateBuildRules(target: Target, pbxTarget: PBXTarget, pbxproj: PBXProj) throws\r\n```", "dataset": "codereview_new", "original_id": 458, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\nimport TuistGraph\nimport XcodeProj\n\nprotocol BuildRulesGenerating: AnyObject {\n    func generateBuildRules(\n        for target: Target,\n        pbxTarget: PBXTarget,\n        pbxproj: PBXProj\n    ) throws\n}\n\nfinal class BuildRulesGenerator: BuildRulesGenerating {\n    func generateBuildRules(for target: Target, pbxTarget: PBXTarget, pbxproj: PBXProj) throws {\n        // implementation goes here\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 59.08697, "model_code": " import TuistGraph \n import XcodeProj \n final class BuildRulesGenerator: BuildRulesGenerating {", "model_em_trim": 0, "model_bleu_trim": 11.006659984170271, "error_type_gpt": "Unclear Location"}
{"id": 103, "old": " func (accessor *applicationResourceNamespaceAccessor) Namespace() string {\n func NewApplicationResourceNamespaceAccessor(appNs, overrideNs string) NamespaceAccessor {\n \treturn &applicationResourceNamespaceAccessor{applicationNamespace: appNs, overrideNamespace: overrideNs}\n }\n-\n-// ComponentNameWithReplicaKey helps generate component name with replica key\n-func ComponentNameWithReplicaKey(componentName, replicaKey string) string {\n-\tif replicaKey == \"\" {\n-\t\treturn componentName\n-\t}\n-\treturn fmt.Sprintf(\"%s-%s\", componentName, replicaKey)\n-}", "new": " func (accessor *applicationResourceNamespaceAccessor) Namespace() string {\n func NewApplicationResourceNamespaceAccessor(appNs, overrideNs string) NamespaceAccessor {\n \treturn &applicationResourceNamespaceAccessor{applicationNamespace: appNs, overrideNamespace: overrideNs}\n }", "review": "what if a user define workload name in cue template?", "dataset": "codereview_new", "original_id": 7032, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nfunc (accessor *applicationResourceNamespaceAccessor) Namespace() string {\nfunc NewApplicationResourceNamespaceAccessor(appNs, overrideNs string) NamespaceAccessor {\n\treturn &applicationResourceNamespaceAccessor{applicationNamespace: appNs, overrideNamespace: overrideNs}\n}\n\n// ComponentNameWithReplicaKey helps generate component name with replica key\nfunc ComponentNameWithReplicaKey(componentName, replicaKey, workloadName string) string {\n\tif replicaKey == \"\" {\n\t\treturn componentName\n\t}\n\tif workloadName != \"\" {\n\t\treturn fmt.Sprintf(\"%s-%s-%s\", componentName, replicaKey, workloadName)\n\t}\n\treturn fmt.Sprintf(\"%s-%s\", componentName, replicaKey)\n}\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " func (accessor *applicationResourceNamespaceAccessor) Namespace() string { \n func NewApplicationResourceNamespaceAccessor(appNs, overrideNs string) NamespaceAccessor { \n return &applicationResourceNamespaceAccessor{applicationNamespace: appNs, overrideNamespace: overrideNs} \n }", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 104, "old": " async def async_will_remove_from_hass(self) -> None:\n     @async_log_errors\n     async def async_update(self) -> None:\n         \"\"\"Get the latest status information from device.\"\"\"\n         await self._receiver.async_update()\n         if self._update_audyssey:\n             await self._receiver.async_update_audyssey()", "new": " async def async_will_remove_from_hass(self) -> None:\n     @async_log_errors\n     async def async_update(self) -> None:\n         \"\"\"Get the latest status information from device.\"\"\"\n+        if (\n+            self._receiver.telnet_connected is True\n+            and self._receiver.telnet_healthy is True\n+        ):\n+            await self._receiver.input.async_update_media_state()\n+            return\n+\n         await self._receiver.async_update()\n         if self._update_audyssey:\n             await self._receiver.async_update_audyssey()", "review": "When the telnet connection is up and running, we don't need to update the entire receiver anymore, but only attributes like interpret, title etc. which are not covered by telnet. This would save some API calls.\r\n\r\n```suggestion\r\n        \"\"\"Get the latest status information from device.\"\"\"\r\n        if self._receiver.telnet_connected is True and self._receiver.telnet_healthy is True\r\n            await self._receiver.input.async_update_media_state()\r\n            return\r\n```\r\n\r\n\r\n", "dataset": "codereview_new", "original_id": 9783, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\nasync def async_will_remove_from_hass(self) -> None:\n    @async_log_errors\n    async def async_update(self) -> None:\n        \"\"\"Get the latest status information from device.\"\"\"\n        if self._receiver.telnet_connected and self._receiver.telnet_healthy:\n            await self._receiver.input.async_update_media_state()\n            return\n        await self._receiver.async_update()\n        if self._update_audyssey:\n            await self._receiver.async_update_audyssey()\n", "gpt_em_trim": 0, "gpt_bleu_trim": 91.44427, "model_code": " async def async_will_remove_from_hass(self) -> None: \n @async_log_errors \n async def async_update(self) -> None: \n \"\"\"Get the latest status information from device.\"\"\" \n await self._receiver.async_update() \n if self._update_audyssey: \n await self._receiver.async_update_audyssey()", "model_em_trim": 0, "model_bleu_trim": 58.45310126444899, "error_type_gpt": "Code Style Difference"}
{"id": 105, "old": " def __init__(\n \n     def update(self) -> bool:\n         \"\"\"Validate connection and retrieve a list of sensors.\"\"\"\n-        self.pyobihai = get_pyobihai(self.host, self.username, self.password)\n \n-        if not self.pyobihai.check_account():\n-            return False\n \n         self.serial = self.pyobihai.get_device_serial()\n         self.services = self.pyobihai.get_state()", "new": " def __init__(\n \n     def update(self) -> bool:\n         \"\"\"Validate connection and retrieve a list of sensors.\"\"\"\n+        if not self.pyobihai:\n+            self.pyobihai = get_pyobihai(self.host, self.username, self.password)\n \n+            if not self.pyobihai.check_account():\n+                return False\n \n         self.serial = self.pyobihai.get_device_serial()\n         self.services = self.pyobihai.get_state()", "review": "Should this not check if `pyobihai` already exists?\r\n```suggestion\r\n        if not self.pyobihai:\r\n            self.pyobihai = get_pyobihai(self.host, self.username, self.password)\r\n```\r\n\r\nAnd maybe `check_account` can also be guarded?", "dataset": "codereview_new", "original_id": 9622, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\ndef __init__(self):\n    self.pyobihai = None\n    self.serial = None\n    self.services = None\n\ndef update(self) -> bool:\n    \"\"\"Validate connection and retrieve a list of sensors.\"\"\"\n    if not self.pyobihai:\n        self.pyobihai = get_pyobihai(self.host, self.username, self.password)\n\n    if not self.pyobihai.check_account():\n        return False\n\n    self.serial = self.pyobihai.get_device_serial()\n    self.services = self.pyobihai.get_state()\n", "gpt_em_trim": 0, "gpt_bleu_trim": 92.88961, "model_code": " def __init__( \n ) -> None: \n \"\"\"Validate connection and retrieve a list of sensors.\"\"\" \n self.serial = self.pyobihai.get_device_serial() \n self.services = self.pyobihai.get_state()", "model_em_trim": 0, "model_bleu_trim": 41.01197873355923, "error_type_gpt": "Need Domain Knowledge"}
{"id": 106, "old": " object SkewedJoins {\n   // some sensible defaults for skewed joins\n   val DefaultHotKeyThreshold: Int = 9000\n   val DefaultHotKeyMethod: HotKeyMethod.Threshold = HotKeyMethod.Threshold(DefaultHotKeyThreshold)\n-  val DefaultHotKeyFanout: Int = 5\n   val DefaultCmsEpsilon: Double = 0.001\n   val DefaultCmsDelta: Double = 1e-10\n   val DefaultCmsSeed: Int = 42", "new": " object SkewedJoins {\n   // some sensible defaults for skewed joins\n   val DefaultHotKeyThreshold: Int = 9000\n   val DefaultHotKeyMethod: HotKeyMethod.Threshold = HotKeyMethod.Threshold(DefaultHotKeyThreshold)\n+  val DefaultHotKeyFanout: Int = 1\n   val DefaultCmsEpsilon: Double = 0.001\n   val DefaultCmsDelta: Double = 1e-10\n   val DefaultCmsSeed: Int = 42", "review": "how was 5 chosen? Any change it will degrade performance if not overridden?", "dataset": "codereview_new", "original_id": 3291, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nobject SkewedJoins {\n  // some sensible defaults for skewed joins\n  val DefaultHotKeyThreshold: Int = 9000\n  val DefaultHotKeyMethod: HotKeyMethod.Threshold = HotKeyMethod.Threshold(DefaultHotKeyThreshold)\n  val DefaultHotKeyFanout: Int = 10 // Updated to 10 from 5\n  val DefaultCmsEpsilon: Double = 0.001\n  val DefaultCmsDelta: Double = 1e-10\n  val DefaultCmsSeed: Int = 42\n", "gpt_em_trim": 0, "gpt_bleu_trim": 85.16834, "model_code": " object SkewedJoins { \n // some sensible defaults for skewed joins \n val DefaultHotKeyThreshold: Int = 9000 \n val DefaultHotKeyMethod: HotKeyMethod.Threshold = HotKeyMethod.Threshold(DefaultHotKeyThreshold) \n val DefaultCmsEpsilon: Double = 0.001 \n val DefaultCmsDelta: Double = 1e-10 \n val DefaultCmsSeed: Int = 42", "model_em_trim": 0, "model_bleu_trim": 87.95334323482557, "error_type_gpt": "Need Domain Knowledge"}
{"id": 107, "old": " NPC::NPC(const NPCType *npc_type_data, Spawn2 *in_respawn, const glm::vec4 &posi\n \tAISpellVar.idle_beneficial_chance          = static_cast<uint8> (RuleI(Spells, AI_IdleBeneficialChance));\n \n \t// It's possible for IsBot() to not be set yet during Bot loading, so have to use an alternative to catch Bots\n-\tif (!EQ::ValueWithin(npc_type_data->npc_spells_id, 3001, 3016)) {\n \t\tAI_Init();\n \t\tAI_Start();\n #ifdef BOTS", "new": " NPC::NPC(const NPCType *npc_type_data, Spawn2 *in_respawn, const glm::vec4 &posi\n \tAISpellVar.idle_beneficial_chance          = static_cast<uint8> (RuleI(Spells, AI_IdleBeneficialChance));\n \n \t// It's possible for IsBot() to not be set yet during Bot loading, so have to use an alternative to catch Bots\n+\tif (!EQ::ValueWithin(npc_type_data->npc_spells_id, EQ::constants::BotSpellIDs::Warrior, EQ::constants::BotSpellIDs::Berserker)) {\n \t\tAI_Init();\n \t\tAI_Start();\n #ifdef BOTS", "review": "Constants? BotSpellIDs::Warrior for example.", "dataset": "codereview_new", "original_id": 5758, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nNPC::NPC(const NPCType *npc_type_data, Spawn2 *in_respawn, const glm::vec4 &posi\n    const uint8_t WARRIOR_SPELL_ID_MIN = 3001;\n    const uint8_t WARRIOR_SPELL_ID_MAX = 3016;\n\n    AISpellVar.idle_beneficial_chance = static_cast<uint8_t>(RuleI(Spells, AI_IdleBeneficialChance));\n\n    if (!EQ::ValueWithin(npc_type_data->npc_spells_id, WARRIOR_SPELL_ID_MIN, WARRIOR_SPELL_ID_MAX)) {\n        AI_Init();\n        AI_Start();\n#ifdef BOTS\n", "gpt_em_trim": 0, "gpt_bleu_trim": 60.71101, "model_code": " NPC::NPC(const NPCType *npc_type_data, Spawn2 *in_respawn, const glm::vec4 &posi \n AISpellVar.idle_beneficial_chance          = static_cast<uint8> (RuleI(Spells, AI_IdleBeneficialChance)); \n if (!EQ::ValueWithin(npc_type_data->npc_spells_id, Constants::Warrior, 3016)) { \n AI_Init(); \n AI_Start(); \n #ifdef BOTS", "model_em_trim": 0, "model_bleu_trim": 60.55688244752031, "error_type_gpt": "Need Domain Knowledge"}
{"id": 108, "old": " extension SecondaryButtonDescription: ViewDescriptor {\n         ])\n \n         return button\n-\n     }\n \n     @objc dynamic func buttonTapped(_ sender: UIButton) {", "new": " extension SecondaryButtonDescription: ViewDescriptor {\n         ])\n \n         return button\n     }\n \n     @objc dynamic func buttonTapped(_ sender: UIButton) {", "review": "```suggestion\r\n```", "dataset": "codereview_new", "original_id": 569, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\nextension SecondaryButtonDescription: ViewDescriptor {\n    func makeView() -> UIView {\n        let button = UIButton(type: .system)\n        button.setTitle(title, for: .normal)\n        button.setTitleColor(.white, for: .normal)\n        button.backgroundColor = .secondary\n        button.layer.cornerRadius = 8\n        button.addTarget(self, action: #selector(buttonTapped(_:)), for: .touchUpInside)\n        return button\n    }\n\n    @objc dynamic func buttonTapped(_ sender: UIButton) {\n        delegate?.secondaryButtonTapped(self)\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 19.37652, "model_code": " extension SecondaryButtonDescription: ViewDescriptor { \n ]) \n } \n return button \n }", "model_em_trim": 0, "model_bleu_trim": 29.32360720569346, "error_type_gpt": "Unclear Location"}
{"id": 109, "old": " class FlatIOSpec extends ChiselFlatSpec {\n     chirrtl should include(\"out[addr] <= in[addr]\")\n   }\n \n-  it should \"Analog members\" in {\n     class MyBundle extends Bundle {\n       val foo = Output(UInt(8.W))\n       val bar = Analog(8.W)", "new": " class FlatIOSpec extends ChiselFlatSpec {\n     chirrtl should include(\"out[addr] <= in[addr]\")\n   }\n \n+  it should \"support Analog members\" in {\n     class MyBundle extends Bundle {\n       val foo = Output(UInt(8.W))\n       val bar = Analog(8.W)", "review": "nit\r\n```suggestion\r\n  it should \"support Analog members\" in {\r\n```", "dataset": "codereview_new", "original_id": 3169, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass FlatIOSpec extends ChiselFlatSpec {\n    chirrtl should include(\"out[addr] <= in[addr]\")\n  }\n\n  it should \"support Analog members\" in {\n    class MyBundle extends Bundle {\n      val foo = Output(UInt(8.W))\n      val bar = Analog(8.W)\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " class FlatIOSpec extends ChiselFlatSpec { \n chirrtl should include(\"out[addr] <= in[addr]\") \n } \n \n it should \"support Analog members\" in { \n class MyBundle extends Bundle { \n val foo = Output(UInt(8.W)) \n val bar = Analog(8.W)", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 110, "old": " private fun validate(\n \n    fun ContainsSpec.violation(tree: JsonNode.ArrayNode): List<SchemaViolation> {\n       val schemaViolations = tree.elements.mapIndexed { i, node ->\n-         validate(\"$currentPath.contains[$i]\", node, schema)\n       }\n       val foundElements = schemaViolations.count { it.isEmpty() }\n-      return when {\n-         foundElements != 0 -> emptyList()\n-         foundElements == 0 -> violation(\"Expected any item of type ${schema.typeName()}\") + schemaViolations.flatten()\n-         schemaViolations.isNotEmpty() -> schemaViolations.flatten()\n-         else -> emptyList()\n-      }\n    }\n \n    fun JsonSchemaElement.violation(tree: JsonNode.ArrayNode): List<SchemaViolation> =", "new": " private fun validate(\n \n    fun ContainsSpec.violation(tree: JsonNode.ArrayNode): List<SchemaViolation> {\n       val schemaViolations = tree.elements.mapIndexed { i, node ->\n+         validate(\"$currentPath[$i]\", node, schema)\n       }\n       val foundElements = schemaViolations.count { it.isEmpty() }\n+      return if (foundElements == 0)\n+         violation(\"Expected some item to match contains-specification:\") + schemaViolations.flatten()\n+      else emptyList()\n    }\n \n    fun JsonSchemaElement.violation(tree: JsonNode.ArrayNode): List<SchemaViolation> =", "review": "These two branches should be impossible to reach, since `foundElements` must either be `== 0` or `!= 0`? Can we remove the branches?", "dataset": "codereview_new", "original_id": 1605, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nprivate fun validate(\n   fun ContainsSpec.violation(tree: JsonNode.ArrayNode): List<SchemaViolation> {\n      val schemaViolations = tree.elements.mapIndexed { i, node ->\n         validate(\"$currentPath.contains[$i]\", node, schema)\n      }\n      val foundElements = schemaViolations.count { it.isEmpty() }\n      return when {\n         foundElements == 0 -> violation(\"Expected any item of type ${schema.typeName()}\") + schemaViolations.flatten()\n         else -> schemaViolations.flatten()\n      }\n   }\n\n   fun JsonSchemaElement.violation(tree: JsonNode.ArrayNode): List<SchemaViolation> =\n", "gpt_em_trim": 0, "gpt_bleu_trim": 74.92053, "model_code": " private fun validate( \n fun ContainsSpec.violation(tree: JsonNode.ArrayNode): List<SchemaViolation> = \n tree.elements.mapIndexed { i, node -> \n validate(\"$currentPath.contains[$i]\", node, schema) \n } \n fun JsonSchemaElement.violation(tree: JsonNode.ArrayNode): List<SchemaViolation> =", "model_em_trim": 0, "model_bleu_trim": 46.41013222950489}
{"id": 111, "old": " public annotation class ExperimentalTraceApi\n \n /** Tracing result of `R`. Allows to inspect `R`, and the traces from where it was raised. */\n @ExperimentalTraceApi\n-@JvmInline value class Trace(private val exception: CancellationException) {\n   /**\n    * Returns the stacktrace as a [String]\n    *", "new": " public annotation class ExperimentalTraceApi\n \n /** Tracing result of `R`. Allows to inspect `R`, and the traces from where it was raised. */\n @ExperimentalTraceApi\n+@JvmInline\n+public value class Trace(private val exception: CancellationException) {\n   /**\n    * Returns the stacktrace as a [String]\n    *", "review": "```suggestion\r\n@JvmInline\r\npublic value class Trace(private val exception: CancellationException) {\r\n```", "dataset": "codereview_new", "original_id": 1144, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n@JvmInline\npublic value class Trace(private val exception: CancellationException) {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 4.357231, "model_code": " public annotation class ExperimentalTraceApi \n { \n /** Tracing result of `R`. Allows to inspect `R`, and the traces from where it was raised. */ \n @ExperimentalTraceApi \n public value class Trace(private val exception: CancellationException) { \n /** \n * Returns the stacktrace as a [String] \n *", "model_em_trim": 0, "model_bleu_trim": 91.60253503926417, "error_type_gpt": "Unclear Changes"}
{"id": 112, "old": " public static Map<Integer, String> indexQuotedNameById(Schema schema) {\n   }\n \n   /**\n-   * convert partition spec to Spark type\n    *\n-   * @param spec\n-   * @return\n    */\n   public static StructType convert(PartitionSpec spec) {\n     return convert(new Schema(spec.partitionType().asNestedType().asStructType().fields()));", "new": " public static Map<Integer, String> indexQuotedNameById(Schema schema) {\n   }\n \n   /**\n+   * Convert a {@link PartitionSpec} to a {@link DataType Spark type}.\n    *\n+   * @param spec iceberg PartitionSpec\n+   * @return {@link StructType}\n    */\n   public static StructType convert(PartitionSpec spec) {\n     return convert(new Schema(spec.partitionType().asNestedType().asStructType().fields()));", "review": "nit: \r\n\r\n```java\r\n\r\n  /**\r\n   * Convert a {@link PartitionSpec} to a {@link DataType Spark type}.\r\n   *\r\n   * @param spec a iceberg PartitionSpec\r\n   * @return the equivalent Spark type\r\n   */\r\n```", "dataset": "codereview_new", "original_id": 4390, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\npublic static Map<Integer, String> indexQuotedNameById(Schema schema) {\n}\n\n/**\n * Convert a {@link PartitionSpec} to a {@link DataType Spark type}.\n *\n * @param spec a iceberg PartitionSpec\n * @return the equivalent Spark type\n */\npublic static StructType convert(PartitionSpec spec) {\n  return convert(new Schema(spec.partitionType().asNestedType().asStructType().fields()));\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 90.00871, "model_code": " public static Map<Integer, String> indexQuotedNameById(Schema schema) { \n } \n public static StructType convert(PartitionSpec spec) { \n return convert(new Schema(spec.partitionType().asNestedType().asStructType().fields()));", "model_em_trim": 0, "model_bleu_trim": 45.992055063380086, "error_type_gpt": "Reasonable Improvement"}
{"id": 113, "old": " sub userIsAuthenticated : Private {\n     );\n     if (is_dpsk_enabled($c->profile)) {\n         my @provisioners = $c->profile->provisionerObjects();\n-        unless(@provisioners){\n-            return;\n-        }\n \n         foreach my $provisioner (@provisioners) {\n             next unless $provisioner->isa(\"pf::provisioner::dpsk\");", "new": " sub userIsAuthenticated : Private {\n     );\n     if (is_dpsk_enabled($c->profile)) {\n         my @provisioners = $c->profile->provisionerObjects();\n \n         foreach my $provisioner (@provisioners) {\n             next unless $provisioner->isa(\"pf::provisioner::dpsk\");", "review": "If there is more than one, it's going to generate it each time but only one item is in the stash after.\r\nWill it behave correctly if more than one DPSK provisioner is setup on the connection profile?", "dataset": "codereview_new", "original_id": 2933, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nsub userIsAuthenticated : Private {\n    );\n    if (is_dpsk_enabled($c->profile)) {\n        my @provisioners = $c->profile->provisionerObjects();\n        unless(@provisioners){\n            return;\n        }\n\n        my $dpsk_provisioner_found = 0;\n        foreach my $provisioner (@provisioners) {\n            if ($provisioner->isa(\"pf::provisioner::dpsk\")) {\n                $dpsk_provisioner_found = 1;\n                # perform necessary actions for dpsk provisioner\n            }\n        }\n\n        unless ($dpsk_provisioner_found) {\n            # handle case when no dpsk provisioner is found\n        }\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 70.86185, "model_code": " sub userIsAuthenticated : Private { \n ); \n if (is_dpsk_enabled($c->profile)) { \n my $provisioners = $c->profile->provisionerObjects(); \n foreach my $provisioner (@provisioners) { \n next unless $provisioner->isa(\"pf::provisioner::dpsk\");", "model_em_trim": 0, "model_bleu_trim": 96.13106798273469, "error_type_gpt": "Need Domain Knowledge"}
{"id": 114, "old": " public long getCandidatesStat() {\n     return candidates;\n   }\n \n-  @Override\n-  public boolean isRootTable() {\n-    return level == DataLevel.ROOT;\n-  }\n-\n-  @Override\n-  public boolean isMetadataTable() {\n-    return level == DataLevel.METADATA;\n-  }\n-\n   @Override\n   public Set<TableId> getCandidateTableIDs() {\n-    if (isRootTable()) {\n       return Collections.singleton(MetadataTable.ID);\n-    } else if (isMetadataTable()) {\n       Set<TableId> tableIds = new HashSet<>(getTableIDs());\n       tableIds.remove(MetadataTable.ID);\n       tableIds.remove(RootTable.ID);", "new": " public long getCandidatesStat() {\n     return candidates;\n   }\n \n   @Override\n   public Set<TableId> getCandidateTableIDs() {\n+    if (level == DataLevel.ROOT) {\n       return Collections.singleton(MetadataTable.ID);\n+    } else if (level == DataLevel.METADATA) {\n       Set<TableId> tableIds = new HashSet<>(getTableIDs());\n       tableIds.remove(MetadataTable.ID);\n       tableIds.remove(RootTable.ID);", "review": "Using level in the name seems better to me.\r\n\r\n```suggestion\r\n  /**\r\n   * @return the tables id for the current data level\r\n   */\r\n  public Set<TableId> getLevelTableIDs() {\r\n```", "dataset": "codereview_new", "original_id": 4540, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\npublic long getCandidatesStat() {\n    return candidates;\n  }\n\n  @Override\n  public boolean isRootTable() {\n    return level == DataLevel.ROOT;\n  }\n\n  @Override\n  public boolean isMetadataTable() {\n    return level == DataLevel.METADATA;\n  }\n\n  /**\n   * @return the tables id for the current data level\n   */\n  @Override\n  public Set<TableId> getLevelTableIDs() {\n    if (isRootTable()) {\n      return Collections.singleton(MetadataTable.ID);\n    } else if (isMetadataTable()) {\n      Set<TableId> tableIds = new HashSet<>(getTableIDs());\n      tableIds.remove(MetadataTable.ID);\n      tableIds.remove(RootTable.ID);\n", "gpt_em_trim": 0, "gpt_bleu_trim": 59.49145, "model_code": " public long getCandidatesStat() { \n return candidates; \n } \n @Override \n public Set<TableId> getCandidateTableIDs() { \n if (isRootTable()) { \n return Collections.singleton(MetadataTable.ID); \n } else if (isMetadataTable()) { \n Set<TableId> tableIds = new HashSet<>(getTableIDs()); \n tableIds.remove(MetadataTable.ID); \n tableIds.remove(RootTable.ID);", "model_em_trim": 0, "model_bleu_trim": 82.44984006240654}
{"id": 115, "old": " sealed trait Files[F[_]] extends FilesPlatform[F] {\n     * Use `writeUtf8Lines(path, Flags.Append)` to append to the end\n     * of the file, or pass other flags to further customize behavior.\n     */\n-  def writeUtf8Lines(path: Path): Pipe[F, String, Nothing] = writeUtf8(path, Flags.Write)\n \n   /** Writes each string to the specified file as utf8 lines\n     * using the specified flags to open the file.\n     */\n   def writeUtf8Lines(path: Path, flags: Flags): Pipe[F, String, Nothing] = in =>\n-    in.through(text.lines).through(writeUtf8(path, flags))\n }\n \n object Files extends FilesCompanionPlatform {", "new": " sealed trait Files[F[_]] extends FilesPlatform[F] {\n     * Use `writeUtf8Lines(path, Flags.Append)` to append to the end\n     * of the file, or pass other flags to further customize behavior.\n     */\n+  def writeUtf8Lines(path: Path): Pipe[F, String, Nothing] = writeUtf8Lines(path, Flags.Write)\n \n   /** Writes each string to the specified file as utf8 lines\n     * using the specified flags to open the file.\n     */\n   def writeUtf8Lines(path: Path, flags: Flags): Pipe[F, String, Nothing] = in =>\n+    in.intersperse(\"\\n\").through(writeUtf8(path, flags))\n }\n \n object Files extends FilesCompanionPlatform {", "review": "As @armanbilge mentioned, if the goal here is to insert a newline between strings, that would be `in => writeUtf8(path, flags)(text.intersperse(\"\\n\")` ", "dataset": "codereview_new", "original_id": 3404, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nsealed trait Files[F[_]] extends FilesPlatform[F] {\n    * Use `writeUtf8Lines(path, Flags.Append)` to append to the end\n    * of the file, or pass other flags to further customize behavior.\n    */\n  def writeUtf8Lines(path: Path): Pipe[F, String, Nothing] = writeUtf8(path, Flags.Write)\n\n  /** Writes each string to the specified file as utf8 lines\n    * using the specified flags to open the file.\n    */\n  def writeUtf8Lines(path: Path, flags: Flags): Pipe[F, String, Nothing] = in =>\n    in.through(text.intersperse(\"\\n\")).through(writeUtf8(path, flags))\n}\n\nobject Files extends FilesCompanionPlatform {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 94.02131, "model_code": " sealed trait Files[F[_]] extends FilesPlatform[F] { \n * Use `writeUtf8Lines(path, Flags.Append)` to append to the end \n * of the file, or pass other flags to further customize behavior. \n */ \n def writeUtf8Lines(path: Path, flags: Flags): Pipe[F, String, Nothing] = writeUtf8(path, flags) \n } \n object Files extends FilesCompanionPlatform {", "model_em_trim": 0, "model_bleu_trim": 43.43586638379606}
{"id": 116, "old": "\n                     .getResourceAsStream(\"version.properties\"));\n         } catch (Exception e) {\n             LoggerFactory.getLogger(PolymerTemplate.class.getName())\n-                    .error(\"Unable to read the version.properties file.\", e);\n         }\n \n         LicenseChecker.checkLicenseFromStaticBlock(\"flow-polymer-template\",", "new": "\n                     .getResourceAsStream(\"version.properties\"));\n         } catch (Exception e) {\n             LoggerFactory.getLogger(PolymerTemplate.class.getName())\n+                    .error(\"Unable to read the version.properties file.\");\n+            throw new ExceptionInInitializerError(e);\n         }\n \n         LicenseChecker.checkLicenseFromStaticBlock(\"flow-polymer-template\",", "review": "```suggestion\r\n            LoggerFactory.getLogger(PolymerTemplate.class.getName())\r\n                    .error(\"Unable to read the version.properties file.\");\r\n            throw new ExceptionInInitializerError(e);\r\n```", "dataset": "codereview_new", "original_id": 9834, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n\n                    .getResourceAsStream(\"version.properties\"));\n        } catch (Exception e) {\n            LoggerFactory.getLogger(PolymerTemplate.class.getName())\n                    .error(\"Unable to read the version.properties file.\");\n            throw new ExceptionInInitializerError(e);\n        }\n\n        LicenseChecker.checkLicenseFromStaticBlock(\"flow-polymer-template\",\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " .getResourceAsStream(\"version.properties\")); \n } catch (Exception e) { \n throw new ExceptionInInitializerError(e); \n } \n LicenseChecker.checkLicenseFromStaticBlock(\"flow-polymer-template\",", "model_em_trim": 0, "model_bleu_trim": 46.754434143804296}
{"id": 117, "old": " class MediaShareRestrictionManager {\n     }\n \n     func canCopyToClipboard() -> Bool {\n-        return  SecurityFlags.fileSharing.isEnabled\n     }\n \n     func canCopyFromClipboard() -> Bool {", "new": " class MediaShareRestrictionManager {\n     }\n \n     func canCopyToClipboard() -> Bool {\n+        return SecurityFlags.fileSharing.isEnabled\n     }\n \n     func canCopyFromClipboard() -> Bool {", "review": "Do we need 2 methods `canCopyToClipboard()` and `canCopyFromClipboard()` ?", "dataset": "codereview_new", "original_id": 703, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\nclass MediaShareRestrictionManager {\n}\n\nfunc canCopyToOrFromClipboard(direction: CopyDirection) -> Bool {\n    switch direction {\n    case .toClipboard:\n        return SecurityFlags.fileSharing.isEnabled\n    case .fromClipboard:\n        // Add code to check if user can copy from clipboard\n        return false\n    }\n}\n\nenum CopyDirection {\n    case toClipboard\n    case fromClipboard\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 44.80304, "model_code": " class MediaShareRestrictionManager { \n } \n func canCopyFromClipboard() -> Bool {", "model_em_trim": 0, "model_bleu_trim": 30.722150320057747}
{"id": 118, "old": " def self.hereditary?\n     context \"when providing a geo haystack index with a bucket_size\" do\n \n       let(:message) do\n-        'The geoHaystack option is deprecated.'\n       end\n \n       it \"logs a deprecation warning\" do", "new": " def self.hereditary?\n     context \"when providing a geo haystack index with a bucket_size\" do\n \n       let(:message) do\n+        'The geoHaystack type is deprecated.'\n       end\n \n       it \"logs a deprecation warning\" do", "review": "I think it should be \"geoHaystack index type\" and not \"option\". https://www.mongodb.com/docs/manual/core/geohaystack/ just calls it an \"index\" which I suppose also is valid if you prefer that.", "dataset": "codereview_new", "original_id": 4811, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\ndef self.hereditary?\n    context \"when providing a geo haystack index type with a bucket_size\" do\n\n      let(:message) do\n        'The geoHaystack index type is deprecated.'\n      end\n\n      it \"logs a deprecation warning\" do\n", "gpt_em_trim": 0, "gpt_bleu_trim": 88.44829, "model_code": " def self.hereditary? \n context \"when providing a geo haystack index with a bucket_size\" do \n let(:message) do \n 'The geoHaystack index type is deprecated.' \n end \n it \"logs a deprecation warning\" do", "model_em_trim": 0, "model_bleu_trim": 94.13140884765569, "error_type_gpt": "Reasonable Improvement"}
{"id": 119, "old": " - (void)simulateDeviceRotationToOrientation:(UIDeviceOrientation)orientation\n             \n             UIWindowSceneGeometryPreferencesIOS* preferences = [[UIWindowSceneGeometryPreferencesIOS alloc]initWithInterfaceOrientations:orientationMask];\n             [windowScene requestGeometryUpdateWithPreferences:preferences errorHandler:^(NSError * _Nonnull error) {\n-                NSLog(@\"error: %@\", error);\n             }];\n         }\n     } else {", "new": " - (void)simulateDeviceRotationToOrientation:(UIDeviceOrientation)orientation\n             \n             UIWindowSceneGeometryPreferencesIOS* preferences = [[UIWindowSceneGeometryPreferencesIOS alloc]initWithInterfaceOrientations:orientationMask];\n             [windowScene requestGeometryUpdateWithPreferences:preferences errorHandler:^(NSError * _Nonnull error) {\n+                [self failWithError:[NSError KIFErrorWithUnderlyingError:error format:@\"Could not rotate the screen\"] stopTest:YES];\n             }];\n         }\n     } else {", "review": "Does this happen in a expected case? If not, makes sense to emit an error and fail the test.", "dataset": "codereview_new", "original_id": 790, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n- (void)simulateDeviceRotationToOrientation:(UIDeviceOrientation)orientation {\n    if (windowScene) {\n        UIWindowSceneGeometryPreferencesIOS* preferences = [[UIWindowSceneGeometryPreferencesIOS alloc]initWithInterfaceOrientations:orientationMask];\n        [windowScene requestGeometryUpdateWithPreferences:preferences errorHandler:nil];\n    } else {\n        NSLog(@\"Error: windowScene is nil\");\n        XCTFail(@\"Test failed due to unexpected nil windowScene\");\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 48.63023, "model_code": " - (void)simulateDeviceRotationToOrientation:(UIDeviceOrientation)orientation \n UIWindowSceneGeometryPreferencesIOS* preferences = [[UIWindowSceneGeometryPreferencesIOS alloc]initWithInterfaceOrientations:orientationMask]; \n [windowScene requestGeometryUpdateWithPreferences:preferences errorHandler:^(NSError * _Nonnull error) { \n if (error) { \n NSLog(@\"error: %@\", error); \n } \n }]; \n } \n } else {", "model_em_trim": 0, "model_bleu_trim": 65.25554556024787, "error_type_gpt": "Need Domain Knowledge"}
{"id": 120, "old": " auto PythonServer::FillListPlayers(std::vector<PlayerSetupData>& players) const\n     const py::extract<py::list> py_players(r);\n     if (py_players.check()) {\n         py::stl_input_iterator<PlayerSetupData> players_begin(py_players), players_end;\n-        players.reserve(std::distance(players_begin, players_end));\n         players.insert(players.end(), players_begin, players_end);\n     } else {\n         DebugLogger() << \"Wrong players list data: check returns \"", "new": " auto PythonServer::FillListPlayers(std::vector<PlayerSetupData>& players) const\n     const py::extract<py::list> py_players(r);\n     if (py_players.check()) {\n         py::stl_input_iterator<PlayerSetupData> players_begin(py_players), players_end;\n+        players.reserve(py::len(py_players));\n         players.insert(players.end(), players_begin, players_end);\n     } else {\n         DebugLogger() << \"Wrong players list data: check returns \"", "review": "if the `std::distance` was the problem, the leave the `.insert` ?", "dataset": "codereview_new", "original_id": 6233, "relevance": "not", "relevance_user1": "perfect", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nauto PythonServer::FillListPlayers(std::vector<PlayerSetupData>& players) const\n    const py::extract<py::list> py_players(r);\n    if (py_players.check()) {\n        py::stl_input_iterator<PlayerSetupData> players_begin(py_players), players_end;\n        players.insert(players.end(), players_begin, players_end);\n    } else {\n        DebugLogger() << \"Wrong players list data: check returns \"\n", "gpt_em_trim": 0, "gpt_bleu_trim": 86.91958, "model_code": " auto PythonServer::FillListPlayers(std::vector<PlayerSetupData>& players) const \n const py::extract<py::list> py_players(r); \n if (py_players.check()) { \n auto players_begin = py_players.begin(); \n auto players_end = py_players.end(); \n players.reserve(std::distance(players_begin, players_end)); \n players.insert(players.end(), players_begin, players_end); \n } else { \n DebugLogger() << \"Wrong players list data: check returns \"", "model_em_trim": 0, "model_bleu_trim": 75.22550646851401}
{"id": 121, "old": "\n     \"python3.7\": \"python:3.7@sha256:be668898a538d5258e006e1920f86f31cab8000dfa68b3be78d5ef67ad15a417\",\n     \"python3.8\": \"python:3.8@sha256:b3402a5f5e9535ba4787a1fd6b0ee39738dee18bdff861a0589571ba74122d35\",\n     \"python3.9\": \"python:3.9@sha256:5b3585b121e6fb9707abb52c1f99cbab51939fee0769752ab6c641f20f479cf6\",\n-    \"nodejs12.x\": \"nodejs:12@sha256:16431b8d5eb26e80b37a80297ad67f70478c045339395bbd32f45091275ebb50\",\n     \"nodejs14.x\": \"nodejs:14@sha256:aa3286c61b6c3f97219da3e9fa39d97f9586672c64c958cb5b980268afdfb554\",\n     \"nodejs16.x\": \"nodejs:16@sha256:c7714124a782801cb7080fd6abddf9354a2ee89642571cb9222bb7541d3df558\",\n     \"nodejs18.x\": \"nodejs:18@sha256:f3c7ebb522417c8212d765b504e8078f99be78d41f82f3a08b7261e183ce4ed6\",", "new": "\n     \"python3.7\": \"python:3.7@sha256:be668898a538d5258e006e1920f86f31cab8000dfa68b3be78d5ef67ad15a417\",\n     \"python3.8\": \"python:3.8@sha256:b3402a5f5e9535ba4787a1fd6b0ee39738dee18bdff861a0589571ba74122d35\",\n     \"python3.9\": \"python:3.9@sha256:5b3585b121e6fb9707abb52c1f99cbab51939fee0769752ab6c641f20f479cf6\",\n+    \"nodejs12.x\": \"nodejs:12@sha256:1389b8cc6bbd321f22b0218233da11bd49382bbee7689dba41c7377c12df65a5\",\n     \"nodejs14.x\": \"nodejs:14@sha256:aa3286c61b6c3f97219da3e9fa39d97f9586672c64c958cb5b980268afdfb554\",\n     \"nodejs16.x\": \"nodejs:16@sha256:c7714124a782801cb7080fd6abddf9354a2ee89642571cb9222bb7541d3df558\",\n     \"nodejs18.x\": \"nodejs:18@sha256:f3c7ebb522417c8212d765b504e8078f99be78d41f82f3a08b7261e183ce4ed6\",", "review": "Any reason for not updating nodejs12.x? (newest sha would be `1389b8cc6bbd321f22b0218233da11bd49382bbee7689dba41c7377c12df65a5`)", "dataset": "codereview_new", "original_id": 6057, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n    \"python3.7\": \"python:3.7@sha256:be668898a538d5258e006e1920f86f31cab8000dfa68b3be78d5ef67ad15a417\",\n    \"python3.8\": \"python:3.8@sha256:b3402a5f5e9535ba4787a1fd6b0ee39738dee18bdff861a0589571ba74122d35\",\n    \"python3.9\": \"python:3.9@sha256:5b3585b121e6fb9707abb52c1f99cbab51939fee0769752ab6c641f20f479cf6\",\n    \"nodejs12.x\": \"nodejs:12@sha256:1389b8cc6bbd321f22b0218233da11bd49382bbee7689dba41c7377c12df65a5\",\n    \"nodejs14.x\": \"nodejs:14@sha256:aa3286c61b6c3f97219da3e9fa39d97f9586672c64c958cb5b980268afdfb554\",\n    \"nodejs16.x\": \"nodejs:16@sha256:c7714124a782801cb7080fd6abddf9354a2ee89642571cb9222bb7541d3df558\",\n    \"nodejs18.x\": \"nodejs:18@sha256:f3c7ebb522417c8212d765b504e8078f99be78d41f82f3a08b7261e183ce4ed6\",\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " \"python3.7\": \"python:3.7@sha256:be668898a538d5258e006e1920f86f31cab8000dfa68b3be78d5ef67ad15a417\", \n \"python3.8\": \"python:3.8@sha256:b3402a5f5e9535ba4787a1fd6b0ee39738dee18bdff861a0589571ba74122d35\", \n \"python3.9\": \"python:3.9@sha256:5b3585b121e6fb9707abb52c1f99cbab51939fee0769752ab6c641f20f479cf6\", \n \"nodejs12.x\": \"nodejs:12@sha256:16431b8d", "model_em_trim": 0, "model_bleu_trim": 47.729450800036894}
{"id": 122, "old": " public boolean hasEvent() {\n      * @param <T> Type of metadata being requested.\n      * @return The value of the additional metadata being requested or null if it's not present.\n      */\n-    public <T> T additionalMetadata(OperationMetadataAttribute<T> key) {\n         return additionalMetadata.get(key);\n     }\n ", "new": " public boolean hasEvent() {\n      * @param <T> Type of metadata being requested.\n      * @return The value of the additional metadata being requested or null if it's not present.\n      */\n+    public <T> T addtionalMetadata(OperationMetadataAttribute<T> key) {\n         return additionalMetadata.get(key);\n     }\n ", "review": "Unfortunately since this is a protected API, we shouldn't fix the spelling without deprecating the 'wrong' method name and creating a new one. We support people using newer core library versions with older client versions, and this could cause them to break by updating a core library.\r\n\r\nWe might be able to get away with fixing this when we do a minor version bump, but we'd probably want to lump it in with other changes to justify the annoyance to customers.", "dataset": "codereview_new", "original_id": 6581, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\n/**\n * Checks if the event is present.\n *\n * @return true if the event is present, false otherwise.\n */\npublic boolean hasEvent() {\n    return event != null;\n}\n\n/**\n * Gets the additional metadata for the specified key.\n *\n * @param <T> Type of metadata being requested.\n * @param key The key for the metadata.\n * @return The value of the additional metadata being requested or null if it's not present.\n * @deprecated This method has a spelling mistake in its name. Use the correct method name 'additionalMetadata' instead.\n */\n@Deprecated\npublic <T> T additionalMedatata(OperationMetadataAttribute<T> key) {\n    return additionalMetadata.get(key);\n}\n\n/**\n * Gets the additional metadata for the specified key.\n *\n * @param <T> Type of metadata being requested.\n * @param key The key for the metadata.\n * @return The value of the additional metadata being requested or null if it's not present.\n */\npublic <T> T additionalMetadata(OperationMetadataAttribute<T> key) {\n    return additionalMetadata.get(key);\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 56.86533, "model_code": " public boolean hasEvent() { \n * @param <T> Type of metadata being requested. \n * @return The value of the additional metadata being requested or null if it's not present. \n * @deprecated use {@link #additionalMetadata(OperationMetadataAttribute)} \n */ \n @Deprecated \n public <T> T additionalMetadata(OperationMetadataAttribute<T> key) { \n return additionalMetadata.get(key); \n } \n", "model_em_trim": 0, "model_bleu_trim": 74.62396632286318, "error_type_gpt": "Need Domain Knowledge"}
{"id": 123, "old": " struct VMConfigurationView: View {\n     @ViewBuilder\n     private var sharing: some View {\n         ConfigurationSection {\n-            SoundConfigurationView(hardware: $viewModel.config.hardware)\n         } header: {\n             summaryHeader(\n                 \"Sharing\",", "new": " struct VMConfigurationView: View {\n     @ViewBuilder\n     private var sharing: some View {\n         ConfigurationSection {\n+            SharingConfigurationView(configuration: $viewModel.config)\n         } header: {\n             summaryHeader(\n                 \"Sharing\",", "review": "I think this is a typo / copy-pasta \ud83d\ude06\r\n\r\nShould be SharingConfigurationView.", "dataset": "codereview_new", "original_id": 451, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nstruct VMConfigurationView: View {\n    @ViewBuilder\n    private var sharing: some View {\n        ConfigurationSection {\n            SharingConfigurationView(hardware: $viewModel.config.hardware)\n        } header: {\n            summaryHeader(\n                \"Sharing\",\n", "gpt_em_trim": 0, "gpt_bleu_trim": 83.04368, "model_code": " struct VMConfigurationView: View { \n @ViewBuilder \n private var sharing: some View { \n ConfigurationSection { \n SharingConfigurationView(hardware: $viewModel.config.hardware) \n } header: { \n summaryHeader( \n \"Sharing\",", "model_em_trim": 0, "model_bleu_trim": 83.0436753584906, "error_type_gpt": "Need Domain Knowledge"}
{"id": 124, "old": " def density_to_air_quality_nitrogen_dioxide(density: float) -> int:\n \n \n def density_to_air_quality_voc(density: float) -> int:\n-    \"\"\"Map VOCs \u00b5g/m3 to HomeKit AirQuality level.\"\"\"\n-    if density <= 250:\n         return 1\n-    if density <= 500:\n         return 2\n-    if density <= 1000:\n         return 3\n-    if density <= 3000:\n         return 4\n-    return 5\n \n \n def get_persist_filename_for_entry_id(entry_id: str) -> str:", "new": " def density_to_air_quality_nitrogen_dioxide(density: float) -> int:\n \n \n def density_to_air_quality_voc(density: float) -> int:\n+    \"\"\"Map VOCs \u00b5g/m3 to HomeKit AirQuality level.\n+\n+    The VOC mappings use the IAQ guidelines for Europe released by the WHO (World Health Organization).\n+    Referenced from Sensirion_Gas_Sensors_SGP3x_TVOC_Concept.pdf\n+    https://github.com/paulvha/svm30/blob/master/extras/Sensirion_Gas_Sensors_SGP3x_TVOC_Concept.pdf\n+    \"\"\"\n+    if density <= 250:  # WHO IAQ 1 (HomeKit: Excellent)\n         return 1\n+    if density <= 500:  # WHO IAQ 2 (HomeKit: Good)\n         return 2\n+    if density <= 1000:  # WHO IAQ 3 (HomeKit: Fair)\n         return 3\n+    if density <= 3000:  # WHO IAQ 4 (HomeKit: Inferior)\n         return 4\n+    return 5  # WHOA IAQ 5 (HomeKit: Poor)\n \n \n def get_persist_filename_for_entry_id(entry_id: str) -> str:", "review": "My main grind with this PR is: We are breaking it again; and to be honest, that is a bit unacceptable to break twice in such a short time.\r\n\r\nI'm not using this myself, so have no personal opinion on it, but I really do want an extensive reasoning and definitive decision documented in the code comments here, that will block/prevent any breaking change to this again in the future.\r\n", "dataset": "codereview_new", "original_id": 9673, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\ndef density_to_air_quality_nitrogen_dioxide(density: float) -> int:\n    \"\"\"Map nitrogen dioxide density (ppb) to HomeKit AirQuality level.\"\"\"\n    if density <= 100:\n        return 1\n    if density <= 200:\n        return 2\n    if density <= 300:\n        return 3\n    if density <= 400:\n        return 4\n    return 5\n\n\ndef density_to_air_quality_voc(density: float) -> int:\n    \"\"\"Map VOCs density (\u00b5g/m3) to HomeKit AirQuality level.\"\"\"\n    if density <= 250:\n        return 1\n    if density <= 500:\n        return 2\n    if density <= 1000:\n        return 3\n    if density <= 3000:\n        return 4\n    return 5\n\n\ndef get_persist_filename_for_entry_id(entry_id: str) -> str:\n    \"\"\"Return the filename for the given entry ID.\"\"\"\n    return f\"{entry_id}.json\"\n", "gpt_em_trim": 0, "gpt_bleu_trim": 43.72335, "model_code": " def density_to_air_quality_nitrogen_dioxide(density: float) -> int: \n return density \n def get_persist_filename_for_entry_id(entry_id: str) -> str:", "model_em_trim": 0, "model_bleu_trim": 2.1574410412747236, "error_type_gpt": "Need Domain Knowledge"}
{"id": 125, "old": " func (hc *HotstuffCollector) SetQCView(view uint64) {\n \thc.qcView.Set(float64(view))\n }\n \n func (hc *HotstuffCollector) SetTCView(view uint64) {\n \thc.tcView.Set(float64(view))\n }\n \n func (hc *HotstuffCollector) VoteProcessingDuration(duration time.Duration) {\n \thc.voteProcessingDuration.Observe(duration.Seconds())\n }\n \n func (hc *HotstuffCollector) TimeoutObjectProcessingDuration(duration time.Duration) {\n \thc.timeoutProcessingDuration.Observe(duration.Seconds())\n }", "new": " func (hc *HotstuffCollector) SetQCView(view uint64) {\n \thc.qcView.Set(float64(view))\n }\n \n+// SetTCView reports the view of the newest known TC\n func (hc *HotstuffCollector) SetTCView(view uint64) {\n \thc.tcView.Set(float64(view))\n }\n \n+// VoteProcessingDuration reports the processing time for a single vote \n func (hc *HotstuffCollector) VoteProcessingDuration(duration time.Duration) {\n \thc.voteProcessingDuration.Observe(duration.Seconds())\n }\n \n+// TimeoutObjectProcessingDuration reports the processing time for a TimeoutObject\n func (hc *HotstuffCollector) TimeoutObjectProcessingDuration(duration time.Duration) {\n \thc.timeoutProcessingDuration.Observe(duration.Seconds())\n }", "review": "```suggestion\r\n// VoteProcessingDuration reports the processing time for a single vote \r\nfunc (hc *HotstuffCollector) VoteProcessingDuration(duration time.Duration) {\r\n```", "dataset": "codereview_new", "original_id": 5461, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-add", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-add", "gpt_code": "\n// VoteProcessingDuration reports the processing time for a single vote \nfunc (hc *HotstuffCollector) VoteProcessingDuration(duration time.Duration) {\n    hc.voteProcessingDuration.Observe(duration.Seconds())\n}\n\n// TimeoutObjectProcessingDuration reports the processing time for a timeout object \nfunc (hc *HotstuffCollector) TimeoutObjectProcessingDuration(duration time.Duration) {\n    hc.timeoutProcessingDuration.Observe(duration.Seconds())\n}\n\n// SetQCView sets the QC view \nfunc (hc *HotstuffCollector) SetQCView(view uint64) {\n    hc.qcView.Set(float64(view))\n}\n\n// SetTCView sets the TC view \nfunc (hc *HotstuffCollector) SetTCView(view uint64) {\n    hc.tcView.Set(float64(view))\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 83.70403, "model_code": " func (hc *HotstuffCollector) SetQCView(view uint64) { \n hc.qcView.Set(float64(view)) \n }", "model_em_trim": 0, "model_bleu_trim": 1.2277339903068436}
{"id": 126, "old": " public function getFromDb()\n \t\t\t\t\tunset($field['index'], $field['origtype']);\n \n \t\t\t\t\t// Field type\n-\t\t\t\t\tif ($field['length'])\n \t\t\t\t\t{\n \t\t\t\t\t\t$field['type'] .= '(' . $field['length'] . (isset($field['precision']) ? ',' . $field['precision'] : '') . ')';\n ", "new": " public function getFromDb()\n \t\t\t\t\tunset($field['index'], $field['origtype']);\n \n \t\t\t\t\t// Field type\n+\t\t\t\t\tif ($field['length'] ?? null)\n \t\t\t\t\t{\n \t\t\t\t\t\t$field['type'] .= '(' . $field['length'] . (isset($field['precision']) ? ',' . $field['precision'] : '') . ')';\n ", "review": "```suggestion\r\n\t\t\t\t\tif ($field['length'] ?? null)\r\n```", "dataset": "codereview_new", "original_id": 4104, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\npublic function getFromDb()\n    unset($field['index'], $field['origtype']);\n\n    // Field type\n    if ($field['length'] ?? null)\n    {\n        $field['type'] .= '(' . $field['length'] . ($field['precision'] ?? '') . ')';\n    }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 80.43684, "model_code": " public function getFromDb() \n unset($field['index'], $field['origtype']); \n { \n $field['type'] .= '(' . $field['length'] . (isset($field['precision']) ? ',' . $field['precision'] : '') . ')'; \n }", "model_em_trim": 0, "model_bleu_trim": 79.23619075498237, "error_type_gpt": "Model fallacy"}
{"id": 127, "old": " package com.chuckerteam.chucker.api\n \n import android.content.Context\n import android.content.Intent\n-import android.net.Uri\n-import android.widget.Toast\n-import com.chuckerteam.chucker.R\n \n /**\n  * No-op implementation.", "new": " package com.chuckerteam.chucker.api\n \n import android.content.Context\n import android.content.Intent\n \n /**\n  * No-op implementation.", "review": "Do we need these new imports here?", "dataset": "codereview_new", "original_id": 1984, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\npackage com.chuckerteam.chucker.api\n\nimport android.content.Context\nimport android.widget.Toast\nimport com.chuckerteam.chucker.R\n\n/**\n * No-op implementation.\n */\nclass Chucker {\n\n    companion object {\n\n        /**\n         * Launches the Chucker UI.\n         *\n         * @param context The current context.\n         */\n        fun show(context: Context) {\n            Toast.makeText(context, R.string.chucker_noop_message, Toast.LENGTH_SHORT).show()\n        }\n\n        /**\n         * Returns the Intent to launch the Chucker UI.\n         *\n         * @param context The current context.\n         * @return The Intent to launch the Chucker UI.\n         */\n        fun getLaunchIntent(context: Context): Intent? {\n            Toast.makeText(context, R.string.chucker_noop_message, Toast.LENGTH_SHORT).show()\n            return null\n        }\n\n        /**\n         * Returns the Intent to share the HTTP transactions as a plain text.\n         *\n         * @param context The current context.\n         * @return The Intent to share the HTTP transactions as a plain text.\n         */\n        fun getShareIntent(context: Context): Intent? {\n            Toast.makeText(context, R.string.chucker_noop_message, Toast.LENGTH_SHORT).show()\n            return null\n        }\n\n        /**\n         * Returns the Intent to share the HTTP transactions as a cURL command.\n         *\n         * @param context The current context.\n         * @return The Intent to share the HTTP transactions as a cURL command.\n         */\n        fun getCurlIntent(context: Context): Intent? {\n            Toast.makeText(context, R.string.chucker_noop_message, Toast.LENGTH_SHORT).show()\n            return null\n        }\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 65.93549, "model_code": " package com.chuckerteam.chucker.api \n import com.chuckerteam.chucker.R; \n /** \n * No-op implementation.", "model_em_trim": 0, "model_bleu_trim": 57.59652448895951, "error_type_gpt": "Unclear Location"}
{"id": 128, "old": " def test_spectral_bisection():\n     pytest.importorskip(\"scipy\")\n     G = nx.barbell_graph(3, 0)\n     C = nx.spectral_bisection(G)\n-    assert C == [{0, 1, 2}, {3, 4, 5}]\n \n     mapping = dict(enumerate(\"badfec\"))\n     G = nx.relabel_nodes(G, mapping)\n     C = nx.spectral_bisection(G)\n-    assert C == [\n         {mapping[0], mapping[1], mapping[2]},\n         {mapping[3], mapping[4], mapping[5]},\n-    ]\n \n \n def check_eigenvector(A, l, x):", "new": " def test_spectral_bisection():\n     pytest.importorskip(\"scipy\")\n     G = nx.barbell_graph(3, 0)\n     C = nx.spectral_bisection(G)\n+    assert C == ({0, 1, 2}, {3, 4, 5})\n \n     mapping = dict(enumerate(\"badfec\"))\n     G = nx.relabel_nodes(G, mapping)\n     C = nx.spectral_bisection(G)\n+    assert C == (\n         {mapping[0], mapping[1], mapping[2]},\n         {mapping[3], mapping[4], mapping[5]},\n+    )\n \n \n def check_eigenvector(A, l, x):", "review": "Does this need scipy or numpy?  The function seems to use numpy.\r\n```suggestion\r\n    pytest.importorskip(\"numpy\")\r\n```", "dataset": "codereview_new", "original_id": 5639, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ndef test_spectral_bisection():\n    pytest.importorskip(\"numpy\")\n    G = nx.barbell_graph(3, 0)\n    C = nx.spectral_bisection(G)\n    assert C == [{0, 1, 2}, {3, 4, 5}]\n\n    mapping = dict(enumerate(\"badfec\"))\n    G = nx.relabel_nodes(G, mapping)\n    C = nx.spectral_bisection(G)\n    assert C == [\n        {mapping[0], mapping[1], mapping[2]},\n        {mapping[3], mapping[4], mapping[5]},\n    ]\n\n\ndef check_eigenvector(A, l, x):\n", "gpt_em_trim": 0, "gpt_bleu_trim": 91.2285, "model_code": " def test_spectral_bisection(): \n pytest.importorskip(\"numpy\") \n def check_eigenvector(A, l, x):", "model_em_trim": 0, "model_bleu_trim": 1.8628911734912232}
{"id": 129, "old": " async def test_reauth(\n \n     assert result[\"type\"] == data_entry_flow.FlowResultType.ABORT\n     assert result[\"reason\"] == \"reauth_successful\"\n-    assert len(hass.config_entries.async_entries()) == 1    ", "new": " async def test_reauth(\n \n     assert result[\"type\"] == data_entry_flow.FlowResultType.ABORT\n     assert result[\"reason\"] == \"reauth_successful\"\n+    assert len(hass.config_entries.async_entries()) == 1", "review": "```suggestion\r\n    assert len(hass.config_entries.async_entries()) == 1\r\n```", "dataset": "codereview_new", "original_id": 9599, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nasync def test_reauth(\n\n    assert result[\"type\"] == data_entry_flow.FlowResultType.ABORT\n    assert result[\"reason\"] == \"reauth_successful\"\n    assert len(hass.config_entries.async_entries()) == 1\n)\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " async def test_reauth( \n result): \n assert result[\"type\"] == data_entry_flow.FlowResultType.ABORT \n assert result[\"reason\"] == \"reauth_successful\" \n assert len(hass.config_entries.async_entries()) == 1", "model_em_trim": 0, "model_bleu_trim": 92.22474698274425}
{"id": 130, "old": "\n  * limitations under the License.\n  */\n /**\n- * Provides classes for testing Streams applications with mocked inputs.\n  */\n package org.apache.kafka.streams.test;\n\\ No newline at end of file", "new": "\n  * limitations under the License.\n  */\n /**\n+ * Provides classes for testing Kafka Streams applications with mocked inputs.\n  */\n package org.apache.kafka.streams.test;\n\\ No newline at end of file", "review": "```suggestion\r\n * Provides classes for testing Kafka Streams applications with mocked inputs.\r\n```", "dataset": "codereview_new", "original_id": 8391, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n\n * limitations under the License.\n */\n/**\n * Provides classes for testing Kafka Streams applications with mocked inputs.\n */\npackage org.apache.kafka.streams.test;\n", "gpt_em_trim": 0, "gpt_bleu_trim": 85.03033, "model_code": " * limitations under the License. \n */ \n package org.apache.kafka.streams.test; \n No newline at end of file", "model_em_trim": 0, "model_bleu_trim": 52.004176472857985, "error_type_gpt": "Insignificant Omission"}
{"id": 131, "old": "\n package com.duckduckgo.savedsites.api.service\n \n import android.net.Uri\n \n interface SavedSitesExporter {\n     suspend fun export(uri: Uri): ExportSavedSitesResult\n }\n ", "new": "\n package com.duckduckgo.savedsites.api.service\n \n import android.net.Uri\n+import com.duckduckgo.savedsites.api.models.BookmarkFolder\n \n+/**\n+ * Class that takes care of exporting [SavedSites]\n+ * This is used to export [SavedSites] to another Browser\n+ */\n interface SavedSitesExporter {\n+    /**\n+     * Generates a HTML based file with all [SavedSites] that the user has\n+     * in Netscape format.\n+     * @param uri of the [File] where we'll store the data\n+     * @return [ExportSavedSitesResult] result of the operation\n+     */\n     suspend fun export(uri: Uri): ExportSavedSitesResult\n }\n ", "review": "we need to write docs here.", "dataset": "codereview_new", "original_id": 2007, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-add", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-add", "gpt_code": "\npackage com.duckduckgo.savedsites.api.service\n\nimport android.net.Uri\n\n/**\n * Interface for exporting saved sites.\n */\ninterface SavedSitesExporter {\n\n    /**\n     * Exports saved sites to the specified URI.\n     *\n     * @param uri The URI to export the saved sites to.\n     * @return The result of the export operation.\n     */\n    suspend fun export(uri: Uri): ExportSavedSitesResult\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 34.42701, "model_code": " package com.duckduckgo.savedsites.api.service \n interface SavedSitesExporter { \n export(uri: Uri): ExportSavedSitesResult \n }", "model_em_trim": 0, "model_bleu_trim": 1.1953954682592836, "error_type_gpt": "Need Domain Knowledge"}
{"id": 132, "old": " int main( int argc, char ** argv )\n         std::string baseName = System::GetBasename( argv[0] );\n \n         std::cerr << baseName << \" generates an image with colors based on a provided palette file.\" << std::endl\n-                  << \"Syntax: \" << baseName << \" palette_file.pal output.bmp\" << std::endl;\n         return EXIT_FAILURE;\n     }\n ", "new": " int main( int argc, char ** argv )\n         std::string baseName = System::GetBasename( argv[0] );\n \n         std::cerr << baseName << \" generates an image with colors based on a provided palette file.\" << std::endl\n+                  << \"Syntax: \" << baseName << \" palette_file.pal output.[bmp|png]\" << std::endl;\n         return EXIT_FAILURE;\n     }\n ", "review": "This tool may (and will, if built with `FHEROES2_WITH_IMAGE` and I pass the `output.png` as the second parameter) also generate PNG files. You can add this to the description, for example:\r\n\r\n```suggestion\r\n                  << \"Syntax: \" << baseName << \" palette_file.pal output.[bmp|png]\" << std::endl;\r\n```\r\n\r\nor you can not add it - it's up to you.", "dataset": "codereview_new", "original_id": 4552, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nint main( int argc, char ** argv )\n        std::string baseName = System::GetBasename( argv[0] );\n\n        std::cerr << baseName << \" generates an image with colors based on a provided palette file.\" << std::endl\n                  << \"Syntax: \" << baseName << \" palette_file.pal output.[bmp|png]\" << std::endl;\n        return EXIT_FAILURE;\n    }\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " int main( int argc, char ** argv ) \n std::string baseName = System::GetBasename( argv[0] ); \n return EXIT_FAILURE; \n } \n", "model_em_trim": 0, "model_bleu_trim": 17.0898632985317}
{"id": 133, "old": " bool System::Unlink( const std::string & path )\n }\n \n #if !defined( _WIN32 ) && !defined( ANDROID )\n-// TODO: Android filesystem is case-sensitive so it should use the code below.\n-//       However, in Android an application has access only to a specific path on the system.\n-\n // based on: https://github.com/OneSadCookie/fcaseopen\n bool System::GetCaseInsensitivePath( const std::string & path, std::string & correctedPath )\n {", "new": " bool System::Unlink( const std::string & path )\n }\n \n #if !defined( _WIN32 ) && !defined( ANDROID )\n // based on: https://github.com/OneSadCookie/fcaseopen\n bool System::GetCaseInsensitivePath( const std::string & path, std::string & correctedPath )\n {", "review": "BTW, Android filesystem is in fact case-insensitive, at least in those places that are accessible by users (`/sdcard`) and where game data files are placed. We don't need to use the workarounds for this.", "dataset": "codereview_new", "original_id": 4623, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-remove", "category_type_user1": "Documentation-remove", "category_type_user2": "Documentation-remove", "gpt_code": "\nbool System::Unlink( const std::string & path )\n}\n\n#if !defined( _WIN32 ) && !defined( ANDROID )\n// TODO: Android filesystem is case-insensitive in accessible places, so we don't need to use the code below.\nbool System::GetCaseInsensitivePath( const std::string & path, std::string & correctedPath )\n{\n", "gpt_em_trim": 0, "gpt_bleu_trim": 67.55915, "model_code": " bool System::Unlink( const std::string & path ) \n } \n // based on: https://github.com/OneSadCookie/fcaseopen \n bool System::GetCaseInsensitivePath( const std::string & path, std::string & correctedPath ) \n {", "model_em_trim": 0, "model_bleu_trim": 73.52820144268777}
{"id": 134, "old": " extension ConversationViewController {\n         NSLayoutConstraint.activate([\n             conversationBarController.view.leadingAnchor.constraint(equalTo: view.leadingAnchor),\n             conversationBarController.view.trailingAnchor.constraint(equalTo: view.trailingAnchor),\n-            conversationBarController.view.topAnchor.constraint(equalTo: view.topAnchor)\n-        ])\n-        NSLayoutConstraint.activate([\n             contentViewController.view.leadingAnchor.constraint(equalTo: view.leadingAnchor),\n             contentViewController.view.trailingAnchor.constraint(equalTo: view.trailingAnchor),\n             contentViewController.view.topAnchor.constraint(equalTo: view.topAnchor)", "new": " extension ConversationViewController {\n         NSLayoutConstraint.activate([\n             conversationBarController.view.leadingAnchor.constraint(equalTo: view.leadingAnchor),\n             conversationBarController.view.trailingAnchor.constraint(equalTo: view.trailingAnchor),\n+            conversationBarController.view.topAnchor.constraint(equalTo: view.topAnchor),\n             contentViewController.view.leadingAnchor.constraint(equalTo: view.leadingAnchor),\n             contentViewController.view.trailingAnchor.constraint(equalTo: view.trailingAnchor),\n             contentViewController.view.topAnchor.constraint(equalTo: view.topAnchor)", "review": "Same here", "dataset": "codereview_new", "original_id": 656, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nextension ConversationViewController {\n        NSLayoutConstraint.activate([\n            conversationBarController.view.leadingAnchor.constraint(equalTo: view.leadingAnchor),\n            conversationBarController.view.trailingAnchor.constraint(equalTo: view.trailingAnchor),\n            conversationBarController.view.topAnchor.constraint(equalTo: view.topAnchor),\n            conversationBarController.view.bottomAnchor.constraint(equalTo: view.bottomAnchor)\n        ])\n        NSLayoutConstraint.activate([\n            contentViewController.view.leadingAnchor.constraint(equalTo: view.leadingAnchor),\n            contentViewController.view.trailingAnchor.constraint(equalTo: view.trailingAnchor),\n            contentViewController.view.topAnchor.constraint(equalTo: conversationBarController.view.bottomAnchor),\n            contentViewController.view.bottomAnchor.constraint(equalTo: view.bottomAnchor)\n        ])\n", "gpt_em_trim": 0, "gpt_bleu_trim": 77.31836, "model_code": " extension ConversationViewController { \n NSLayoutConstraint.activate([ \n conversationBarController.view.leadingAnchor.constraint(equalTo: view.leadingAnchor), \n conversationBarController.view.trailingAnchor.constraint(equalTo: view.trailingAnchor) \n ]) \n NSLayoutConstraint.activate([ \n contentViewController.view.leadingAnchor.constraint(equalTo: view.leadingAnchor), \n contentViewController.view.trailingAnchor.constraint(equalTo: view.trailingAnchor) \n ])", "model_em_trim": 0, "model_bleu_trim": 65.20551733460113}
{"id": 135, "old": " private[effect] class JvmCpuStarvationMetrics private (mbean: CpuStarvationMbean\n   override def recordClockDrift(drift: FiniteDuration): IO[Unit] = mbean.recordDrift(drift)\n }\n \n-object JvmCpuStarvationMetrics {\n   private[this] val mBeanObjectName = new ObjectName(\"cats.effect.metrics:type=CpuStarvation\")\n \n   private[this] def warning(th: Throwable) = {", "new": " private[effect] class JvmCpuStarvationMetrics private (mbean: CpuStarvationMbean\n   override def recordClockDrift(drift: FiniteDuration): IO[Unit] = mbean.recordDrift(drift)\n }\n \n+private[effect] object JvmCpuStarvationMetrics {\n   private[this] val mBeanObjectName = new ObjectName(\"cats.effect.metrics:type=CpuStarvation\")\n \n   private[this] def warning(th: Throwable) = {", "review": "Minor nit, but can we make this `private[effect]` as well?", "dataset": "codereview_new", "original_id": 3612, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nprivate[effect] class JvmCpuStarvationMetrics private[effect] (mbean: CpuStarvationMbean) {\n  override def recordClockDrift(drift: FiniteDuration): IO[Unit] = mbean.recordDrift(drift)\n}\n\nobject JvmCpuStarvationMetrics {\n  private[this] val mBeanObjectName = new ObjectName(\"cats.effect.metrics:type=CpuStarvation\")\n\n  private[this] def warning(th: Throwable) = {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 89.99163, "model_code": " private[effect] class JvmCpuStarvationMetrics private (mbean: CpuStarvationMbean \n override def recordClockDrift(drift: FiniteDuration): IO[Unit] = mbean.recordDrift(drift) \n private[effect] class JvmCpuStarvationMetrics { \n private[this] val mBeanObjectName = new ObjectName(\"cats.effect.metrics:type=CpuStarvation\") \n private[this] def warning(th: Throwable) = {", "model_em_trim": 0, "model_bleu_trim": 93.1093602377654, "error_type_gpt": "Unclear Location"}
{"id": 136, "old": " static void createCompactedTopic(String topicName, short partitions, short repli\n             }\n             if (cause instanceof UnsupportedVersionException) {\n                 log.debug(\"Unable to create topic '{}' since the brokers do not support the CreateTopics API.\" +\n-                                \" Falling back to assume topic exist or will be auto-created by the broker.\",\n                         topicName);\n             }\n             if (cause instanceof ClusterAuthorizationException) {", "new": " static void createCompactedTopic(String topicName, short partitions, short repli\n             }\n             if (cause instanceof UnsupportedVersionException) {\n                 log.debug(\"Unable to create topic '{}' since the brokers do not support the CreateTopics API.\" +\n+                                \" Falling back to assume topic exists or will be auto-created by the broker.\",\n                         topicName);\n             }\n             if (cause instanceof ClusterAuthorizationException) {", "review": "Should it be `exists`?", "dataset": "codereview_new", "original_id": 8482, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "\nstatic void createCompactedTopic(String topicName, short partitions, short replicationFactor) {\n            try {\n                AdminUtils.createTopic(zkUtils, topicName, partitions, replicationFactor, new Properties(), new RackAwareMode.Enforced$());\n            } catch (TopicExistsException e) {\n                log.debug(\"Topic '{}' already exists.\", topicName);\n            } catch (Throwable cause) {\n                if (cause instanceof UnsupportedVersionException) {\n                    log.debug(\"Unable to create topic '{}' since the brokers do not support the CreateTopics API.\" +\n                                    \" Falling back to assume topic exists or will be auto-created by the broker.\",\n                            topicName);\n                }\n                if (cause instanceof ClusterAuthorizationException) {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 52.47032, "model_code": " static void createCompactedTopic(String topicName, short partitions, short repli \n } \n if (cause instanceof UnsupportedVersionException) { \n log.debug(\"Unable to create topic '{}' since the brokers do not support the CreateTopics API.\" + \n \" Falling back to assume topic exists or will be auto-created by the broker.\", \n topicName); \n } \n if (cause instanceof ClusterAuthorizationException) {", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 137, "old": " func newPipelineRegistries(views map[Reader][]view.View) *pipelineRegistry {\n \t}\n }\n \n-func (reg *pipelineRegistry) createInt64Aggregators(inst view.Instrument, instUnit unit.Unit) ([]internal.Aggregator[int64], error) {\n-\treturn createAggregators[int64](reg, inst, instUnit)\n-}\n-\n-func (reg *pipelineRegistry) createFloat64Aggregators(inst view.Instrument, instUnit unit.Unit) ([]internal.Aggregator[float64], error) {\n-\treturn createAggregators[float64](reg, inst, instUnit)\n-}\n-\n // TODO (#3053) Only register callbacks if any instrument matches in a view.\n func (reg *pipelineRegistry) registerCallback(fn func(context.Context)) {\n \tfor _, pipe := range reg.pipelines {", "new": " func newPipelineRegistries(views map[Reader][]view.View) *pipelineRegistry {\n \t}\n }\n \n // TODO (#3053) Only register callbacks if any instrument matches in a view.\n func (reg *pipelineRegistry) registerCallback(fn func(context.Context)) {\n \tfor _, pipe := range reg.pipelines {", "review": "These do not seem needed. Each call site can just as easily call `createAggregators[int64 | float64](reg, inst, instUnit)` directly.", "dataset": "codereview_new", "original_id": 5899, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nfunc newPipelineRegistries(views map[Reader][]view.View) *pipelineRegistry {\n\t}\n}\n\n// createInt64Aggregators creates int64 aggregators for the given instrument and unit.\nfunc (reg *pipelineRegistry) createInt64Aggregators(inst view.Instrument, instUnit unit.Unit) ([]internal.Aggregator[int64], error) {\n\treturn createAggregators[int64](reg, inst, instUnit)\n}\n\n// createFloat64Aggregators creates float64 aggregators for the given instrument and unit.\nfunc (reg *pipelineRegistry) createFloat64Aggregators(inst view.Instrument, instUnit unit.Unit) ([]internal.Aggregator[float64], error) {\n\treturn createAggregators[float64](reg, inst, instUnit)\n}\n\n// TODO (#3053) Only register callbacks if any instrument matches in a view.\nfunc (reg *pipelineRegistry) registerCallback(fn func(context.Context)) {\n\tfor _, pipe := range reg.pipelines {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 36.04226, "model_code": " func newPipelineRegistries(views map[Reader][]view.View) *pipelineRegistry { \n } \n } \n // TODO (#3053) Only register callbacks if any instrument matches in a view. \n func (reg *pipelineRegistry) registerCallback(fn func(context.Context)) { \n for _, pipe := range reg.pipelines {", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 138, "old": " protected function getCurrentSubgraphs(): ContentSubgraphs\n     {\n         $currentSubgraphs = [];\n         foreach ($this->getActiveContentGraphs() as $adapterName => $contentGraph) {\n-            $contentGraph->disableCache();\n             $currentSubgraphs[$adapterName] = $contentGraph->getSubgraphByIdentifier(\n                 $this->contentStreamIdentifier,\n                 $this->dimensionSpacePoint,", "new": " protected function getCurrentSubgraphs(): ContentSubgraphs\n     {\n         $currentSubgraphs = [];\n         foreach ($this->getActiveContentGraphs() as $adapterName => $contentGraph) {\n             $currentSubgraphs[$adapterName] = $contentGraph->getSubgraphByIdentifier(\n                 $this->contentStreamIdentifier,\n                 $this->dimensionSpacePoint,", "review": "uh, why is this needed? I am not sure this is correct actually, but it might mask actual errors during the tests.", "dataset": "codereview_new", "original_id": 4685, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nprotected function getCurrentSubgraphs(): ContentSubgraphs\n{\n    $currentSubgraphs = [];\n    foreach ($this->getActiveContentGraphs() as $adapterName => $contentGraph) {\n        // Commenting out the line below as it might mask actual errors during tests\n        // $contentGraph->disableCache();\n        $currentSubgraphs[$adapterName] = $contentGraph->getSubgraphByIdentifier(\n            $this->contentStreamIdentifier,\n            $this->dimensionSpacePoint,\n            // Adding a third parameter to the getSubgraphByIdentifier method to disable cache\n            false\n        );\n    }\n    return new ContentSubgraphs($currentSubgraphs);\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 67.34639, "model_code": " protected function getCurrentSubgraphs(): ContentSubgraphs \n { \n $currentSubgraphs = []; \n foreach ($this->getActiveContentGraphs() as $adapterName => $contentGraph) { \n $currentSubgraphs[$adapterName] = $contentGraph->getSubgraphByIdentifier( \n $this->contentStreamIdentifier, \n $this->dimensionSpacePoint,", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Code Style Difference"}
{"id": 139, "old": " object_name_xpath <- local({\n #'   Then, if `upstream` is not installed when this linter runs, a lint\n #'   will be thrown on this object (even though you don't \"own\" its name).\n #'\n-#' There are three workarounds: (1) always namespace-qualify usages,\n-#'   because this linter ignores names in `pkg::foo()` form; (2) use\n-#'   `@importFrom(pkg, foo)` instead of a blanket `import(pkg)` in your\n #'   NAMESPACE, because this linter takes these imported names as given;\n #'   and of course (3) install the package so that it's available in\n #'   the session where this linter is running.", "new": " object_name_xpath <- local({\n #'   Then, if `upstream` is not installed when this linter runs, a lint\n #'   will be thrown on this object (even though you don't \"own\" its name).\n #'\n+#' There are three options to get lintr to work correctly:\n+#'   (1) install the package so that it's available in\n+#'   the session where this linter is running; (2) always namespace-qualify\n+#'   usages, because this linter ignores names in `pkg::foo()` form; and\n+#'   (3) use `importFrom(pkg, foo)` instead of a blanket `import(pkg)` in your\n #'   NAMESPACE, because this linter takes these imported names as given;\n #'   and of course (3) install the package so that it's available in\n #'   the session where this linter is running.", "review": "Make this the first option?", "dataset": "codereview_new", "original_id": 2509, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nobject_name_xpath <- local({\n#'   There are three workarounds to avoid a lint error on this object if `upstream` is not installed when this linter runs:\n#'   (1) always namespace-qualify usages, because this linter ignores names in `pkg::foo()` form;\n#'   (2) use `@importFrom(pkg, foo)` instead of a blanket `import(pkg)` in your NAMESPACE, because this linter takes these imported names as given;\n#'   (3) install the package so that it's available in the session where this linter is running.\n#'\n#'   It's recommended to use the first option as it's the simplest and most straightforward.\n", "gpt_em_trim": 0, "gpt_bleu_trim": 56.72671, "model_code": " object_name_xpath <- local({ \n #'   Then, if `upstream` is not installed when this linter runs, a lint \n #'   will be thrown on this object (even though you don't \"own\" its name). \n #'   There are three workarounds: (1) always namespace-qualify usages, \n #'   because this linter ignores names in `pkg::foo()` form; (2) use \n #'   `@importFrom(pkg, foo)` instead of a blanket `import(pkg)` in your \n #'   NAMESPACE, because this linter takes these imported names as given; \n #'   and of course (3) install the package so that it's available in \n #'   the session where this linter is running.", "model_em_trim": 0, "model_bleu_trim": 73.11724854667932}
{"id": 140, "old": " use_binder_badge <- function(ref = git_default_branch(), urlpath = NULL) {\n }\n \n #' @rdname badges\n-#' @param cloudurl A link to an existing [RStudio Cloud](https://rstudio.cloud) project\n #' @export\n use_rscloud_badge <- function(cloudurl) {\n   if (grepl(\"rstudio.cloud/project\", cloudurl)) {", "new": " use_binder_badge <- function(ref = git_default_branch(), urlpath = NULL) {\n }\n \n #' @rdname badges\n+#' @param cloudurl A link to an existing [RStudio Cloud](https://rstudio.cloud)\n+#'   project. See the [RStudio Cloud\n+#'   documentation](https://rstudio.cloud/learn/guide#project-settings-access)\n+#'   for details on how to set project access and obtain a project link.\n #' @export\n use_rscloud_badge <- function(cloudurl) {\n   if (grepl(\"rstudio.cloud/project\", cloudurl)) {", "review": "Maybe `url` is a better name?", "dataset": "codereview_new", "original_id": 2577, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nuse_binder_badge <- function(ref = git_default_branch(), url = NULL) {\n}\n\n#' @rdname badges\n#' @param cloudurl A link to an existing [RStudio Cloud](https://rstudio.cloud) project\n#' @export\nuse_rscloud_badge <- function(cloudurl) {\n  if (grepl(\"rstudio.cloud/project\", cloudurl)) {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 54.97, "model_code": " use_binder_badge <- function(ref = git_default_branch(), urlpath = NULL) { \n } \n #' @rdname badges \n #' @param cloudurl A link to an existing [RStudio Cloud](https://rstudio.cloud) project \n #' @export \n use_rscloud_badge <- function(cloudurl) { \n if (grepl(\"rstudio.cloud/project\", cloudurl)) {", "model_em_trim": 0, "model_bleu_trim": 56.70583600581735}
{"id": 141, "old": " def get_context(context):\n \tcontext[\"title\"] = \"Login\"\n \tcontext[\"provider_logins\"] = []\n \tcontext[\"disable_signup\"] = frappe.utils.cint(frappe.get_website_settings(\"disable_signup\"))\n-\tcontext[\"disable_user_pass_login\"] = frappe.utils.cint(frappe.get_system_settings(\"disable_user_pass_login\"))\n \tcontext[\"logo\"] = frappe.get_website_settings(\"app_logo\") or frappe.get_hooks(\"app_logo_url\")[-1]\n \tcontext[\"app_name\"] = (\n \t\tfrappe.get_website_settings(\"app_name\") or frappe.get_system_settings(\"app_name\") or _(\"Frappe\")", "new": " def get_context(context):\n \tcontext[\"title\"] = \"Login\"\n \tcontext[\"provider_logins\"] = []\n \tcontext[\"disable_signup\"] = frappe.utils.cint(frappe.get_website_settings(\"disable_signup\"))\n+\tcontext[\"disable_user_pass_login\"] = frappe.utils.cint(\n+\t\tfrappe.get_system_settings(\"disable_user_pass_login\")\n+\t)\n \tcontext[\"logo\"] = frappe.get_website_settings(\"app_logo\") or frappe.get_hooks(\"app_logo_url\")[-1]\n \tcontext[\"app_name\"] = (\n \t\tfrappe.get_website_settings(\"app_name\") or frappe.get_system_settings(\"app_name\") or _(\"Frappe\")", "review": "Based on first impression it looks like this checkbox just removes user/pass section from login page. However, one can very easily log in by sending a POST request. We should disable that too with same settings. \r\n\r\n\r\n\r\n```javascript\r\nfetch(\"http://site\", {\r\n  \"headers\": {\r\n    \"accept\": \"application/json, text/javascript, */*; q=0.01\",\r\n    \"accept-language\": \"en-GB,en;q=0.9\",\r\n    \"content-type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\r\n    \"x-frappe-cmd\": \"login\",\r\n    \"x-requested-with\": \"XMLHttpRequest\"\r\n  },\r\n  \"body\": \"cmd=login&usr=Administrator&pwd=admin&device=desktop\",\r\n  \"method\": \"POST\",\r\n\r\n});\r\n```", "dataset": "codereview_new", "original_id": 8835, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ndef get_context(context):\n\tcontext[\"title\"] = \"Login\"\n\tcontext[\"provider_logins\"] = []\n\tcontext[\"disable_signup\"] = frappe.utils.cint(frappe.get_website_settings(\"disable_signup\"))\n\tcontext[\"disable_user_pass_login\"] = frappe.utils.cint(frappe.get_system_settings(\"disable_user_pass_login\"))\n\tcontext[\"logo\"] = frappe.get_website_settings(\"app_logo\") or frappe.get_hooks(\"app_logo_url\")[-1]\n\tcontext[\"app_name\"] = (\n\t\tfrappe.get_website_settings(\"app_name\") or frappe.get_system_settings(\"app_name\") or _(\"Frappe\")\n\t)\n\t\n\t# Disable login via POST request\n\tcontext[\"disable_login_via_post\"] = frappe.utils.cint(frappe.get_system_settings(\"disable_login_via_post\"))\n", "gpt_em_trim": 0, "gpt_bleu_trim": 99.45429, "model_code": " def get_context(context): \n context[\"title\"] = \"Login\" \n context[\"provider_logins\"] = [] \n context[\"logo\"] = frappe.get_website_settings(\"app_logo\") or frappe.get_hooks(\"app_logo_url\")[-1] \n context[\"app_name\"] = ( \n frappe.get_website_settings(\"app_name\") or frappe.get_system_settings(\"app_name\") or _(\"Frappe\")", "model_em_trim": 0, "model_bleu_trim": 55.643389824542844}
{"id": 142, "old": " class CityCombatant(val city: CityInfo) : ICombatant {\n         return getCityStrength()\n     }\n \n-    private fun getCityStrength(combatAction: CombatAction = CombatAction.Defend): Int { // Civ fanatics forum, from a modder who went through the original code\n         val modConstants = getCivInfo().gameInfo.ruleSet.modOptions.constants\n         var strength = modConstants.cityStrengthBase\n         strength += (city.population.population * modConstants.cityStrengthPerPop) // Each 5 pop gives 2 defence", "new": " class CityCombatant(val city: CityInfo) : ICombatant {\n         return getCityStrength()\n     }\n \n+    @Suppress(\"MemberVisibilityCanBePrivate\")\n+    fun getCityStrength(combatAction: CombatAction = CombatAction.Defend): Int { // Civ fanatics forum, from a modder who went through the original code\n         val modConstants = getCivInfo().gameInfo.ruleSet.modOptions.constants\n         var strength = modConstants.cityStrengthBase\n         strength += (city.population.population * modConstants.cityStrengthPerPop) // Each 5 pop gives 2 defence", "review": "Should stay public", "dataset": "codereview_new", "original_id": 1411, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass CityCombatant(val city: CityInfo) : ICombatant {\n    fun getCityStrength(combatAction: CombatAction = CombatAction.Defend): Int { // Civ fanatics forum, from a modder who went through the original code\n        val modConstants = getCivInfo().gameInfo.ruleSet.modOptions.constants\n        var strength = modConstants.cityStrengthBase\n        strength += (city.population.population * modConstants.cityStrengthPerPop) // Each 5 pop gives 2 defence\n        return strength\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 85.82457, "model_code": " class CityCombatant(val city: CityInfo) : ICombatant { \n return getCityStrength() \n } \n \n public fun getCityStrength(combatAction: CombatAction = CombatAction.Defend): Int { // Civ fanatics forum, from a modder who went through the original code \n val modConstants = getCivInfo().gameInfo.ruleSet.modOptions.constants \n var strength = modConstants.cityStrengthBase \n strength += (city.population.population * modConstants.cityStrengthPerPop) // Each 5 pop gives 2 defence", "model_em_trim": 0, "model_bleu_trim": 90.78281993481097}
{"id": 143, "old": " object Battle {\n                     .toFloat() * 0.4f\n             )\n             /** Between 0 and 1.  Defaults to turn and location-based random to avoid save scumming */\n-            return Random(attacker.getCivInfo().gameInfo.turns * defender.getTile().position.hashCode().toLong()).nextFloat() <= captureChance\n         }\n \n         fun unitGainFromEncampment(): Boolean {", "new": " object Battle {\n                     .toFloat() * 0.4f\n             )\n             /** Between 0 and 1.  Defaults to turn and location-based random to avoid save scumming */\n+            val random = Random(attacker.getCivInfo().gameInfo.turns * defender.getTile().position.hashCode())\n+            return random.nextFloat() <= captureChance\n         }\n \n         fun unitGainFromEncampment(): Boolean {", "review": "```suggestion\r\n            val random = Random(attacker.getCivInfo().gameInfo.turns * defender.getTile().position.hashCode())\r\n            return random.nextFloat() <= captureChance\r\n```\r\nFor readability (line is too long) separate 'what is random' and 'what do we do with it'", "dataset": "codereview_new", "original_id": 1316, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\nval random = Random(attacker.getCivInfo().gameInfo.turns * defender.getTile().position.hashCode())\nreturn random.nextFloat() <= captureChance\n", "gpt_em_trim": 0, "gpt_bleu_trim": 32.13787, "model_code": " object Battle { \n .toFloat() * 0.4f \n ) \n } \n fun unitGainFromEncampment(): Boolean {", "model_em_trim": 0, "model_bleu_trim": 5.79534670041046, "error_type_gpt": "Insignificant Omission"}
{"id": 144, "old": "\n #' @description\n #' `r lifecycle::badge(\"superseded\")`\n #'\n-#' If a variable contains observations with multiple delimited values, this\n-#' separates the values and places each one in its own row.\n-#'\n #' `separate_rows()` has been superseded in favour of [separate_longer_delim()]\n #' because it has a more consistent API with other separate functions.\n #' Superseded functions will not go away, but will only receive critical bug\n #' fixes.\n #'\n #' @inheritParams drop_na\n #' @inheritParams gather\n #' @param sep Separator delimiting collapsed values.", "new": "\n #' @description\n #' `r lifecycle::badge(\"superseded\")`\n #'\n #' `separate_rows()` has been superseded in favour of [separate_longer_delim()]\n #' because it has a more consistent API with other separate functions.\n #' Superseded functions will not go away, but will only receive critical bug\n #' fixes.\n #'\n+#' If a variable contains observations with multiple delimited values,\n+#' `separate_rows()` separates the values and places each one in its own row.\n+#'\n #' @inheritParams drop_na\n #' @inheritParams gather\n #' @param sep Separator delimiting collapsed values.", "review": "This is a general comment about this sort of statement, I think it makes more sense above the description, as the first thing in the documentation for the function. I often find myself regretting having spent mental energy deciphering the description of the function just to find out it's been superseded/deprecated.", "dataset": "codereview_new", "original_id": 2251, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Documentation-add", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n#' `separate_rows()` has been superseded in favour of [separate_longer_delim()]\n#' because it has a more consistent API with other separate functions.\n#' Superseded functions will not go away, but will only receive critical bug\n#' fixes.\n#'\n#' @description\n#' If a variable contains observations with multiple delimited values, this\n#' separates the values and places each one in its own row.\n#'\n#' @inheritParams drop_na\n#' @inheritParams gather\n#' @param sep Separator delimiting collapsed values.\n", "gpt_em_trim": 0, "gpt_bleu_trim": 79.45215, "model_code": " #' @description \n #' `r lifecycle::badge(\"superseded\")` \n #' @inheritParams drop_na \n #' @inheritParams gather \n #' @param sep Separator delimiting collapsed values.", "model_em_trim": 0, "model_bleu_trim": 9.554762388174606, "error_type_gpt": "Need Domain Knowledge"}
{"id": 145, "old": " public static void execute(String[] args) {\n                     System.out.println(prop);\n                 }\n             }\n-        } else if (key.equals(\"-args\")) {\n             // get all properties\n             StringBuffer prop = new StringBuffer(\"\");\n             for (Map.Entry<Object, Object> entry : config.entrySet()) {", "new": " public static void execute(String[] args) {\n                     System.out.println(prop);\n                 }\n             }\n+        } else if (\"-args\".equals(key)) {\n             // get all properties\n             StringBuffer prop = new StringBuffer(\"\");\n             for (Map.Entry<Object, Object> entry : config.entrySet()) {", "review": "\u5b57\u7b26\u4e32\u5e94\u8be5\u5199\u5728equals\u524d\u9762", "dataset": "codereview_new", "original_id": 3663, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\npublic static void execute(String[] args) {\n    if (\"-args\".equals(key)) {\n        // get all properties\n        StringBuffer prop = new StringBuffer(\"\");\n        for (Map.Entry<Object, Object> entry : config.entrySet()) {\n            // code logic here\n        }\n        System.out.println(prop);\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 88.10517, "model_code": " public static void execute(String[] args) { \n System.out.println(prop); \n } \n } else if (key.equals(\"-args\")) { \n // get all properties \n StringBuffer prop = new StringBuffer(\"\"); \n for (Map.Entry<Object, Object> entry : config.entrySet()) {", "model_em_trim": 0, "model_bleu_trim": 91.95779083685044, "error_type_gpt": "Need Domain Knowledge"}
{"id": 146, "old": " class BookmarksActivity : DuckDuckGoActivity() {\n         TextAlertDialogBuilder(this)\n             .setTitle(R.string.delete)\n             .setMessage(getMessageString(bookmarkFolder))\n-            .setPositiveButton(android.R.string.yes)\n-            .setNegativeButton(android.R.string.no)\n             .addEventListener(\n                 object : EventListener() {\n                     override fun onPositiveButtonClicked() {", "new": " class BookmarksActivity : DuckDuckGoActivity() {\n         TextAlertDialogBuilder(this)\n             .setTitle(R.string.delete)\n             .setMessage(getMessageString(bookmarkFolder))\n+            .setPositiveButton(R.string.yes)\n+            .setNegativeButton(R.string.no)\n             .addEventListener(\n                 object : EventListener() {\n                     override fun onPositiveButtonClicked() {", "review": "```suggestion\r\n            .setNegativeButton(R.string.no)\r\n```", "dataset": "codereview_new", "original_id": 2024, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass BookmarksActivity : DuckDuckGoActivity() {\n        TextAlertDialogBuilder(this)\n            .setTitle(R.string.delete)\n            .setMessage(getMessageString(bookmarkFolder))\n            .setPositiveButton(android.R.string.yes)\n            .setNegativeButton(R.string.no)\n            .addEventListener(\n                object : EventListener() {\n                    override fun onPositiveButtonClicked() {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 94.33517, "model_code": " class BookmarksActivity : DuckDuckGoActivity() { \n TextAlertDialogBuilder(this) \n .setTitle(R.string.delete) \n .setMessage(getMessageString(bookmarkFolder)) \n .setPositiveButton(android.R.string.yes) \n .setNegativeButton(R.string.no) \n .addEventListener( \n object : EventListener() { \n override fun onPositiveButtonClicked() {", "model_em_trim": 0, "model_bleu_trim": 94.33516657155005}
{"id": 147, "old": " func TestImportImportPost(t *testing.T) {\n \t\tposts, nErr := th.App.Srv().Store.Post().GetPostsCreatedAt(channel.Id, now)\n \t\trequire.NoError(t, nErr)\n \t\trequire.Len(t, posts, 2, \"Unexpected number of posts found.\")\n-\t\tth.TestLogger.Flush()\n \t\ttestlib.AssertLog(t, th.LogBuffer, mlog.LvlWarn.Name, \"Reply CreateAt is before parent post CreateAt, setting it to parent post CreateAt\")\n \n \t\trootPost := posts[0]", "new": " func TestImportImportPost(t *testing.T) {\n \t\tposts, nErr := th.App.Srv().Store.Post().GetPostsCreatedAt(channel.Id, now)\n \t\trequire.NoError(t, nErr)\n \t\trequire.Len(t, posts, 2, \"Unexpected number of posts found.\")\n+\t\trequire.NoError(t, th.TestLogger.Flush())\n \t\ttestlib.AssertLog(t, th.LogBuffer, mlog.LvlWarn.Name, \"Reply CreateAt is before parent post CreateAt, setting it to parent post CreateAt\")\n \n \t\trootPost := posts[0]", "review": "I couldn't reproduce the failure on my local setup. \r\n\r\nFrom the traceback in the jira ticket (https://mattermost.atlassian.net/browse/MM-44922) , it looks like the log line is present, but the check happens before the log line is printed. My guess is that this is happening because of buffering, so flushing the logs before the assertion.\r\n", "dataset": "codereview_new", "original_id": 8974, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfunc TestImportImportPost(t *testing.T) {\n\t\tposts, nErr := th.App.Srv().Store.Post().GetPostsCreatedAt(channel.Id, now)\n\t\trequire.NoError(t, nErr)\n\t\trequire.Len(t, posts, 2, \"Unexpected number of posts found.\")\n\t\tth.TestLogger.Flush()\n\n\t\t// Flush the logs before the assertion\n\t\ttestlib.AssertLog(t, th.LogBuffer, mlog.LvlWarn.Name, \"Reply CreateAt is before parent post CreateAt, setting it to parent post CreateAt\")\n\n\t\trootPost := posts[0]\n", "gpt_em_trim": 0, "gpt_bleu_trim": 90.66354, "model_code": " func TestImportImportPost(t *testing.T) { \n posts, nErr := th.App.Srv().Store.Post().GetPostsCreatedAt(channel.Id, now) \n require.NoError(t, nErr) \n require.Len(t, posts, 2, \"Unexpected number of posts found.\") \n th.TestLogger.Flush() \n rootPost := posts[0]", "model_em_trim": 0, "model_bleu_trim": 58.4789448806044, "error_type_gpt": "Need Domain Knowledge"}
{"id": 148, "old": " func (d *BackendServiceDescriber) Describe() (HumanJSONStringer, error) {\n \t\tif err != nil {\n \t\t\treturn nil, fmt.Errorf(\"retrieve platform: %w\", err)\n \t\t}\n-\t\tdeploymentType, err := svcDescr.DeploymentType()\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"retrieve rollback alarm names: %w\", err)\n-\t\t}\n \t\tconfigs = append(configs, &ECSServiceConfig{\n \t\t\tServiceConfig: &ServiceConfig{\n \t\t\t\tEnvironment: env,\n-\t\t\t\tDeployment:  deploymentType,\n \t\t\t\tPort:        port,\n \t\t\t\tCPU:         svcParams[cfnstack.WorkloadTaskCPUParamKey],\n \t\t\t\tMemory:      svcParams[cfnstack.WorkloadTaskMemoryParamKey],", "new": " func (d *BackendServiceDescriber) Describe() (HumanJSONStringer, error) {\n \t\tif err != nil {\n \t\t\treturn nil, fmt.Errorf(\"retrieve platform: %w\", err)\n \t\t}\n \t\tconfigs = append(configs, &ECSServiceConfig{\n \t\t\tServiceConfig: &ServiceConfig{\n \t\t\t\tEnvironment: env,\n \t\t\t\tPort:        port,\n \t\t\t\tCPU:         svcParams[cfnstack.WorkloadTaskCPUParamKey],\n \t\t\t\tMemory:      svcParams[cfnstack.WorkloadTaskMemoryParamKey],", "review": "Should it be \"Get deployment type\" instead?", "dataset": "codereview_new", "original_id": 4916, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nfunc (d *BackendServiceDescriber) Describe() (HumanJSONStringer, error) {\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"retrieve platform: %w\", err)\n\t\t}\n\t\tdeploymentType, err := svcDescr.GetDeploymentType()\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"retrieve rollback alarm names: %w\", err)\n\t\t}\n\t\tconfigs = append(configs, &ECSServiceConfig{\n\t\t\tServiceConfig: &ServiceConfig{\n\t\t\t\tEnvironment: env,\n\t\t\t\tDeployment:  deploymentType,\n\t\t\t\tPort:        port,\n\t\t\t\tCPU:         svcParams[cfnstack.WorkloadTaskCPUParamKey],\n\t\t\t\tMemory:      svcParams[cfnstack.WorkloadTaskMemoryParamKey],\n", "gpt_em_trim": 0, "gpt_bleu_trim": 65.52561, "model_code": " func (d *BackendServiceDescriber) Describe() (HumanJSONStringer, error) { \n if err != nil { \n return nil, fmt.Errorf(\"Get platform: %w\", err) \n } \n deploymentType, err := svcDescr.DeploymentType() \n if err != nil { \n return nil, fmt.Errorf(\"Get deployment type: %w\", err) \n } \n configs = append(configs, &ECSServiceConfig{ \n ServiceConfig: &ServiceConfig{ \n Environment: env, \n Deployment:  deploymentType, \n Port:        port, \n CPU:         svcParams[cfnstack.WorkloadTaskCPUParamKey], \n Memory:      svcParams[cfnstack.WorkloadTaskMemoryParamKey],", "model_em_trim": 0, "model_bleu_trim": 63.92443271922894}
{"id": 149, "old": "\n         key=\"gas_consumed_interval\",\n         name=\"Gas consumed interval\",\n         icon=\"mdi:meter-gas\",\n-        native_unit_of_measurement=\"m\u00b3/h\",\n         state_class=SensorStateClass.TOTAL,\n     ),\n     SensorEntityDescription(", "new": "\n         key=\"gas_consumed_interval\",\n         name=\"Gas consumed interval\",\n         icon=\"mdi:meter-gas\",\n+        native_unit_of_measurement=f\"UnitOfVolume.CUBIC_METERS/UnitOfTime.HOURS\",\n         state_class=SensorStateClass.TOTAL,\n     ),\n     SensorEntityDescription(", "review": "As the sensor becomes a rate, it can't be a total.", "dataset": "codereview_new", "original_id": 9687, "relevance": "not", "relevance_user1": "partial", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n        key=\"gas_consumed_interval\",\n        name=\"Gas consumed interval\",\n        icon=\"mdi:meter-gas\",\n        native_unit_of_measurement=\"m\u00b3/h\",\n        state_class=SensorStateClass.MEASUREMENT,\n    ),\n    SensorEntityDescription(\n", "gpt_em_trim": 0, "gpt_bleu_trim": 75.41116, "model_code": " key=\"gas_consumed_interval\", \n name=\"Gas consumed interval\", \n icon=\"mdi:meter-gas\", \n native_unit_of_measurement=\"m\u00b3/h\", \n state_class=SensorStateClass.RATE, \n ), \n SensorEntityDescription(", "model_em_trim": 0, "model_bleu_trim": 75.4111589813896}
{"id": 150, "old": " object Lowering {\n         val parExp = mkParApply(Expression.Apply(e, es, t, pur, eff, loc1))\n         Expression.Cast(parExp, None, Some(Type.Pure), Some(Type.Empty), t, pur, eff, loc0)\n \n-      case Expression.Par(_, _) =>\n-        throw InternalCompilerException(s\"Unexpected par expression near ${loc0.format}: $exp\")\n     }\n \n     case Expression.Lazy(exp, tpe, loc) =>", "new": " object Lowering {\n         val parExp = mkParApply(Expression.Apply(e, es, t, pur, eff, loc1))\n         Expression.Cast(parExp, None, Some(Type.Pure), Some(Type.Empty), t, pur, eff, loc0)\n \n+      case _ =>\n+        throw InternalCompilerException(s\"Unexpected par expression near ${exp.loc.format}: $exp\")\n     }\n \n     case Expression.Lazy(exp, tpe, loc) =>", "review": "OK, I see now that this has to be a wildcard. Thats OK.", "dataset": "codereview_new", "original_id": 3761, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nobject Lowering {\n        val parExp = mkParApply(Expression.Apply(e, es, t, pur, eff, loc1))\n        Expression.Cast(parExp, None, Some(Type.Pure), Some(Type.Empty), t, pur, eff, loc0)\n\n      case Expression.Par(_, _) =>\n        throw InternalCompilerException(s\"Unexpected par expression near ${loc0.format}: $exp\")\n    }\n\n    case Expression.Lazy(_, _, loc) =>\n", "gpt_em_trim": 0, "gpt_bleu_trim": 83.97378, "model_code": " object Lowering { \n val parExp = mkParApply(Expression.Apply(e, es, t, pur, eff, loc1)) \n Expression.Cast(parExp, None, Some(Type.Pure), Some(Type.Empty), t, pur, eff, loc0) \n } \n case Expression.Lazy(exp, tpe, loc) =>", "model_em_trim": 0, "model_bleu_trim": 68.20115537848358, "error_type_gpt": "Need Domain Knowledge"}
{"id": 151, "old": "\n //\n //===----------------------------------------------------------------------===//\n \n-#if os(Linux)\n import Glibc\n-#else\n import Darwin\n #endif\n \n extension CommandParser {", "new": "\n //\n //===----------------------------------------------------------------------===//\n \n+#if canImport(Glibc)\n import Glibc\n+#elseif canImport(Darwin)\n import Darwin\n+#elseif canImport(CRT)\n+import CRT\n+#elseif canImport(WASILibc)\n+import WASILibc\n #endif\n \n extension CommandParser {", "review": "Let's use `canImport` for these instead of `os` checks \u2014 you can see the other files that import `Glibc`.", "dataset": "codereview_new", "original_id": 783, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n//\n//===----------------------------------------------------------------------===//\n\n#if canImport(Glibc)\nimport Glibc\n#elseif canImport(Darwin)\nimport Darwin\n#endif\n\nextension CommandParser {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 85.10287, "model_code": " // \n //===----------------------------------------------------------------------===// \n extension CommandParser {", "model_em_trim": 0, "model_bleu_trim": 66.13081681007156, "error_type_gpt": "Need Domain Knowledge"}
{"id": 152, "old": " public class TaskUpdateThemeImport implements FallibleCommand {\n \n     TaskUpdateThemeImport(File npmFolder, ThemeDefinition theme,\n             File frontendDirectory) {\n-        this(npmFolder, theme, frontendDirectory,\n-                new File(frontendDirectory, GENERATED));\n-    }\n-\n-    TaskUpdateThemeImport(File npmFolder, ThemeDefinition theme,\n-            File frontendDirectory, File frontendGeneratedFolder) {\n         this.theme = theme;\n         this.frontendDirectory = frontendDirectory;\n         this.npmFolder = npmFolder;\n         themeImportFile = new File(frontendGeneratedFolder, THEME_IMPORTS_NAME);\n         themeImportFileDefinition = new File(frontendGeneratedFolder,\n                 THEME_IMPORTS_D_TS_NAME);", "new": " public class TaskUpdateThemeImport implements FallibleCommand {\n \n     TaskUpdateThemeImport(File npmFolder, ThemeDefinition theme,\n             File frontendDirectory) {\n         this.theme = theme;\n         this.frontendDirectory = frontendDirectory;\n         this.npmFolder = npmFolder;\n+        File frontendGeneratedFolder = new File(frontendDirectory, GENERATED);\n         themeImportFile = new File(frontendGeneratedFolder, THEME_IMPORTS_NAME);\n         themeImportFileDefinition = new File(frontendGeneratedFolder,\n                 THEME_IMPORTS_D_TS_NAME);", "review": "Should we deprecate this constructor that accepts a custom `frontendGeneratedFolder`?", "dataset": "codereview_new", "original_id": 9871, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\npublic class TaskUpdateThemeImport implements FallibleCommand {\n\n    TaskUpdateThemeImport(File npmFolder, ThemeDefinition theme,\n            File frontendDirectory) {\n        this(npmFolder, theme, frontendDirectory,\n                new File(frontendDirectory, GENERATED));\n    }\n\n    /**\n     * @deprecated Use {@link #TaskUpdateThemeImport(File, ThemeDefinition, File)} instead.\n     */\n    @Deprecated\n    TaskUpdateThemeImport(File npmFolder, ThemeDefinition theme,\n            File frontendDirectory, File frontendGeneratedFolder) {\n        this.theme = theme;\n        this.frontendDirectory = frontendDirectory;\n        this.npmFolder = npmFolder;\n        themeImportFile = new File(frontendGeneratedFolder, THEME_IMPORTS_NAME);\n        themeImportFileDefinition = new File(frontendGeneratedFolder,\n                THEME_IMPORTS_D_TS_NAME);\n    }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 57.48047, "model_code": " public class TaskUpdateThemeImport implements FallibleCommand { \n TaskUpdateThemeImport(File npmFolder, ThemeDefinition theme, \n File frontendDirectory, File frontendGeneratedFolder) { \n this.theme = theme; \n this.frontendDirectory = frontendDirectory; \n this.npmFolder = npmFolder; \n themeImportFile = new File(frontendGeneratedFolder, THEME_IMPORTS_NAME); \n themeImportFileDefinition = new File(frontendGeneratedFolder, \n THEME_IMPORTS_D_TS_NAME);", "model_em_trim": 0, "model_bleu_trim": 84.24292015960532, "error_type_gpt": "Need Domain Knowledge"}
{"id": 153, "old": " async def test_connection_error(hass: HomeAssistant, client: MagicMock) -> None:\n async def test_auth_error(hass: HomeAssistant, client: MagicMock) -> None:\n     \"\"\"Test that an error message is shown on login fail.\"\"\"\n     client.login.side_effect = AIOSomecomfort.AuthError\n-    with patch(\"AIOSomecomfort.AIOSomeComfort\", return_value=client,), patch(\n         \"homeassistant.components.honeywell.async_setup_entry\",\n         return_value=True,\n     ):", "new": " async def test_connection_error(hass: HomeAssistant, client: MagicMock) -> None:\n async def test_auth_error(hass: HomeAssistant, client: MagicMock) -> None:\n     \"\"\"Test that an error message is shown on login fail.\"\"\"\n     client.login.side_effect = AIOSomecomfort.AuthError\n+    with patch(\n         \"homeassistant.components.honeywell.async_setup_entry\",\n         return_value=True,\n     ):", "review": "We normally only need to patch the integration setup function on create entry results or other results that may lead to setting up the config entry again. It's not a bug to do it anyway, but it won't have any effect so we can remove it.", "dataset": "codereview_new", "original_id": 9764, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nasync def test_connection_error(hass: HomeAssistant, client: MagicMock) -> None:\nasync def test_auth_error(hass: HomeAssistant, client: MagicMock) -> None:\n    \"\"\"Test that an error message is shown on login fail.\"\"\"\n    client.login.side_effect = AIOSomecomfort.AuthError\n    with patch(\"AIOSomecomfort.AIOSomeComfort\", return_value=client):\n", "gpt_em_trim": 0, "gpt_bleu_trim": 82.8694, "model_code": " async def test_connection_error(hass: HomeAssistant, client: MagicMock) -> None: \n \"\"\"Test that an error message is shown on login fail.\"\"\" \n client.login.side_effect = AIOSomecomfort.AuthError \n with patch(\"homeassistant.components.honeywell.async_setup_entry\", \n return_value=True, \n ):", "model_em_trim": 0, "model_bleu_trim": 76.3173203433005, "error_type_gpt": "Need Domain Knowledge"}
{"id": 154, "old": "\n #' Progress bars in purrr\n #'\n #' @description\n-#' purrr's map functions come have a `.progress` argument, that you can use to\n #' create a progress bar. `.progress` can be:\n #'\n-#' * `FALSE`, the default: purrr does not create a progress bar.\n-#' * `TRUE`: purrr creates a basic unnamed progress bar.\n #' * A string: creates a basic progress bar with the given name.\n #' * A named list of progress bar parameters, as described below.\n #'", "new": "\n #' Progress bars in purrr\n #'\n #' @description\n+#' purrr's map functions have a `.progress` argument that you can use to\n #' create a progress bar. `.progress` can be:\n #'\n+#' * `FALSE`, the default: does not create a progress bar.\n+#' * `TRUE`: creates a basic unnamed progress bar.\n #' * A string: creates a basic progress bar with the given name.\n #' * A named list of progress bar parameters, as described below.\n #'", "review": "```suggestion\r\n#' purrr's map functions have a `.progress` argument that you can use to\r\n#' create a progress bar. `.progress` can be:\r\n```", "dataset": "codereview_new", "original_id": 2287, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n#' purrr's map functions have a `.progress` argument that you can use to\n#' create a progress bar. `.progress` can be:\n", "gpt_em_trim": 0, "gpt_bleu_trim": 10.08503, "model_code": " #' Progress bars in purrr \n #' \n #' * `FALSE`, the default: purrr does not create a progress bar. \n #' * `TRUE`: purrr creates a basic unnamed progress bar. \n #' * A string: creates a basic progress bar with the given name. \n #' * A named list of progress bar parameters, as described below. \n #'", "model_em_trim": 0, "model_bleu_trim": 57.03445235363134}
{"id": 155, "old": " def get_distconv_environment(parallel_io=False, num_io_partitions=1):\n \n     return {\n         'DISTCONV_WS_CAPACITY_FACTOR': 0.8,\n-#        'DISTCONV_OVERLAP_HALO_EXCHANGE': 1,\n         'LBANN_DISTCONV_HALO_EXCHANGE': 'AL',\n         'LBANN_DISTCONV_TENSOR_SHUFFLER': 'AL',\n-#        'LBANN_DISTCONV_HALO_EXCHANGE': 'HYBRID',\n-#        'LBANN_DISTCONV_TENSOR_SHUFFLER': 'HYBRID',\n         'LBANN_DISTCONV_CONVOLUTION_FWD_ALGORITHM': 'AUTOTUNE',\n         'LBANN_DISTCONV_CONVOLUTION_BWD_DATA_ALGORITHM': 'AUTOTUNE',\n         'LBANN_DISTCONV_CONVOLUTION_BWD_FILTER_ALGORITHM': 'AUTOTUNE',", "new": " def get_distconv_environment(parallel_io=False, num_io_partitions=1):\n \n     return {\n         'DISTCONV_WS_CAPACITY_FACTOR': 0.8,\n         'LBANN_DISTCONV_HALO_EXCHANGE': 'AL',\n         'LBANN_DISTCONV_TENSOR_SHUFFLER': 'AL',\n         'LBANN_DISTCONV_CONVOLUTION_FWD_ALGORITHM': 'AUTOTUNE',\n         'LBANN_DISTCONV_CONVOLUTION_BWD_DATA_ALGORITHM': 'AUTOTUNE',\n         'LBANN_DISTCONV_CONVOLUTION_BWD_FILTER_ALGORITHM': 'AUTOTUNE',", "review": "```suggestion\r\n        'LBANN_DISTCONV_HALO_EXCHANGE': 'AL',\r\n        'LBANN_DISTCONV_TENSOR_SHUFFLER': 'AL',\r\n```", "dataset": "codereview_new", "original_id": 5147, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Documentation-remove", "gpt_code": "\ndef get_distconv_environment(parallel_io=False, num_io_partitions=1):\n\n    return {\n        'DISTCONV_WS_CAPACITY_FACTOR': 0.8,\n        'LBANN_DISTCONV_HALO_EXCHANGE': 'AL',\n        'LBANN_DISTCONV_TENSOR_SHUFFLER': 'AL',\n        'LBANN_DISTCONV_CONVOLUTION_FWD_ALGORITHM': 'AUTOTUNE',\n        'LBANN_DISTCONV_CONVOLUTION_BWD_DATA_ALGORITHM': 'AUTOTUNE',\n        'LBANN_DISTCONV_CONVOLUTION_BWD_FILTER_ALGORITHM': 'AUTOTUNE',\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " def get_distconv_environment(parallel_io=False, num_io_partitions=1): \n return { \n 'DISTCONV_WS_CAPACITY_FACTOR': 0.8, \n 'LBANN_DISTCONV_HALO_EXCHANGE': 'AL', \n 'LBANN_DISTCONV_TENSOR_SHUFFLER': 'AL', \n 'LBANN_DISTCONV_CONVOLUTION_FWD_ALGORITHM': 'AUTOTUNE', \n 'LBANN_DISTCONV_CONVOLUTION_FWD_ALGORITHM': 'HYBRID',", "model_em_trim": 0, "model_bleu_trim": 77.68686260507798}
{"id": 156, "old": " func (tcn *TestConsensusNode) initNode(\n \t}\n \tkeysHandler, _ := keysManagement.NewKeysHandler(argsKeysHandler)\n \n-\tsignatureHolderArgs := cryptoFactory.ArgsSigningHandler{\n \t\tPubKeys:              []string{pubKeyString},\n \t\tMultiSignerContainer: multiSigContainer,\n \t\tKeyGenerator:         keyGen,\n \t\tKeysHandler:          keysHandler,\n \t\tSingleSigner:         TestSingleBlsSigner,\n \t}\n-\tsigHandler, _ := cryptoFactory.NewSigningHandler(signatureHolderArgs)\n \n \tnetworkComponents := GetDefaultNetworkComponents()\n \tnetworkComponents.Messenger = tcn.Messenger", "new": " func (tcn *TestConsensusNode) initNode(\n \t}\n \tkeysHandler, _ := keysManagement.NewKeysHandler(argsKeysHandler)\n \n+\tsigningHandlerArgs := cryptoFactory.ArgsSigningHandler{\n \t\tPubKeys:              []string{pubKeyString},\n \t\tMultiSignerContainer: multiSigContainer,\n \t\tKeyGenerator:         keyGen,\n \t\tKeysHandler:          keysHandler,\n \t\tSingleSigner:         TestSingleBlsSigner,\n \t}\n+\tsigHandler, _ := cryptoFactory.NewSigningHandler(signingHandlerArgs)\n \n \tnetworkComponents := GetDefaultNetworkComponents()\n \tnetworkComponents.Messenger = tcn.Messenger", "review": "rename here also", "dataset": "codereview_new", "original_id": 7761, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\nfunc (tcn *TestConsensusNode) initNode() {\n\tkeysHandler, _ := keysManagement.NewHandler(argsKeysHandler)\n\n\tsignatureHolderArgs := cryptoFactory.ArgsSigningHandler{\n\t\tPublicKeys:           []string{pubKeyString},\n\t\tMultiSignerContainer: multiSigContainer,\n\t\tKeyGenerator:         keyGen,\n\t\tKeysHandler:          keysHandler,\n\t\tSingleSigner:         TestSingleBlsSigner,\n\t}\n\tsigHandler, _ := cryptoFactory.NewSigningHandler(signatureHolderArgs)\n\n\tnetworkComponents := GetDefaultNetworkComponents()\n\tnetworkComponents.Messenger = tcn.Messenger\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 81.34159, "model_code": " func (tcn *TestConsensusNode) initNode( \n } \n keysHandler, _ := keysManagement.NewKeysHandler(argsKeysHandler) \n sigHandler, _ := cryptoFactory.NewSigningHandler(argsKeysHandler) \n networkComponents := GetDefaultNetworkComponents() \n networkComponents.Messenger = tcn.Messenger", "model_em_trim": 0, "model_bleu_trim": 43.464790784933065, "error_type_gpt": "Need Domain Knowledge"}
{"id": 157, "old": " def complex_numbers(\n     it is an error to enable ``allow_infinity``.\n \n     ``allow_subnormal`` is applied to each part of the complex number\n-    separately.\n \n     The magnitude constraints are respected up to a relative error\n     of (around) floating-point epsilon, due to implementation via", "new": " def complex_numbers(\n     it is an error to enable ``allow_infinity``.\n \n     ``allow_subnormal`` is applied to each part of the complex number\n+    separately, as for :func:`~hypothesis.strategies.floats`.\n \n     The magnitude constraints are respected up to a relative error\n     of (around) floating-point epsilon, due to implementation via", "review": "```suggestion\r\n    separately, as for :func:`~hypothesis.strategies.floats`.\r\n```", "dataset": "codereview_new", "original_id": 6977, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\ndef complex_numbers(\n    it is an error to enable ``allow_infinity``.\n\n    ``allow_subnormal`` is applied to each part of the complex number\n    separately, as for :func:`~hypothesis.strategies.floats`.\n\n    The magnitude constraints are respected up to a relative error\n    of (around) floating-point epsilon, due to implementation via\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " def complex_numbers( \n it is an error to enable ``allow_infinity``. \n The magnitude constraints are respected up to a relative error \n of (around) floating-point epsilon, due to implementation via", "model_em_trim": 0, "model_bleu_trim": 47.21231112402453}
{"id": 158, "old": " public enum Property {\n       \"Describes how the system will record the 'last' location for tablets, which can be used for assigning them when a cluster restarts.\"\n           + \" If 'compaction' is the mode, then the system will record the location where the tablet's most recent compaction occurred.\"\n           + \" If 'assignment' is the mode, then the most recently assigned location will be recorded.\"\n-          + \" Also note that manger.startup.tserver properties might need to be set as well to ensure\"\n           + \" the tserver is available before tablets are initially assigned if the 'last' location is to be used.\",\n       \"3.0.0\"),\n ", "new": " public enum Property {\n       \"Describes how the system will record the 'last' location for tablets, which can be used for assigning them when a cluster restarts.\"\n           + \" If 'compaction' is the mode, then the system will record the location where the tablet's most recent compaction occurred.\"\n           + \" If 'assignment' is the mode, then the most recently assigned location will be recorded.\"\n+          + \" The manager.startup.tserver properties might also need to be set to ensure\"\n           + \" the tserver is available before tablets are initially assigned if the 'last' location is to be used.\",\n       \"3.0.0\"),\n ", "review": "```suggestion\r\n          + \" The manager.startup.tserver properties might also need to be set to ensure\"\r\n```", "dataset": "codereview_new", "original_id": 4478, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\npublic enum Property {\n      \"Describes how the system will record the 'last' location for tablets, which can be used for assigning them when a cluster restarts.\"\n          + \" If 'compaction' is the mode, then the system will record the location where the tablet's most recent compaction occurred.\"\n          + \" If 'assignment' is the mode, then the most recently assigned location will be recorded.\"\n          + \" The manager.startup.tserver properties might also need to be set to ensure\"\n          + \" the tserver is available before tablets are initially assigned if the 'last' location is to be used.\",\n      \"3.0.0\"),\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " public enum Property { \n \"Describes how the system will record the 'last' location for tablets, which can be used for assigning them when a cluster restarts.\" \n + \" If 'compaction' is the mode, then the system will record the location where the tablet's most recent compaction occurred.\" \n + \" If 'assignment' is the mode, then the most recently assigned location will be recorded.\" \n + \" Also note that manger.startup.tserver properties might need to be set to ensure\" \n + \" the tserver is available before tablets are initially assigned if the 'last' location is to be used.\", \n \"3.0.0\"),", "model_em_trim": 0, "model_bleu_trim": 94.86399388073244}
{"id": 159, "old": " func InitBaseMetrics(statusHandlerUtils StatusHandlersUtils) error {\n \tappStatusHandler.SetUInt64Value(common.MetricNumConnectedPeers, initUint)\n \tappStatusHandler.SetUInt64Value(common.MetricEpochForEconomicsData, initUint)\n \n-\tappStatusHandler.SetInt64Value(common.MetricTrieSnapshotStartTime, initInt)\n \n \tappStatusHandler.SetStringValue(common.MetricConsensusState, initString)\n \tappStatusHandler.SetStringValue(common.MetricConsensusRoundState, initString)", "new": " func InitBaseMetrics(statusHandlerUtils StatusHandlersUtils) error {\n \tappStatusHandler.SetUInt64Value(common.MetricNumConnectedPeers, initUint)\n \tappStatusHandler.SetUInt64Value(common.MetricEpochForEconomicsData, initUint)\n \n+\tappStatusHandler.SetInt64Value(common.MetricTrieSnapshotLastDurationSec, initInt)\n \n \tappStatusHandler.SetStringValue(common.MetricConsensusState, initString)\n \tappStatusHandler.SetStringValue(common.MetricConsensusRoundState, initString)", "review": "I do not think this metric is useful. We know the snapshot starts at epoch change. I would rather log the snapshot duration.", "dataset": "codereview_new", "original_id": 7832, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfunc InitBaseMetrics(statusHandlerUtils StatusHandlersUtils) error {\n\tappStatusHandler.SetUInt64Value(common.MetricNumConnectedPeers, initUint)\n\tappStatusHandler.SetUInt64Value(common.MetricEpochForEconomicsData, initUint)\n\n\tappStatusHandler.SetInt64Value(common.MetricTrieSnapshotStartTime, initInt)\n\n\tappStatusHandler.SetStringValue(common.MetricConsensusState, initString)\n\tappStatusHandler.SetStringValue(common.MetricConsensusRoundState, initString)\n\n\t// Log the snapshot duration instead of the start time\n\tappStatusHandler.RegisterDuration(common.MetricTrieSnapshotDuration)\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 95.58402, "model_code": " func InitBaseMetrics(statusHandlerUtils StatusHandlersUtils) error { \n appStatusHandler.SetUInt64Value(common.MetricNumConnectedPeers, initUint) \n appStatusHandler.SetStringValue(common.MetricConsensusState, initString) \n appStatusHandler.SetStringValue(common.MetricConsensusRoundState, initString)", "model_em_trim": 0, "model_bleu_trim": 59.88042596485019, "error_type_gpt": "Need Domain Knowledge"}
{"id": 160, "old": " interface Driver {\n \n     fun setLocation(latitude: Double, longitude: Double)\n \n-    fun eraseAllText(charactersToErase: Int)\n }", "new": " interface Driver {\n \n     fun setLocation(latitude: Double, longitude: Double)\n \n+    fun eraseText(charactersToErase: Int)\n }", "review": "It isn't really \"all text\" if you give it a number of characters. Shall we just call it `eraseText`?", "dataset": "codereview_new", "original_id": 1782, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\ninterface Driver {\n\n    fun setLocation(latitude: Double, longitude: Double)\n\n    fun eraseText(charactersToErase: Int)\n}\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " interface Driver { \n fun setLocation(latitude: Double, longitude: Double) \n }", "model_em_trim": 0, "model_bleu_trim": 62.124007971970386}
{"id": 161, "old": " class FPToUIOpConversion : public OpConversionPattern<arith::FPToUIOp> {\n       ConversionPatternRewriter &rewriter) const override {\n     auto srcType = srcOp.getIn().getType();\n     auto dstType = srcOp.getResult().getType();\n-    dstType.dump();\n     auto resultType = getTypeConverter()->convertType(dstType);\n     if (srcType.isF32()) {\n       if (dstType.isSignlessInteger(32) || dstType.isUnsignedInteger(32)) {", "new": " class FPToUIOpConversion : public OpConversionPattern<arith::FPToUIOp> {\n       ConversionPatternRewriter &rewriter) const override {\n     auto srcType = srcOp.getIn().getType();\n     auto dstType = srcOp.getResult().getType();\n     auto resultType = getTypeConverter()->convertType(dstType);\n     if (srcType.isF32()) {\n       if (dstType.isSignlessInteger(32) || dstType.isUnsignedInteger(32)) {", "review": "Remove debug code before merge please", "dataset": "codereview_new", "original_id": 9452, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nclass FPToUIOpConversion : public OpConversionPattern<arith::FPToUIOp> {\n      ConversionPatternRewriter &rewriter) const override {\n    auto srcType = srcOp.getIn().getType();\n    auto dstType = srcOp.getResult().getType();\n    auto resultType = getTypeConverter()->convertType(dstType);\n    if (srcType.isF32()) {\n      if (dstType.isSignlessInteger(32) || dstType.isUnsignedInteger(32)) {\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " class FPToUIOpConversion : public OpConversionPattern<arith::FPToUIOp> { \n ConversionPatternRewriter &rewriter) const override { \n auto srcType = srcOp.getIn().getType(); \n auto dstType = srcOp.getResult().getType(); \n auto resultType = getTypeConverter()->convertType(dstType); \n if (srcType.isF32()) { \n if (dstType.isSignlessInteger(32) || dstType.isUnsignedInteger(32)) {", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 162, "old": " class StreamParJoinSuite extends Fs2Suite {\n         .compile\n         .drain\n         .value\n-        .flatMap { actual =>\n-          IO(assertEquals(actual, Left(TestException)))\n-        }\n     }\n \n     test(\"do not block while evaluating an EitherT.left outer stream\") {", "new": " class StreamParJoinSuite extends Fs2Suite {\n         .compile\n         .drain\n         .value\n+        .assertEquals(Left(TestException))\n     }\n \n     test(\"do not block while evaluating an EitherT.left outer stream\") {", "review": "I think you can just:\r\n```scala\r\n.drain.value.assertEquals(Left(TestException))\r\n```", "dataset": "codereview_new", "original_id": 3420, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass StreamParJoinSuite extends Fs2Suite {\n        .compile\n        .drain\n        .value\n        .assertEquals(Left(TestException))\n    }\n\n    test(\"do not block while evaluating an EitherT.left outer stream\") {\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " class StreamParJoinSuite extends Fs2Suite { \n .compile \n .drain \n .value \n .assertEquals(Left(TestException)) \n } \n test(\"do not block while evaluating an EitherT.left outer stream\") {", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 163, "old": " final class ProfileSelfPictureViewController: UIViewController {\n             bottomOffset = -UIScreen.safeArea.bottom + 20.0\n         }\n \n-        cameraButton.centerXAnchor.constraint(equalTo: bottomOverlayView.centerXAnchor, constant: 0).isActive = true\n         cameraButton.centerYAnchor.constraint(equalTo: bottomOverlayView.centerYAnchor, constant: bottomOffset).isActive = true\n \n         cameraButton.setIconColor(.white, for: .normal)", "new": " final class ProfileSelfPictureViewController: UIViewController {\n             bottomOffset = -UIScreen.safeArea.bottom + 20.0\n         }\n \n+        cameraButton.centerXAnchor.constraint(equalTo: bottomOverlayView.centerXAnchor).isActive = true\n         cameraButton.centerYAnchor.constraint(equalTo: bottomOverlayView.centerYAnchor, constant: bottomOffset).isActive = true\n \n         cameraButton.setIconColor(.white, for: .normal)", "review": "```suggestion\r\n        cameraButton.centerXAnchor.constraint(equalTo: bottomOverlayView.centerXAnchor).isActive = true\r\n```", "dataset": "codereview_new", "original_id": 654, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfinal class ProfileSelfPictureViewController: UIViewController {\n            bottomOffset = -UIScreen.safeArea.bottom + 20.0\n        }\n\n        cameraButton.centerXAnchor.constraint(equalTo: bottomOverlayView.centerXAnchor).isActive = true\n        cameraButton.centerYAnchor.constraint(equalTo: bottomOverlayView.centerYAnchor, constant: bottomOffset).isActive = true\n\n        cameraButton.setIconColor(.white, for: .normal)\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " final class ProfileSelfPictureViewController: UIViewController { \n bottomOffset = -UIScreen.safeArea.bottom + 20.0 \n } \n cameraButton.setIconColor(.white, for: .normal)", "model_em_trim": 0, "model_bleu_trim": 32.18168030554571}
{"id": 164, "old": "\n EmptyQueryValues = object()\n FallBackDateTimeStr = \"0001-01-01 00:00:00.000000\"\n \n-nested_set_hierarchy = (\n-\t\t\t\"ancestors of\",\n-\t\t\t\"descendants of\",\n-\t\t\t\"not ancestors of\",\n-\t\t\t\"not descendants of\",\n-\t\t)\n \n def is_query_type(query: str, query_type: str | tuple[str]) -> bool:\n \treturn query.lstrip().split(maxsplit=1)[0].lower().startswith(query_type)", "new": "\n EmptyQueryValues = object()\n FallBackDateTimeStr = \"0001-01-01 00:00:00.000000\"\n \n+NESTED_SET_HIERARCHY = (\n+\t\"ancestors of\",\n+\t\"descendants of\",\n+\t\"not ancestors of\",\n+\t\"not descendants of\",\n+)\n+\n \n def is_query_type(query: str, query_type: str | tuple[str]) -> bool:\n \treturn query.lstrip().split(maxsplit=1)[0].lower().startswith(query_type)", "review": "format this like other global variables? much consistent that way\r\n\r\nalso, maybe run pre-commit :P", "dataset": "codereview_new", "original_id": 8833, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\nEMPTY_QUERY_VALUES = object()\nFALLBACK_DATETIME_STR = \"0001-01-01 00:00:00.000000\"\n\nNESTED_SET_HIERARCHY = (\n    \"ancestors of\",\n    \"descendants of\",\n    \"not ancestors of\",\n    \"not descendants of\",\n)\n\ndef is_query_type(query: str, query_type: str | tuple[str]) -> bool:\n    return query.lstrip().split(maxsplit=1)[0].lower().startswith(query_type)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 89.64874, "model_code": " EmptyQueryValues = object() \n FallBackDateTimeStr = \"0001-01-01 00:00:00.000000\" \n def is_query_type(query: str, query_type: str | tuple[str]) -> bool: \n return query.lstrip().split(maxsplit=1)[0].lower().startswith(query_type)", "model_em_trim": 0, "model_bleu_trim": 65.63991546366302, "error_type_gpt": "Reasonable Improvement"}
{"id": 165, "old": " def call\n       end\n \n       def add_diff!(filepath, lines)\n-        @diffs << Diff.new(filepath, exercise.dir, lines.join) if filepath\n       end\n \n       Diff = Struct.new(:filepath, :dir, :diff) do", "new": " def call\n       end\n \n       def add_diff!(filepath, lines)\n+        @diffs << Diff.new(filepath, exercise.git_dir, lines.join) if filepath\n       end\n \n       Diff = Struct.new(:filepath, :dir, :diff) do", "review": "```suggestion\r\n        @diffs << Diff.new(filepath, exercise.git_dir, lines.join) if filepath\r\n```", "dataset": "codereview_new", "original_id": 6482, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ndef call\nend\n\ndef add_diff!(filepath, lines)\n  @diffs << Diff.new(filepath, exercise.git_dir, lines.join) if filepath\nend\n\nDiff = Struct.new(:filepath, :git_dir, :diff) do\n", "gpt_em_trim": 0, "gpt_bleu_trim": 93.36846, "model_code": " def call \n end \n Diff = Struct.new(:filepath, :dir, :diff) do", "model_em_trim": 0, "model_bleu_trim": 18.34927009352068, "error_type_gpt": "Model fallacy"}
{"id": 166, "old": "\n #' @section Case weights:\n #'\n #' This step performs an unsupervised operation that can utilize case weights.\n-#' As a result, case weights are only used with frequency weights. For more\n #' information, see the documentation in [case_weights] and the examples on\n #' `tidymodels.org`.\n #'\n-#' This step requires the case weights to be available when the recipe is being\n-#' baked. If you are using this step with a workflow, you need to pass A\n #' blueprint to the `blueprint` argument of `workflows::add_recipe()` letting\n #' the workflow know that case weights are required at bake time. The blueprint\n #' can be created with\n #' `hardhat::default_recipe_blueprint(bake_dependent_roles = \"case_weights\")`.\n #'\n #' @references Serneels, S., De Nolf, E., and Van Espen, P.\n #'  (2006). Spatial sign preprocessing: a simple way to impart", "new": "\n #' @section Case weights:\n #'\n #' This step performs an unsupervised operation that can utilize case weights.\n+#' As a result, only frequency weights are allowed. For more\n #' information, see the documentation in [case_weights] and the examples on\n #' `tidymodels.org`.\n #'\n+#' Unlike most, this step requires the case weights to be available when new\n+#' samples are processed (e.g., when `bake()` is used or `predict()` with a\n+#' workflow). If you are using this step with a workflow, you need to pass a\n #' blueprint to the `blueprint` argument of `workflows::add_recipe()` letting\n #' the workflow know that case weights are required at bake time. The blueprint\n #' can be created with\n #' `hardhat::default_recipe_blueprint(bake_dependent_roles = \"case_weights\")`.\n+#' See [hardhat::default_recipe_blueprint()] as well as [update_roles()].\n #'\n #' @references Serneels, S., De Nolf, E., and Van Espen, P.\n #'  (2006). Spatial sign preprocessing: a simple way to impart", "review": "The template isn't used because the extra text can't be added to the section created inside the template", "dataset": "codereview_new", "original_id": 2754, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n#' @section Case weights:\n#'\n#' This step performs an unsupervised operation that can utilize case weights.\n#' As a result, case weights are only used with frequency weights. For more\n#' information, see the documentation in [case_weights] and the examples on\n#' `tidymodels.org`.\n#'\n#' This step requires the case weights to be available when the recipe is being\n#' baked. If you are using this step with a workflow, you need to pass a\n#' blueprint to the `blueprint` argument of `workflows::add_recipe()`. The\n#' blueprint should indicate that case weights are required at bake time. You\n#' can create the blueprint using\n#' `hardhat::default_recipe_blueprint(bake_dependent_roles = \"case_weights\")`.\n#'\n#' For more information on creating blueprints, see the documentation in\n#' [blueprints].\n#'\n#' @references Serneels, S., De Nolf, E., and Van Espen, P.\n#'  (2006). Spatial sign preprocessing: a simple way to impart\n", "gpt_em_trim": 0, "gpt_bleu_trim": 69.84035, "model_code": " #' @section Case weights: \n #' \n #' This step requires the case weights to be available when the recipe is being \n #' baked. If you are using this step with a workflow, you need to pass A \n #' blueprint to the `blueprint` argument of `workflows::add_recipe()` letting \n #' the workflow know that case weights are required at bake time. The blueprint can be created with \n #' `hardhat::default_recipe_blueprint(bake_dependent_roles = \"case_weights\")`. \n #' \n #' @references Serneels, S., De Nolf, E., and", "model_em_trim": 0, "model_bleu_trim": 33.9844200207844}
{"id": 167, "old": " class MapUnit : IsPartOfGameInfoSerialization {\n     }\n \n \n-    private fun isAlly(otherCiv: Civilization?): Boolean {\n         return otherCiv == civInfo\n-                || (otherCiv?.isCityState() == true && otherCiv.getAllyCiv() == civInfo.civName)\n-                || (civInfo.isCityState() && otherCiv != null && civInfo.getAllyCiv() == otherCiv.civName)\n     }\n \n     /** Implements [UniqueParameterType.MapUnitFilter][com.unciv.models.ruleset.unique.UniqueParameterType.MapUnitFilter] */", "new": " class MapUnit : IsPartOfGameInfoSerialization {\n     }\n \n \n+    private fun isAlly(otherCiv: Civilization): Boolean {\n         return otherCiv == civInfo\n+                || (otherCiv.isCityState() && otherCiv.getAllyCiv() == civInfo.civName)\n+                || (civInfo.isCityState() && civInfo.getAllyCiv() == otherCiv.civName)\n     }\n \n     /** Implements [UniqueParameterType.MapUnitFilter][com.unciv.models.ruleset.unique.UniqueParameterType.MapUnitFilter] */", "review": "When would this get null as an input? Sounds to be like it's better off not nullable, OR checking that explicitly first", "dataset": "codereview_new", "original_id": 1313, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass MapUnit : IsPartOfGameInfoSerialization {\n    }\n\n    private fun isAlly(otherCiv: Civilization): Boolean {\n        return otherCiv == civInfo\n                || (otherCiv.isCityState() && otherCiv.getAllyCiv() == civInfo.civName)\n                || (civInfo.isCityState() && otherCiv != null && civInfo.getAllyCiv() == otherCiv.civName)\n    }\n\n    /** Implements [UniqueParameterType.MapUnitFilter][com.unciv.models.ruleset.unique.UniqueParameterType.MapUnitFilter] */\n", "gpt_em_trim": 0, "gpt_bleu_trim": 93.50125, "model_code": " class MapUnit : IsPartOfGameInfoSerialization { \n } \n /** Implements [UniqueParameterType.MapUnitFilter][com.unciv.models.ruleset.unique.UniqueParameterType.MapUnitFilter] */", "model_em_trim": 0, "model_bleu_trim": 16.311097828776838}
{"id": 168, "old": " final class CallViewController: UIViewController {\n             permissions.requestOrWarnAboutVideoPermission { isVideoPermissionGranted in\n                 self.disableVideoIfNeeded()\n                 self.updateVideoStatusPlaceholder()\n-                if !isVideoPermissionGranted { return }\n             }\n         }\n ", "new": " final class CallViewController: UIViewController {\n             permissions.requestOrWarnAboutVideoPermission { isVideoPermissionGranted in\n                 self.disableVideoIfNeeded()\n                 self.updateVideoStatusPlaceholder()\n+                guard isVideoPermissionGranted else { return }\n             }\n         }\n ", "review": "```suggestion\r\n                guard isVideoPermissionGranted else { return }\r\n```", "dataset": "codereview_new", "original_id": 759, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfinal class CallViewController: UIViewController {\n            permissions.requestOrWarnAboutVideoPermission { isVideoPermissionGranted in\n                self.disableVideoIfNeeded()\n                self.updateVideoStatusPlaceholder()\n                guard isVideoPermissionGranted else { return }\n            }\n        }\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " final class CallViewController: UIViewController { \n permissions.requestOrWarnAboutVideoPermission { isVideoPermissionGranted in \n self.disableVideoIfNeeded() \n self.updateVideoStatusPlaceholder() \n } \n } \n", "model_em_trim": 0, "model_bleu_trim": 74.28876974677485}
{"id": 169, "old": " func shouldIncludeAllTokens(tokensFilter string) bool {\n \treturn tokensFilter == \"*\" || tokensFilter == \"all\"\n }\n \n-func (bap *baseAPIBlockProcessor) apiBlockToTxsPool(apiBlock *api.Block) *outport.Pool {\n \tpool := &outport.Pool{\n \t\tTxs:     make(map[string]data.TransactionHandlerWithGasUsedAndFee),\n \t\tScrs:    make(map[string]data.TransactionHandlerWithGasUsedAndFee),", "new": " func shouldIncludeAllTokens(tokensFilter string) bool {\n \treturn tokensFilter == \"*\" || tokensFilter == \"all\"\n }\n \n+func (bap *baseAPIBlockProcessor) apiBlockToOutportPool(apiBlock *api.Block) *outport.Pool {\n \tpool := &outport.Pool{\n \t\tTxs:     make(map[string]data.TransactionHandlerWithGasUsedAndFee),\n \t\tScrs:    make(map[string]data.TransactionHandlerWithGasUsedAndFee),", "review": "Perhaps rename to `*toOutportPool`?", "dataset": "codereview_new", "original_id": 7841, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\nfunc shouldIncludeAllTokens(tokensFilter string) bool {\n\treturn tokensFilter == \"*\" || tokensFilter == \"all\"\n}\n\nfunc (bap *baseAPIBlockProcessor) apiBlockToOutportPool(apiBlock *api.Block) *outport.Pool {\n\tpool := &outport.Pool{\n\t\tTxs:     make(map[string]data.TransactionHandlerWithGasUsedAndFee),\n\t\tScrs:    make(map[string]data.TransactionHandlerWithGasUsedAndFee),\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " func shouldIncludeAllTokens(tokensFilter string) bool { \n return tokensFilter == \"*\" || tokensFilter == \"all\" \n } \n func (bap *baseAPIBlockProcessor) toOutportPool() *outport.Pool { \n pool := &outport.Pool{ \n Txs:     make(map[string]data.TransactionHandlerWithGasUsedAndFee), \n Scrs:    make(map[string]data.TransactionHandlerWithGasUsedAndFee),", "model_em_trim": 0, "model_bleu_trim": 89.078389569715}
{"id": 170, "old": " WITH items_statistical_codes AS (\n SELECT\n     items_statistical_codes.item_id,\n     items_statistical_codes.item_hrid,\n-    items_statistical_codes.statistical_code_id,\n     sc__t.code AS statistical_code,\n     sc__t.name AS statistical_code_name,\n-    sc__t.statistical_code_type_id,\n     sct__t.name AS statistical_code_type_name\n FROM\n     items_statistical_codes", "new": " WITH items_statistical_codes AS (\n SELECT\n     items_statistical_codes.item_id,\n     items_statistical_codes.item_hrid,\n+    items_statistical_codes.statistical_code_id::uuid,\n     sc__t.code AS statistical_code,\n     sc__t.name AS statistical_code_name,\n+    sc__t.statistical_code_type_id::uuid,\n     sct__t.name AS statistical_code_type_name\n FROM\n     items_statistical_codes", "review": "Should be of type uuid.", "dataset": "codereview_new", "original_id": 2195, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nWITH items_statistical_codes AS (\nSELECT\n    items_statistical_codes.item_id,\n    items_statistical_codes.item_hrid,\n    items_statistical_codes.statistical_code_id::uuid,\n    sc__t.code AS statistical_code,\n    sc__t.name AS statistical_code_name,\n    sc__t.statistical_code_type_id::uuid,\n    sct__t.name AS statistical_code_type_name\nFROM\n    items_statistical_codes\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " WITH items_statistical_codes AS ( \n SELECT \n items_statistical_codes.item_id, \n items_statistical_codes.item_uuid, \n items_statistical_codes.statistical_code_id, \n sc__t.code AS statistical_code, \n sc__t.name AS statistical_code_name, \n sc__t.statistical_code_type_uuid, \n sct__t.name AS statistical_code_type_name \n FROM \n items_statistical_codes", "model_em_trim": 0, "model_bleu_trim": 89.17189645391973}
{"id": 171, "old": " func NewGenKubeConfigCommand(f velacmd.Factory, streams util.IOStreams) *cobra.C\n \t}\n \tcmd.Flags().StringVarP(&o.User, \"user\", \"u\", o.User, \"The user of the generated kubeconfig. If set, an X509-based kubeconfig will be intended to create. It will be embedded as the Subject in the X509 certificate.\")\n \tcmd.Flags().StringSliceVarP(&o.Groups, \"group\", \"g\", o.Groups, \"The groups of the generated kubeconfig. This flag only works when `--user` is set. It will be embedded as the Organization in the X509 certificate.\")\n-\tcmd.Flags().StringVarP(&o.ServiceAccount, \"serviceaccount\", \"\", \"\", \"The serviceaccount of the generated kubeconfig. If set, a kubeconfig will be generated based on the secret token of the serviceaccount. Cannot be set when `--user` presents.\")\n \tcmdutil.CheckErr(cmd.RegisterFlagCompletionFunc(\n \t\t\"serviceaccount\", func(cmd *cobra.Command, args []string, toComplete string) ([]string, cobra.ShellCompDirective) {\n \t\t\tif strings.TrimSpace(o.User) != \"\" {", "new": " func NewGenKubeConfigCommand(f velacmd.Factory, streams util.IOStreams) *cobra.C\n \t}\n \tcmd.Flags().StringVarP(&o.User, \"user\", \"u\", o.User, \"The user of the generated kubeconfig. If set, an X509-based kubeconfig will be intended to create. It will be embedded as the Subject in the X509 certificate.\")\n \tcmd.Flags().StringSliceVarP(&o.Groups, \"group\", \"g\", o.Groups, \"The groups of the generated kubeconfig. This flag only works when `--user` is set. It will be embedded as the Organization in the X509 certificate.\")\n+\tcmd.Flags().StringVarP(&o.ServiceAccount, \"serviceaccount\", o.ServiceAccount, \"\", \"The serviceaccount of the generated kubeconfig. If set, a kubeconfig will be generated based on the secret token of the serviceaccount. Cannot be set when `--user` presents.\")\n \tcmdutil.CheckErr(cmd.RegisterFlagCompletionFunc(\n \t\t\"serviceaccount\", func(cmd *cobra.Command, args []string, toComplete string) ([]string, cobra.ShellCompDirective) {\n \t\t\tif strings.TrimSpace(o.User) != \"\" {", "review": "Any reason for change here?", "dataset": "codereview_new", "original_id": 7018, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nfunc NewGenKubeConfigCommand(f velacmd.Factory, streams util.IOStreams) *cobra.Command {\n\tcmd := &cobra.Command{\n\t\tUse:   \"gen-kubeconfig\",\n\t\tShort: \"Generate kubeconfig file for the Vela cluster\",\n\t\tLong:  \"Generate kubeconfig file for the Vela cluster.\",\n\t\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\t\treturn runGenKubeConfig(f, streams, cmd, args)\n\t\t},\n\t}\n\n\to := &options{}\n\n\tcmd.Flags().StringVarP(&o.User, \"user\", \"u\", o.User, \"The user of the generated kubeconfig. If set, an X509-based kubeconfig will be intended to create. It will be embedded as the Subject in the X509 certificate.\")\n\tcmd.Flags().StringSliceVarP(&o.Groups, \"group\", \"g\", o.Groups, \"The groups of the generated kubeconfig. This flag only works when `--user` is set. It will be embedded as the Organization in the X509 certificate.\")\n\tcmd.Flags().StringVarP(&o.ServiceAccount, \"serviceaccount\", \"\", \"\", \"The serviceaccount of the generated kubeconfig. If set, a kubeconfig will be generated based on the secret token of the serviceaccount. Cannot be set when `--user` presents.\")\n\n\tcmdutil.CheckErr(cmd.RegisterFlagCompletionFunc(\n\t\t\"serviceaccount\", func(cmd *cobra.Command, args []string, toComplete string) ([]string, cobra.ShellCompDirective) {\n\t\t\tif strings.TrimSpace(o.User) != \"\" {\n\t\t\t\treturn nil, cobra.ShellCompDirectiveNoFileComp\n\t\t\t}\n\t\t\treturn cmdutil.FilterValidServiceAccounts(f, toComplete)\n\t\t},\n\t))\n\n\treturn cmd\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 91.49462, "model_code": " func NewGenKubeConfigCommand(f velacmd.Factory, streams util.IOStreams) *cobra.C \n } \n cmd.Flags().StringVarP(&o.User, \"user\", \"\", \"\", \"The user of the generated kubeconfig. If set, an X509-based kubeconfig will be intended to create. It will be embedded as the Subject in the X509 certificate.\") \n cmd.Flags().StringSliceVarP(&o.Groups, \"group\", \"g\", o.Groups, \"The groups of the generated kubeconfig. This flag only works when `--user` is set. It will be embedded as the Organization in the X509 certificate.\") \n cmd.Flags().StringVarP(&o.ServiceAccount, \"serviceaccount\", \"\", \"\", \"The serviceaccount of the generated kubeconfig. If set, a kubeconfig will be generated based on the secret token of the serviceaccount. Cannot be", "model_em_trim": 0, "model_bleu_trim": 67.81267649634776, "error_type_gpt": "Need Domain Knowledge"}
{"id": 172, "old": " corresponding Unix manual entries for more information on calls.\");\n \n #if defined(HAVE_SYS_XATTR_H) && defined(__linux__) && !defined(__FreeBSD_kernel__) && !defined(__GNU__)\n #  define USE_XATTRS\n-#  include <linux/limits.h>\n #endif\n \n #ifdef USE_XATTRS", "new": " corresponding Unix manual entries for more information on calls.\");\n \n #if defined(HAVE_SYS_XATTR_H) && defined(__linux__) && !defined(__FreeBSD_kernel__) && !defined(__GNU__)\n #  define USE_XATTRS\n+#  include <linux/limits.h>  // Needed for XATTR_SIZE_MAX on musl libc.\n #endif\n \n #ifdef USE_XATTRS", "review": "is this specific direct linux kernel include required (it _should_ be harmless), i'd expect `sys/xattr.h` (below) or even `<limits.h>` to include this kernel header for us?  (as seems to be the case on glibc and android bionic libc?)", "dataset": "codereview_new", "original_id": 9337, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-add", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-add", "gpt_code": "\ncorresponding Unix manual entries for more information on calls.\");\n\n#if defined(HAVE_SYS_XATTR_H) && defined(__linux__) && !defined(__FreeBSD_kernel__) && !defined(__GNU__)\n#  define USE_XATTRS\n#  include <sys/xattr.h>\n#endif\n\n#ifdef USE_XATTRS\n", "gpt_em_trim": 0, "gpt_bleu_trim": 79.16633, "model_code": " corresponding Unix manual entries for more information on calls.\"); \n #ifdef USE_XATTRS", "model_em_trim": 0, "model_bleu_trim": 1.839075061609719, "error_type_gpt": "Need Domain Knowledge"}
{"id": 173, "old": " type downstreamRouter interface {\n \tStartRouteMgmtTicking(context.Context, func() string, int) error\n \n \t// ProcessPendingConnections starts a function that continually processes\n-\t// incoming client connections and processes them. This never returns until\n \t// the provided context is done.\n \tStartProcessingPendingConnections(context.Context, func() string)\n }", "new": " type downstreamRouter interface {\n \tStartRouteMgmtTicking(context.Context, func() string, int) error\n \n \t// ProcessPendingConnections starts a function that continually processes\n+\t// incoming client connections. This never returns until\n \t// the provided context is done.\n \tStartProcessingPendingConnections(context.Context, func() string)\n }", "review": "Nit: this reads a bit weird \ud83d\ude01.\r\n```suggestion\r\n\r\n\t// ProcessPendingConnections starts a function that continually processes\r\n\t// incoming client connections. This never returns until\r\n```", "dataset": "codereview_new", "original_id": 5308, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\ntype downstreamRouter interface {\n\tStartRouteMgmtTicking(context.Context, func() string, int) error\n\n\t// StartProcessingPendingConnections starts a function that continually processes\n\t// incoming client connections. This never returns until the provided context is done.\n\tStartProcessingPendingConnections(context.Context, func() string)\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 89.22334, "model_code": " type downstreamRouter interface { \n StartRouteMgmtTicking(context.Context, func() string, int) error \n }", "model_em_trim": 0, "model_bleu_trim": 14.327884760331012, "error_type_gpt": "Reasonable Improvement"}
{"id": 174, "old": " def cmd_start(docker: bool, host: bool, no_banner: bool, detached: bool):\n         try:\n             bootstrap.start_infra_locally()\n         except ImportError:\n             raise click.ClickException(\n                 \"It appears you have a light install of localstack which only supports running in docker\\n\"\n                 \"If you would like to use --host, please reinstall localstack using `pip install localstack[runtime]`\"", "new": " def cmd_start(docker: bool, host: bool, no_banner: bool, detached: bool):\n         try:\n             bootstrap.start_infra_locally()\n         except ImportError:\n+            if config.DEBUG:\n+                console.print_exception()\n             raise click.ClickException(\n                 \"It appears you have a light install of localstack which only supports running in docker\\n\"\n                 \"If you would like to use --host, please reinstall localstack using `pip install localstack[runtime]`\"", "review": "```suggestion\r\n        except ImportError:\r\n            if config.DEBUG:\r\n                console.print_exception()\r\n            raise click.ClickException(\r\n```", "dataset": "codereview_new", "original_id": 6068, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\ndef cmd_start(docker: bool, host: bool, no_banner: bool, detached: bool):\n        try:\n            bootstrap.start_infra_locally()\n        except ImportError:\n            if config.DEBUG:\n                console.print_exception()\n            raise click.ClickException(\n                \"It appears you have a light install of localstack which only supports running in docker\\n\"\n                \"If you would like to use --host, please reinstall localstack using `pip install localstack[runtime]`\"\n            )\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " def cmd_start(docker: bool, host: bool, no_banner: bool, detached: bool): \n try: \n bootstrap.start_infra_locally() \n except ImportError: \n if config.DEBUG: \n console.print_exception() \n raise click.ClickException( \n \"It appears you have a light install of localstack which only supports running in docker\\n\" \n \"If you would like to use --host, please reinstall localstack using `pip install localstack[runtime]`\"", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 175, "old": " def create\n         format.turbo_stream do\n           flash.now[:error] = flash_error\n           render turbo_stream: [\n-            turbo_stream.replace(@distribution, partial: \"form\", locals: { distribution: @distribution }),\n-            turbo_stream.replace(\"flash\", partial: \"shared/flash\"),\n-          ], status: 400\n         end\n       end\n     end", "new": " def create\n         format.turbo_stream do\n           flash.now[:error] = flash_error\n           render turbo_stream: [\n+            turbo_stream.replace(@distribution, partial: \"form\", locals: {distribution: @distribution}),\n+            turbo_stream.replace(\"flash\", partial: \"shared/flash\")\n+          ], status: :bad_request\n         end\n       end\n     end", "review": "This is needed to replace the distribution form's distribution record that contains the errors. This line of code makes it so the errors now appear beneath the input fields with issues.", "dataset": "codereview_new", "original_id": 4033, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ndef create\n  format.turbo_stream do\n    flash.now[:error] = flash_error\n    render turbo_stream: [\n      turbo_stream.replace(@distribution, partial: \"form\", locals: { distribution: @distribution }),\n      turbo_stream.replace(\"flash\", partial: \"shared/flash\"),\n      turbo_stream.replace(\"error_messages\", partial: \"shared/error_messages\", locals: { object: @distribution })\n    ], status: 400\n  end\nend\n", "gpt_em_trim": 0, "gpt_bleu_trim": 67.47147, "model_code": " def create \n format.turbo_stream do \n render turbo_stream: [turbo_stream.replace(@distribution, partial: \"form\", locals: { distribution: @distribution }), turbo_stream.replace(\"flash\", partial: \"shared/flash\"),], status: 400 \n end \n end \n end", "model_em_trim": 0, "model_bleu_trim": 73.97819651833693, "error_type_gpt": "Need Domain Knowledge"}
{"id": 176, "old": " class Valid[+T <: Data](gen: T) extends Bundle {\n   *   }\n   * }}}\n   *\n-  * To convert this to a `valid`` interface, you wrap it with a call to the `Valid`.apply `Valid` companion object's\n-  * apply method:\n   * {{{\n   *   val bar = Valid(new MyBundle)\n   * }}}", "new": " class Valid[+T <: Data](gen: T) extends Bundle {\n   *   }\n   * }}}\n   *\n+  * To convert this to a `valid` interface, you wrap it with a call to the `Valid` companion object's apply method:\n   * {{{\n   *   val bar = Valid(new MyBundle)\n   * }}}", "review": "```suggestion\r\n  * To convert this to a `valid` interface, you wrap it with a call to the `Valid` companion object's\r\n  * apply method:\r\n```", "dataset": "codereview_new", "original_id": 3133, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nclass Valid[+T <: Data](gen: T) extends Bundle {\n  *   }\n  * }}}\n  *\n  * To convert this to a `valid` interface, you can use the `Valid` companion object's apply method:\n  * {{{\n  *   val bar = Valid(new MyBundle)\n  * }}}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 89.1153, "model_code": " class Valid[+T <: Data](gen: T) extends Bundle { \n *   } \n * }}} \n * \n * To convert this to a `valid` interface, you wrap it with a call to the `Valid` companion object's \n * {{{ \n *   val bar = Valid(new MyBundle) \n * }}}", "model_em_trim": 0, "model_bleu_trim": 93.48368543173315, "error_type_gpt": "Model fallacy"}
{"id": 177, "old": " private fun createSidewalksDefault(tags: Map<String, String>): LeftAndRightSidew\n     \"left\" -> LeftAndRightSidewalk(left = YES, right = NO)\n     \"right\" -> LeftAndRightSidewalk(left = NO, right = YES)\n     \"both\" -> LeftAndRightSidewalk(left = YES, right = YES)\n-    \"no\" -> LeftAndRightSidewalk(left = NO, right = NO)\n-    \"none\" -> LeftAndRightSidewalk(left = NO, right = NO)\n     \"separate\" -> LeftAndRightSidewalk(left = SEPARATE, right = SEPARATE)\n     else -> null\n }", "new": " private fun createSidewalksDefault(tags: Map<String, String>): LeftAndRightSidew\n     \"left\" -> LeftAndRightSidewalk(left = YES, right = NO)\n     \"right\" -> LeftAndRightSidewalk(left = NO, right = YES)\n     \"both\" -> LeftAndRightSidewalk(left = YES, right = YES)\n+    \"no\", \"none\" -> LeftAndRightSidewalk(left = NO, right = NO)\n     \"separate\" -> LeftAndRightSidewalk(left = SEPARATE, right = SEPARATE)\n     else -> null\n }", "review": "```suggestion\r\n    \"no\", \"none\" -> LeftAndRightSidewalk(left = NO, right = NO)\r\n```\r\nshortcut", "dataset": "codereview_new", "original_id": 1933, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\nprivate fun createSidewalksDefault(tags: Map<String, String>): LeftAndRightSidew\n    \"left\" -> LeftAndRightSidewalk(left = YES, right = NO)\n    \"right\" -> LeftAndRightSidewalk(left = NO, right = YES)\n    \"both\" -> LeftAndRightSidewalk(left = YES, right = YES)\n    \"separate\" -> LeftAndRightSidewalk(left = SEPARATE, right = SEPARATE)\n    else -> LeftAndRightSidewalk(left = NO, right = NO)\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 88.19975, "model_code": " private fun createSidewalksDefault(tags: Map<String, String>): LeftAndRightSidew \n \"left\" -> LeftAndRightSidewalk(left = YES, right = NO) \n \"right\" -> LeftAndRightSidewalk(left = NO, right = YES) \n \"separate\" -> LeftAndRightSidewalk(left = SEPARATE, right = SEPARATE) \n else -> null \n }", "model_em_trim": 0, "model_bleu_trim": 59.502515293592786, "error_type_gpt": "Model fallacy"}
{"id": 178, "old": " class StylableButton: UIButton, Stylable {\n         setBackgroundImageColor(style.normalStateColors.background, for: .normal)\n         setBackgroundImageColor(style.highlightedStateColors.background, for: .highlighted)\n         setBackgroundImageColor(style.selectedStateColors?.background, for: .selected)\n- \n         self.layer.borderWidth = 1\n         self.layer.borderColor = isHighlighted ? style.highlightedStateColors.border.cgColor : style.normalStateColors.border.cgColor\n         self.layer.borderColor = isSelected ? style.selectedStateColors?.border.cgColor : style.normalStateColors.border.cgColor", "new": " class StylableButton: UIButton, Stylable {\n         setBackgroundImageColor(style.normalStateColors.background, for: .normal)\n         setBackgroundImageColor(style.highlightedStateColors.background, for: .highlighted)\n         setBackgroundImageColor(style.selectedStateColors?.background, for: .selected)\n+\n         self.layer.borderWidth = 1\n         self.layer.borderColor = isHighlighted ? style.highlightedStateColors.border.cgColor : style.normalStateColors.border.cgColor\n         self.layer.borderColor = isSelected ? style.selectedStateColors?.border.cgColor : style.normalStateColors.border.cgColor", "review": "Swift lint issue", "dataset": "codereview_new", "original_id": 561, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\nclass StylableButton: UIButton, Stylable {\n    func setButtonBackgroundImageColor(_ color: UIColor?, for state: UIControl.State) {\n        guard let color = color else { return }\n        setBackgroundImageColor(color, for: state)\n    }\n    \n    func setButtonBorderColor(_ color: UIColor?, for state: UIControl.State) {\n        guard let color = color else { return }\n        self.layer.borderColor = color.cgColor\n    }\n    \n    func setButtonBorderWidth(_ width: CGFloat) {\n        self.layer.borderWidth = width\n    }\n    \n    func updateButtonStyle(_ style: ButtonStyle, isHighlighted: Bool, isSelected: Bool) {\n        setButtonBackgroundImageColor(style.normalStateColors.background, for: .normal)\n        setButtonBackgroundImageColor(style.highlightedStateColors.background, for: .highlighted)\n        setButtonBackgroundImageColor(style.selectedStateColors?.background, for: .selected)\n        \n        setButtonBorderColor(isHighlighted ? style.highlightedStateColors.border : style.normalStateColors.border, for: .highlighted)\n        setButtonBorderColor(isSelected ? style.selectedStateColors?.border : style.normalStateColors.border, for: .selected)\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 53.75169, "model_code": " class StylableButton: UIButton, Stylable { \n self.layer.borderWidth = 1 \n self.layer.borderColor = isHighlighted ? style.highlightedStateColors.border.cgColor : style.normalStateColors.border.cgColor \n self.layer.borderColor = isSelected ? style.selectedStateColors?.border.cgColor : style.normalStateColors.border.cgColor", "model_em_trim": 0, "model_bleu_trim": 51.12914791921979, "error_type_gpt": "Need Domain Knowledge"}
{"id": 179, "old": " class LocalLeaderEndPointTest {\n         .setEndOffset(6L))\n \n     assertEquals(expected, result)\n   }\n \n   private class CallbackResult[T] {", "new": " class LocalLeaderEndPointTest {\n         .setEndOffset(6L))\n \n     assertEquals(expected, result)\n+\n+    // Check missing epoch\n+    result = endPoint.fetchEpochEndOffsets(Map(\n+      topicPartition -> new OffsetForLeaderPartition()\n+        .setPartition(topicPartition.partition)\n+        .setLeaderEpoch(5)))\n+\n+    expected = Map(\n+      topicPartition -> new EpochEndOffset()\n+        .setPartition(topicPartition.partition)\n+        .setErrorCode(Errors.NONE.code)\n+        .setLeaderEpoch(-1)\n+        .setEndOffset(-1L))\n+\n+    assertEquals(expected, result)\n   }\n \n   private class CallbackResult[T] {", "review": "Could we also test a missing epoch like 3?", "dataset": "codereview_new", "original_id": 8268, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\nclass LocalLeaderEndPointTest {\n        .setEndOffset(6L))\n\n    assertEquals(expected, result)\n  }\n\n  @Test\n  def testMissingEpoch(): Unit = {\n    val epoch = 3L\n    val result = new CallbackResult[Map[TopicPartition, EpochEndOffset]]()\n    val expected = Map(new TopicPartition(\"test-topic\", 0) -> new EpochEndOffset(epoch, 0L))\n\n    endpoint.handleListOffsetRequest(\n      1,\n      \"test-client\",\n      new ListOffsetRequest.PartitionData(\n        epoch,\n        0L,\n        Optional.of(1),\n        ListOffsetRequest.CONSUMER_REPLICA_ID),\n      result,\n      null)\n\n    assertEquals(expected, result.value)\n  }\n\n  private class CallbackResult[T] {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 30.42695, "model_code": " class LocalLeaderEndPointTest { \n .setEndOffset(6L)) \n assertEquals(expected, result) \n } \n private class CallbackResult[T] {", "model_em_trim": 0, "model_bleu_trim": 3.567399334725241}
{"id": 180, "old": " class Bonus(NamedTuple):\n     def get_bonus(self, stability: float) -> float:\n         if not self.available:\n             return 0.0\n-        return self.value if stability > self.min_stability else 0.0", "new": " class Bonus(NamedTuple):\n     def get_bonus(self, stability: float) -> float:\n         if not self.available:\n             return 0.0\n+        return self.value if stability >= self.min_stability else 0.0", "review": "```suggestion\r\n        if not self.available or stability < self.min_stability:\r\n            return 0.0\r\n        return self.value\r\n```", "dataset": "codereview_new", "original_id": 6242, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass Bonus(NamedTuple):\n    def get_bonus(self, stability: float) -> float:\n        if not self.available or stability < self.min_stability:\n            return 0.0\n        return self.value\n", "gpt_em_trim": 0, "gpt_bleu_trim": 76.79841, "model_code": " class Bonus(NamedTuple): \n def get_bonus(self, stability: float) -> float: \n if not self.available or stability < self.min_stability: \n return 0.0 \n return self.value \n return stability", "model_em_trim": 0, "model_bleu_trim": 76.96375987398885}
{"id": 181, "old": " private val Stage.popups: List<Popup>\n     get() = actors.filterIsInstance<Popup>()\n \n /** @return The currently active [Popup] or [null] if none. */\n-// FIXME: We depend on the order of actors; is this fine?\n val BaseScreen.activePopup: Popup?\n     get() = popups.lastOrNull { it.isVisible }\n ", "new": " private val Stage.popups: List<Popup>\n     get() = actors.filterIsInstance<Popup>()\n \n /** @return The currently active [Popup] or [null] if none. */\n val BaseScreen.activePopup: Popup?\n     get() = popups.lastOrNull { it.isVisible }\n ", "review": "Yes.", "dataset": "codereview_new", "original_id": 1386, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-remove", "category_type_user1": "Documentation-remove", "category_type_user2": "Documentation-remove", "gpt_code": "\nprivate val Stage.popups: List<Popup>\n    get() = actors.filterIsInstance<Popup>()\n\n/** @return The currently active [Popup] or [null] if none. */\nval BaseScreen.activePopup: Popup?\n    get() = popups.lastOrNull { it.isVisible }\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " private val Stage.popups: List<Popup> \n get() = actors.filterIsInstance<Popup>() \n \n /** @return The currently active [Popup] or [null] if none. */ \n val BaseScreen.activePopup: Popup? \n get() = popups.lastOrNull { it.isVisible }", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 182, "old": " def show\n \n   def index\n     @projects = Project.active\n     respond_to do |format|\n-      format.html { @events = Event.upcoming_events(specified_project) }\n       format.json do\n-        @events = Event.upcoming_events(specified_project)\n         @scrums = EventInstance.this_month_until_now\n       end\n     end", "new": " def show\n \n   def index\n     @projects = Project.active\n+    @events = Event.upcoming_events(specified_project)\n     respond_to do |format|\n+      format.html { @events }\n       format.json do\n+        @events\n         @scrums = EventInstance.this_month_until_now\n       end\n     end", "review": "do we need to delete this?", "dataset": "codereview_new", "original_id": 6541, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ndef index\n  @projects = Project.active\n  respond_to do |format|\n    format.html { @events = Event.upcoming_events(specified_project) }\n    format.json do\n      @events = Event.upcoming_events(specified_project)\n      @scrums = EventInstance.this_month_until_now\n    end\n  end\nend\n", "gpt_em_trim": 0, "gpt_bleu_trim": 73.94172, "model_code": " def show \n def index \n respond_to do |format| \n format.html { @events = Event.upcoming_events(specified_project) } \n format.json do \n @events = Event.upcoming_events(specified_project) \n end \n end", "model_em_trim": 0, "model_bleu_trim": 57.56487666535324}
{"id": 183, "old": " internal virtual Document WithFrozenPartialSemantics(CancellationToken cancellat\n             if (solution.PartialSemanticsEnabled &&\n                 this.Project.SupportsCompilation)\n             {\n-                var linkedDocumentIds = this.GetLinkedDocumentIds();\n-                var newSolution = this.Project.Solution.WithFrozenPartialCompilationIncludingSpecificDocument(this.Id, linkedDocumentIds, cancellationToken);\n                 return newSolution.GetDocument(this.Id)!;\n             }\n             else", "new": " internal virtual Document WithFrozenPartialSemantics(CancellationToken cancellat\n             if (solution.PartialSemanticsEnabled &&\n                 this.Project.SupportsCompilation)\n             {\n+                var newSolution = this.Project.Solution.WithFrozenPartialCompilationIncludingSpecificDocument(this.Id, cancellationToken);\n                 return newSolution.GetDocument(this.Id)!;\n             }\n             else", "review": "Since we're already passing in the DocumentId for the primary document, can we just fetch the linked DocumentIds further down? I'm not sure why we're fetching it here only to pass it through.", "dataset": "codereview_new", "original_id": 7365, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ninternal virtual Document WithFrozenPartialSemantics(CancellationToken cancellationToken)\n{\n    if (solution.PartialSemanticsEnabled &&\n        this.Project.SupportsCompilation)\n    {\n        var newSolution = this.Project.Solution.WithFrozenPartialCompilationIncludingSpecificDocument(this.Id, this.GetLinkedDocumentIds(), cancellationToken);\n        return newSolution.GetDocument(this.Id)!;\n    }\n    else\n", "gpt_em_trim": 0, "gpt_bleu_trim": 80.88669, "model_code": " internal virtual Document WithFrozenPartialSemantics(CancellationToken cancellat \n ) \n { \n var newSolution = this.Project.Solution.WithFrozenPartialCompilationIncludingSpecificDocument(this.Id, cancellationToken); \n return newSolution.GetDocument(this.Id)!; \n } \n else", "model_em_trim": 0, "model_bleu_trim": 71.69617143884598, "error_type_gpt": "Need Domain Knowledge"}
{"id": 184, "old": " sub plugin_info {\n         description =>\n           \"Bypasses the Cloudflare Javascript-challenge by re-using cookies from your browser. Both CF cookies and the user-agent must originate from the same webbrowser.\",\n         parameters => [\n-            { type => \"string\", desc => \"user-agent\" },\n-\t\t\t{ type => \"string\", desc => \"csrftoken cookie of domain nhentai.net\" },\n-\t\t\t{ type => \"string\", desc => \"cf_clearance cookie of domain .nhentai.net\" }\n         ]\n     );\n ", "new": " sub plugin_info {\n         description =>\n           \"Bypasses the Cloudflare Javascript-challenge by re-using cookies from your browser. Both CF cookies and the user-agent must originate from the same webbrowser.\",\n         parameters => [\n+              { type => \"string\", desc => \"Browser UserAgent string (Can be found at http://useragentstring.com/ for your browser)\" },\n+\t\t\t{ type => \"string\", desc => \"csrftoken cookie for domain nhentai.net\" },\n+\t\t\t{ type => \"string\", desc => \"cf_clearance cookie for domain nhentai.net\" }\n         ]\n     );\n ", "review": "```suggestion\r\n\t\t\t{ type => \"string\", desc => \"cf_clearance cookie for domain nhentai.net\" }\r\n```", "dataset": "codereview_new", "original_id": 2910, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\nsub plugin_info {\n        description =>\n          \"Bypasses the Cloudflare Javascript-challenge by re-using cookies from your browser. Both CF cookies and the user-agent must originate from the same webbrowser.\",\n        parameters => [\n            { type => \"string\", desc => \"user-agent\" },\n\t\t\t{ type => \"string\", desc => \"csrftoken cookie of domain nhentai.net\" },\n\t\t\t{ type => \"string\", desc => \"cf_clearance cookie for domain nhentai.net\" }\n        ]\n    );\n", "gpt_em_trim": 0, "gpt_bleu_trim": 80.42964, "model_code": " sub plugin_info { \n description => \n \"Bypasses the Cloudflare Javascript-challenge by re-using cookies from your browser. Both CF cookies and the user-agent must originate from the same webbrowser.\", \n parameters => \n { type => \"string\", desc => \"user-agent\" }, \n { type => \"string\", desc => \"csrftoken cookie of domain nhentai.net\" }, \n { type => \"string\", desc => \"cf_clearance cookie for domain nhentai.net\" } \n ] \n ); \n", "model_em_trim": 0, "model_bleu_trim": 78.3502586187774}
{"id": 185, "old": " def wait_for_completion(self, timeout_seconds: Optional[int] = 600) -> bool:\n             elif status in {'INTERNAL_ERROR', 'FAILED', 'TIMEDOUT', 'CANCELED'}:\n                 result = RunsApi(self.api_client).get_run_output(self.res_job_id)\n                 # See here for the returned fields: https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/2.0/jobs#--response-structure-8\n-                # print out logs and stack trace if the job is failed\n-                logger.error(\"Feathr job is failed. Please visit this page to view error message: {}\", self.job_url)\n                 if \"error\" in result:\n                     logger.error(\"Error Code: {}\", result[\"error\"])\n                 if \"error_trace\" in result:", "new": " def wait_for_completion(self, timeout_seconds: Optional[int] = 600) -> bool:\n             elif status in {'INTERNAL_ERROR', 'FAILED', 'TIMEDOUT', 'CANCELED'}:\n                 result = RunsApi(self.api_client).get_run_output(self.res_job_id)\n                 # See here for the returned fields: https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/2.0/jobs#--response-structure-8\n+                # print out logs and stack trace if the job has failed\n+                logger.error(\"Feathr job has failed. Please visit this page to view error message: {}\", self.job_url)\n                 if \"error\" in result:\n                     logger.error(\"Error Code: {}\", result[\"error\"])\n                 if \"error_trace\" in result:", "review": "has failed", "dataset": "codereview_new", "original_id": 3905, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\ndef wait_for_completion(self, timeout_seconds: Optional[int] = 600) -> bool:\n            elif status in {'INTERNAL_ERROR', 'FAILED', 'TIMEDOUT', 'CANCELED'}:\n                result = RunsApi(self.api_client).get_run_output(self.res_job_id)\n                # See here for the returned fields: https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/2.0/jobs#--response-structure-8\n                # print out logs and stack trace if the job is failed\n                logger.error(\"Feathr job has failed. Please visit this page to view error message: {}\", self.job_url)\n                if \"error\" in result:\n                    logger.error(\"Error Code: {}\", result[\"error\"])\n                if \"error_trace\" in result:\n", "gpt_em_trim": 0, "gpt_bleu_trim": 98.69547, "model_code": " def wait_for_completion(self, timeout_seconds: Optional[int] = 600) -> bool: \n elif status in {'INTERNAL_ERROR', 'FAILED', 'TIMEDOUT', 'CANCELED'}: \n result = RunsApi(self.api_client).get_run_output(self.res_job_id) \n if \"error\" in result: \n logger.error(\"Error Code: {}\", result[\"error\"]) \n if \"error_trace\" in result:", "model_em_trim": 0, "model_bleu_trim": 44.77707829214367, "error_type_gpt": "Model fallacy"}
{"id": 186, "old": " class ScalaJSWorkerImpl extends ScalaJSWorkerApi {\n         .withSemantics(semantics)\n         .withModuleKind(scalaJSModuleKind)\n         .withESFeatures(scalaJSESFeatures)\n-        .withBatchMode(true)\n \n       // Separating ModuleSplitStyle in a standalone object avoids\n       // early classloading which fails in Scala.js versions where", "new": " class ScalaJSWorkerImpl extends ScalaJSWorkerApi {\n         .withSemantics(semantics)\n         .withModuleKind(scalaJSModuleKind)\n         .withESFeatures(scalaJSESFeatures)\n \n       // Separating ModuleSplitStyle in a standalone object avoids\n       // early classloading which fails in Scala.js versions where", "review": "Hum, you probably don't want this. This will force clearing every time, including upon success, discarding any incremental state.", "dataset": "codereview_new", "original_id": 3560, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nclass ScalaJSWorkerImpl extends ScalaJSWorkerApi {\n        .withSemantics(semantics)\n        .withModuleKind(scalaJSModuleKind)\n        .withESFeatures(scalaJSESFeatures)\n\n      // Separating ModuleSplitStyle in a standalone object avoids\n      // early classloading which fails in Scala.js versions where\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " class ScalaJSWorkerImpl extends ScalaJSWorkerApi { \n // Separating ModuleSplitStyle in a standalone object avoids \n // early classloading which fails in Scala.js versions where", "model_em_trim": 0, "model_bleu_trim": 53.81655866841118}
{"id": 187, "old": " extension JSONEncoder {\n         case raw\n     }\n \n-    /// The encoding strategy to use for key names\n     /// Defaults to .camelCase\n     public enum KeyNameEncodingStrategy {\n         // Convert key names to `camelCase`", "new": " extension JSONEncoder {\n         case raw\n     }\n \n+    /// The encoding strategy to use for key names in Codable implementations\n     /// Defaults to .camelCase\n     public enum KeyNameEncodingStrategy {\n         // Convert key names to `camelCase`", "review": "We should be clear that this is the key names *in JSON maps*.", "dataset": "codereview_new", "original_id": 1558, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nextension JSONEncoder {\n        case raw\n    }\n\n    /// The encoding strategy to use for key names in JSON maps\n    /// Defaults to .camelCase\n    public enum KeyNameEncodingStrategy {\n        // Convert key names to `camelCase`\n", "gpt_em_trim": 0, "gpt_bleu_trim": 90.97033, "model_code": " extension JSONEncoder { \n case raw \n } \n public enum KeyNameEncodingStrategy { \n // Convert key names to `camelCase`", "model_em_trim": 0, "model_bleu_trim": 31.933915222645126}
{"id": 188, "old": " internal static SyntaxToken GetArgumentModifiers(RefKind refKind)\n             var hasBody = !modifiers.IsAbstract && (!modifiers.IsPartial || statements != null);\n \n             if (!hasBody)\n-            {\n-                modifiers = modifiers - DeclarationModifiers.Async;\n-            }\n \n             return SyntaxFactory.MethodDeclaration(\n                 attributeLists: default,", "new": " internal static SyntaxToken GetArgumentModifiers(RefKind refKind)\n             var hasBody = !modifiers.IsAbstract && (!modifiers.IsPartial || statements != null);\n \n             if (!hasBody)\n+                modifiers -= DeclarationModifiers.Async;\n \n             return SyntaxFactory.MethodDeclaration(\n                 attributeLists: default,", "review": "does this work? (if not, leave as is):\r\n```suggestion\r\n            if (!hasBody)\r\n                modifiers -= DeclarationModifiers.Async;\r\n```", "dataset": "codereview_new", "original_id": 7475, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\ninternal static SyntaxToken GetArgumentModifiers(RefKind refKind)\n{\n    var hasBody = !modifiers.IsAbstract && (!modifiers.IsPartial || statements != null);\n\n    if (!hasBody)\n    {\n        modifiers -= DeclarationModifiers.Async;\n    }\n\n    return SyntaxFactory.MethodDeclaration(\n        attributeLists: default,\n", "gpt_em_trim": 0, "gpt_bleu_trim": 85.56858, "model_code": " internal static SyntaxToken GetArgumentModifiers(RefKind refKind) \n var hasBody = !modifiers.IsAbstract && (!modifiers.IsPartial || statements != null); \n return SyntaxFactory.MethodDeclaration( \n attributeLists: default,", "model_em_trim": 0, "model_bleu_trim": 72.57428524994968, "error_type_gpt": "Code Style Difference"}
{"id": 189, "old": " protocol GiphySearchViewControllerDelegate: AnyObject {\n     func giphySearchViewController(_ giphySearchViewController: GiphySearchViewController, didSelectImageData imageData: Data, searchTerm: String)\n }\n \n-extension UIImage {\n-    func imageWithColor(color: UIColor) -> UIImage {\n-        UIGraphicsBeginImageContextWithOptions(self.size, false, self.scale)\n-        color.setFill()\n-\n-        let context = UIGraphicsGetCurrentContext()\n-        context?.translateBy(x: 0, y: self.size.height)\n-        context?.scaleBy(x: 1.0, y: -1.0)\n-        context?.setBlendMode(CGBlendMode.normal)\n-\n-        let rect = CGRect(origin: .zero, size: CGSize(width: self.size.width, height: self.size.height))\n-        context?.clip(to: rect, mask: self.cgImage!)\n-        context?.fill(rect)\n-\n-        let newImage = UIGraphicsGetImageFromCurrentImageContext()\n-        UIGraphicsEndImageContext()\n-        return newImage!\n-    }\n-}\n-\n extension UISearchBar {\n     var textColor: UIColor? {\n         get {", "new": " protocol GiphySearchViewControllerDelegate: AnyObject {\n     func giphySearchViewController(_ giphySearchViewController: GiphySearchViewController, didSelectImageData imageData: Data, searchTerm: String)\n }\n \n extension UISearchBar {\n     var textColor: UIColor? {\n         get {", "review": "I think we don't need this", "dataset": "codereview_new", "original_id": 679, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nextension UIImage {\n    func imageWithColor(color: UIColor) -> UIImage {\n        UIGraphicsBeginImageContextWithOptions(self.size, false, self.scale)\n        color.setFill()\n\n        let context = UIGraphicsGetCurrentContext()\n        context?.translateBy(x: 0, y: self.size.height)\n        context?.scaleBy(x: 1.0, y: -1.0)\n        context?.setBlendMode(CGBlendMode.normal)\n\n        let rect = CGRect(origin: .zero, size: CGSize(width: self.size.width, height: self.size.height))\n        context?.clip(to: rect, mask: self.cgImage!)\n        context?.fill(rect)\n\n        let newImage = UIGraphicsGetImageFromCurrentImageContext()\n        UIGraphicsEndImageContext()\n        return newImage!\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 4.693529, "model_code": " protocol GiphySearchViewControllerDelegate: AnyObject { \n func giphySearchViewController(_ giphySearchViewController: GiphySearchViewController, didSelectImageData imageData: Data, searchTerm: String) \n }", "model_em_trim": 0, "model_bleu_trim": 63.23366621862498, "error_type_gpt": "Need Domain Knowledge"}
{"id": 190, "old": " struct DeleteGenshinStoredAccountView: View {\n                             Image(systemName: \"person.fill\")\n                             Text(account)\n                         }.frame(minWidth: 300, alignment: .center)\n-                    }\n-                    .padding([.top, .bottom])\n-                    .buttonStyle(.borderedProminent)\n-                    .controlSize(.large)\n-                    .tint(.accentColor)\n-                    .frame(width: 300, alignment: .center)\n                 }\n             }.frame(width: 450)\n             Spacer()", "new": " struct DeleteGenshinStoredAccountView: View {\n                             Image(systemName: \"person.fill\")\n                             Text(account)\n                         }.frame(minWidth: 300, alignment: .center)\n+                    }.controlSize(.large).buttonStyle(GrowingButton()).font(.title3)\n+                        .frame(width: 300, alignment: .center)\n                 }\n             }.frame(width: 450)\n             Spacer()", "review": "UI changes don't belong to this PR. Please open another PR for those.", "dataset": "codereview_new", "original_id": 436, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nstruct DeleteGenshinStoredAccountView: View {\n    var account: String\n    \n    var body: some View {\n        VStack {\n            Image(systemName: \"person.fill\")\n            Text(account)\n        }\n        .frame(minWidth: 300, alignment: .center)\n        .padding([.top, .bottom])\n        .buttonStyle(.borderedProminent)\n        .controlSize(.large)\n        .tint(.accentColor)\n        .frame(width: 300, alignment: .center)\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 66.30604, "model_code": " struct DeleteGenshinStoredAccountView: View { \n Image(systemName: \"person.fill\") \n Text(account) \n }.frame(minWidth: 300) \n } \n .padding([.top, .bottom]) \n .buttonStyle(.borderedProminent) \n .controlSize(.large) \n .tint(.accentColor) \n .frame(width: 300) \n } \n }.frame(width: 450) \n Spacer()", "model_em_trim": 0, "model_bleu_trim": 70.20516794703386}
{"id": 191, "old": " func validateUpdateRequest(req *pbs.UpdateHostRequest) error {\n \t\t\t\tattrs := req.GetItem().GetStaticHostAttributes()\n \t\t\t\tswitch {\n \t\t\t\tcase attrs == nil:\n-\t\t\t\t\tbadFields[\"attributes\"] = \"Attributes field not supplied request\"\n \t\t\t\tdefault:\n \t\t\t\t\tif attrs.GetAddress() == nil ||\n \t\t\t\t\t\tlen(strings.TrimSpace(attrs.GetAddress().GetValue())) < static.MinHostAddressLength ||", "new": " func validateUpdateRequest(req *pbs.UpdateHostRequest) error {\n \t\t\t\tattrs := req.GetItem().GetStaticHostAttributes()\n \t\t\t\tswitch {\n \t\t\t\tcase attrs == nil:\n+\t\t\t\t\tbadFields[\"attributes\"] = \"Attributes field not supplied in request\"\n \t\t\t\tdefault:\n \t\t\t\t\tif attrs.GetAddress() == nil ||\n \t\t\t\t\t\tlen(strings.TrimSpace(attrs.GetAddress().GetValue())) < static.MinHostAddressLength ||", "review": "Not your change, but this text reads a bit strange?\r\n```suggestion\r\n\t\t\t\t\tbadFields[\"attributes\"] = \"Attributes field not supplied in request\"\r\n```", "dataset": "codereview_new", "original_id": 5270, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nfunc validateUpdateRequest(req *pbs.UpdateHostRequest) error {\n\t\t\t\tattrs := req.GetItem().GetStaticHostAttributes()\n\t\t\t\tswitch {\n\t\t\t\tcase attrs == nil:\n\t\t\t\t\tbadFields[\"attributes\"] = \"Attributes field not supplied in request\"\n\t\t\t\tdefault:\n\t\t\t\t\tif attrs.GetAddress() == nil ||\n\t\t\t\t\t\tlen(strings.TrimSpace(attrs.GetAddress().GetValue())) < static.MinHostAddressLength ||\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": " func validateUpdateRequest(req *pbs.UpdateHostRequest) error { \n attrs := req.GetItem().GetStaticHostAttributes() \n switch { \n case attrs == nil: \n badFields[\"attributes\"] = \"Attributes field not supplied in request\" \n default: \n if attrs.GetAddress() == nil || \n len(strings.TrimSpace(attrs.GetAddress().GetValue())) < static.MinHostAddressLength ||", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 192, "old": " const (\n )\n \n // envDuration returns an environment variable's value as duration in milliseconds if it is exists,\n-// or the defaultValue if not or the value is not valid.\n func envDuration(key string, defaultValue time.Duration) time.Duration {\n \tv := os.Getenv(key)\n \tif v == \"\" {", "new": " const (\n )\n \n // envDuration returns an environment variable's value as duration in milliseconds if it is exists,\n+// or the defaultValue if the environment variable is not defined or the value is not valid.\n func envDuration(key string, defaultValue time.Duration) time.Duration {\n \tv := os.Getenv(key)\n \tif v == \"\" {", "review": "```suggestion\r\n// or the defaultValue if the environment variable is not defined or the value is not valid.\r\n```", "dataset": "codereview_new", "original_id": 5856, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nconst (\n)\n\n// envDuration returns an environment variable's value as duration in milliseconds if it exists,\n// or the defaultValue if the environment variable is not defined or the value is not valid.\nfunc envDuration(key string, defaultValue time.Duration) time.Duration {\n\tv := os.Getenv(key)\n\tif v == \"\" {\n\t\treturn defaultValue\n\t}\n\n\tduration, err := time.ParseDuration(v)\n\tif err != nil {\n\t\treturn defaultValue\n\t}\n\n\treturn duration\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 96.48382, "model_code": " const ( \n ) \n // envDuration returns an environment variable's value as duration in milliseconds if it is exists. \n func envDuration(key string, defaultValue time.Duration) time.Duration { \n v := os.Getenv(key) \n if v == \"\" {", "model_em_trim": 0, "model_bleu_trim": 68.278501524779, "error_type_gpt": "Insignificant Omission"}
{"id": 193, "old": " public sealed class BinaryLogger : ILogger\n         //   - TargetSkippedEventArgs: added OriginallySucceeded, Condition, EvaluatedCondition\n         // version 14:\n         //   - TargetSkippedEventArgs: added SkipReason, OriginalBuildEventContext\n-        // version 15:\n-        //   - Log our own environment variables by default. (This affects the message in the structured log viewer.)\n-        internal const int FileFormatVersion = 15;\n \n         private Stream stream;\n         private BinaryWriter binaryWriter;", "new": " public sealed class BinaryLogger : ILogger\n         //   - TargetSkippedEventArgs: added OriginallySucceeded, Condition, EvaluatedCondition\n         // version 14:\n         //   - TargetSkippedEventArgs: added SkipReason, OriginalBuildEventContext\n+        internal const int FileFormatVersion = 14;\n \n         private Stream stream;\n         private BinaryWriter binaryWriter;", "review": "I don't understand this. There's no actual change, but you want a marker to change things in the viewer?", "dataset": "codereview_new", "original_id": 8091, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\npublic sealed class BinaryLogger : ILogger\n        //   - TargetSkippedEventArgs: added OriginallySucceeded, Condition, EvaluatedCondition\n        // version 14:\n        //   - TargetSkippedEventArgs: added SkipReason, OriginalBuildEventContext\n        // version 15:\n        //   - Log our own environment variables by default. (This affects the message in the structured log viewer.)\n        // version 16:\n        //   - Added a marker to indicate changes in the viewer.\n        internal const int FileFormatVersion = 16;\n\n        private Stream stream;\n        private BinaryWriter binaryWriter;\n", "gpt_em_trim": 0, "gpt_bleu_trim": 45.08383, "model_code": " public sealed class BinaryLogger : ILogger \n //   - TargetSkippedEventArgs: added OriginallySucceeded, Condition, EvaluatedCondition \n private Stream stream; \n private BinaryWriter binaryWriter;", "model_em_trim": 0, "model_bleu_trim": 41.70324943312222}
{"id": 194, "old": " class GTPHeader(Packet):\n     def post_build(self, p, pay):\n         p += pay\n         if self.length is None:\n             tmp_len = len(p) - 4 if self.version == 2 else len(p) - 8\n             p = p[:2] + struct.pack(\"!H\", tmp_len) + p[4:]\n         return p", "new": " class GTPHeader(Packet):\n     def post_build(self, p, pay):\n         p += pay\n         if self.length is None:\n+            # The message length field is calculated different in GTPv1 and GTPv2.  # noqa: E501\n+            # For GTPv1 it is defined as the rest of the packet following the mandatory 8-byte GTP header  # noqa: E501\n+            # For GTPv2 it is defined as the length of the message in bytes excluding the mandatory part of the GTP-C header (the first 4 bytes)  # noqa: E501\n             tmp_len = len(p) - 4 if self.version == 2 else len(p) - 8\n             p = p[:2] + struct.pack(\"!H\", tmp_len) + p[4:]\n         return p", "review": "Could you explain how a test on the version fixes your issue?", "dataset": "codereview_new", "original_id": 6980, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-add", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-add", "gpt_code": "\nclass GTPHeader(Packet):\n    def post_build(self, p, pay):\n        p += pay\n        if self.length is None:\n            if self.version not in [1, 2]:\n                raise ValueError(\"Invalid version: {}\".format(self.version))\n            tmp_len = len(p) - 4 if self.version == 2 else len(p) - 8\n            p = p[:2] + struct.pack(\"!H\", tmp_len) + p[4:]\n        return p\n", "gpt_em_trim": 0, "gpt_bleu_trim": 48.98175, "model_code": " class GTPHeader(Packet): \n def post_build(self, p, pay): \n p += pay \n if self.version == 2: \n tmp_len = len(p) - 4 \n p = p[:2] + struct.pack(\"!H\", tmp_len) + p[4:] \n return p", "model_em_trim": 0, "model_bleu_trim": 26.849149090245465, "error_type_gpt": "Need Domain Knowledge"}
{"id": 195, "old": " func startMonitorThread(ctx context.Context, name string, f func(ctx context.Con\n \n func (pc *pipelineController) monitorPipeline(ctx context.Context, pipelineInfo *pps.PipelineInfo) {\n \tpipelineName := pipelineInfo.Pipeline.Name\n-\tlog.Printf(\"PPS master: monitoring pipeline %q/%q\", pipelineInfo.Pipeline.Project.GetName(), pipelineName)\n \tvar eg errgroup.Group\n \tpps.VisitInput(pipelineInfo.Details.Input, func(in *pps.Input) error { //nolint:errcheck\n \t\tif in.Cron != nil {", "new": " func startMonitorThread(ctx context.Context, name string, f func(ctx context.Con\n \n func (pc *pipelineController) monitorPipeline(ctx context.Context, pipelineInfo *pps.PipelineInfo) {\n \tpipelineName := pipelineInfo.Pipeline.Name\n+\tlog.Printf(\"PPS master: monitoring pipeline %q\", pipelineInfo.Pipeline)\n \tvar eg errgroup.Group\n \tpps.VisitInput(pipelineInfo.Details.Input, func(in *pps.Input) error { //nolint:errcheck\n \t\tif in.Cron != nil {", "review": "shouldn't this just print `pipelineInfo.Pipeline.String()`?", "dataset": "codereview_new", "original_id": 8642, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfunc startMonitorThread(ctx context.Context, name string, f func(ctx context.Con\n\nfunc (pc *pipelineController) monitorPipeline(ctx context.Context, pipelineInfo *pps.PipelineInfo) {\n\tpipelineName := pipelineInfo.Pipeline.String()\n\tlog.Printf(\"PPS master: monitoring pipeline %q\", pipelineName)\n\tvar eg errgroup.Group\n\tpps.VisitInput(pipelineInfo.Details.Input, func(in *pps.Input) error { //nolint:errcheck\n\t\tif in.Cron != nil {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 93.04119, "model_code": " func startMonitorThread(ctx context.Context, name string, f func(ctx context.Con \n ) { \n var eg errgroup.Group \n pps.VisitInput(pipelineInfo.Details.Input, func(in *pps.Input) error { //nolint:errcheck \n if in.Cron != nil {", "model_em_trim": 0, "model_bleu_trim": 45.108165850129545}
{"id": 196, "old": "\n #import <XCTest/XCTest.h>\n \n #import \"Source/santametricservice/Formats/SNTMetricFormatTestHelper.h\"\n #import \"Source/santametricservice/Formats/SNTMetricMonarchJSONFormat.h\"\n \n @interface SNTMetricMonarchJSONFormatTest : XCTestCase\n @end\n \n-// Stub out NSDate's date method\n-@implementation NSDate (custom)\n-\n-+ (instancetype)date {\n-  NSDateFormatter *formatter = NSDateFormatter.new;\n-  [formatter setDateFormat:@\"yyyy-MM-dd HH:mm:ssZZZ\"];\n-  return [formatter dateFromString:@\"2021-09-16 21:08:10+0000\"];\n-}\n-\n-@end\n-\n @implementation SNTMetricMonarchJSONFormatTest\n \n - (void)testMetricsConversionToJSON {\n   NSDictionary *validMetricsDict = [SNTMetricFormatTestHelper createValidMetricsDictionary];\n   SNTMetricMonarchJSONFormat *formatter = [[SNTMetricMonarchJSONFormat alloc] init];\n   NSError *err = nil;", "new": "\n #import <XCTest/XCTest.h>\n \n+#import <Foundation/Foundation.h>\n+#import <OCMock/OCMock.h>\n #import \"Source/santametricservice/Formats/SNTMetricFormatTestHelper.h\"\n #import \"Source/santametricservice/Formats/SNTMetricMonarchJSONFormat.h\"\n \n @interface SNTMetricMonarchJSONFormatTest : XCTestCase\n @end\n \n @implementation SNTMetricMonarchJSONFormatTest\n \n - (void)testMetricsConversionToJSON {\n+  id classMock = OCMClassMock([NSDate class]);\n+  NSDateFormatter *testDateFormatter = NSDateFormatter.new;\n+  [testDateFormatter setDateFormat:@\"yyyy-MM-dd HH:mm:ssZZZ\"];\n+  NSDate *mockedEndTimestamp = [testDateFormatter dateFromString:@\"2021-09-16 21:08:10+0000\"];\n+\n+  OCMStub([classMock date]).andReturn(mockedEndTimestamp);\n+\n   NSDictionary *validMetricsDict = [SNTMetricFormatTestHelper createValidMetricsDictionary];\n   SNTMetricMonarchJSONFormat *formatter = [[SNTMetricMonarchJSONFormat alloc] init];\n   NSError *err = nil;", "review": "It would be preferable to use OCMock for this, it's shorter and handles a lot of edge cases.", "dataset": "codereview_new", "original_id": 930, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n#import <XCTest/XCTest.h>\n#import <OCMock/OCMock.h>\n\n#import \"Source/santametricservice/Formats/SNTMetricFormatTestHelper.h\"\n#import \"Source/santametricservice/Formats/SNTMetricMonarchJSONFormat.h\"\n\n@interface SNTMetricMonarchJSONFormatTest : XCTestCase\n@end\n\n@implementation SNTMetricMonarchJSONFormatTest\n\n- (void)testMetricsConversionToJSON {\n  NSDictionary *validMetricsDict = [SNTMetricFormatTestHelper createValidMetricsDictionary];\n  SNTMetricMonarchJSONFormat *formatter = [[SNTMetricMonarchJSONFormat alloc] init];\n  \n  id mockDate = OCMClassMock([NSDate class]);\n  OCMStub([mockDate date]).andReturn([NSDate dateWithTimeIntervalSince1970:1631844490]);\n  \n  NSError *err = nil;\n  NSData *jsonData = [formatter JSONDataFromMetrics:validMetricsDict error:&err];\n  \n  XCTAssertNotNil(jsonData);\n  XCTAssertNil(err);\n}\n\n@end\n", "gpt_em_trim": 0, "gpt_bleu_trim": 59.02889, "model_code": " #import <XCTest/XCTest.h> \n @interface SNTMetricMonarchJSONFormatTest : XCTestCase \n @end \n @implementation SNTMetricMonarchJSONFormatTest \n - (void)testMetricsConversionToJSON {", "model_em_trim": 0, "model_bleu_trim": 0.3277268485606695, "error_type_gpt": "Need Domain Knowledge"}
{"id": 197, "old": "\n module.exports = {\n   global: {\n     lines: 64.39,\n-    branches: 53.03,\n-    statements: 63.66,\n     functions: 56.67,\n   },\n   transforms: {", "new": "\n module.exports = {\n   global: {\n     lines: 64.39,\n+    branches: 53.01,\n+    statements: 63.63,\n     functions: 56.67,\n   },\n   transforms: {", "review": "Was getting `Error: Coverage EXCEEDS threshold for global and must be bumped`\r\n\r\n![Screenshot 2023-03-03 at 11 29 33 AM](https://user-images.githubusercontent.com/8112138/222809633-6e7f7749-c8da-4339-ae86-b93d68b1733a.png)\r\n", "dataset": "codereview_new", "original_id": 9028, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nmodule.exports = {\n  global: {\n    lines: 65,\n    branches: 54,\n    statements: 64,\n    functions: 57,\n  },\n  transforms: {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 48.31543, "model_code": " /** \n * Copyright (c) 2015-present, Facebook, Inc. \n * All rights reserved. \n * \n * This source code is licensed under the Apache-2.0 license found in the \n * LICENSE file in the root directory of this source tree. \n */ \n \n module.exports = { \n global: { \n lines: 64.39, \n branches: 53.03, \n functions: 56.67, \n }, \n transforms: {", "model_em_trim": 0, "model_bleu_trim": 66.41993080980713}
{"id": 198, "old": " class PcRenameProvider(\n ) extends PcCollector[TextEdit](compiler, params) {\n   import compiler._\n   val newName: String = Identifier.backtickWrap(name.stripBackticks)\n-  def collect(tree: Tree, pos: Position): TextEdit = {\n-    val isBackticked = text(pos.start) == '`' && text(pos.end - 1) == '`'\n-    // when the old name contains backticks, the position is incorrect\n-    val isOldNameBackticked = text(pos.start) == '`' &&\n-      text(pos.end - 1) != '`' &&\n-      text(pos.end + 1) == '`'\n-    if (isBackticked)\n-      new TextEdit(pos.toLsp, \"`\" + newName.stripBackticks + \"`\")\n-    else if (isOldNameBackticked)\n-      new TextEdit(pos.withEnd(pos.end + 2).toLsp, newName)\n-    else new TextEdit(pos.toLsp, newName)\n   }\n \n   def rename(): List[TextEdit] =", "new": " class PcRenameProvider(\n ) extends PcCollector[TextEdit](compiler, params) {\n   import compiler._\n   val newName: String = Identifier.backtickWrap(name.stripBackticks)\n+  def collect(tree: Tree, toAdjust: Position): TextEdit = {\n+    val (pos, stripBackticks) = adjust(toAdjust)\n+    new TextEdit(\n+      pos.toLsp,\n+      if (stripBackticks) newName.stripBackticks else newName\n+    )\n   }\n \n   def rename(): List[TextEdit] =", "review": "```suggestion\r\n    if (isOldNameBackticked)\r\n      new TextEdit(pos.withEnd(pos.end + 1).withStart(pos.start - 1).toLsp, newName)\r\n    else new TextEdit(pos.toLsp, newName)\r\n```\r\ndoesn't seem that we need to strip backticks and add them again, no? Also `+2` doesn't seem correct. Could you make sure that the behaviour is consistent in VS Code and in tests?\r\n\r\nBtw. it seems we should handle backtick in `PcCollector`, since semantic highlighting also is having issues. So ideally we would adjust the position `pos` before giving it to the collect function.", "dataset": "codereview_new", "original_id": 3461, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nclass PcRenameProvider(\n) extends PcCollector[TextEdit](compiler, params) {\n  import compiler._\n  val newName: String = Identifier.backtickWrap(name.stripBackticks)\n  def collect(tree: Tree, pos: Position): TextEdit = {\n    val isBackticked = text(pos.start) == '`' && text(pos.end - 1) == '`'\n    // when the old name contains backticks, the position is incorrect\n    val isOldNameBackticked = text(pos.start) == '`' &&\n      text(pos.end - 1) != '`' &&\n      text(pos.end) == '`'\n    val adjustedPos = if (isOldNameBackticked) pos.withEnd(pos.end - 1).withStart(pos.start + 1) else pos\n    if (isBackticked)\n      new TextEdit(adjustedPos.toLsp, newName)\n    else new TextEdit(adjustedPos.toLsp, newName.stripBackticks)\n  }\n\n  def rename(): List[TextEdit] =\n", "gpt_em_trim": 0, "gpt_bleu_trim": 33.36511, "model_code": " class PcRenameProvider( \n ) extends PcCollector[TextEdit](compiler, params) { \n import compiler._ \n val newName: String = Identifier.backtickWrap(name.stripBackticks) \n def collect(tree: Tree, pos: Position): TextEdit = { \n if (isBackticked) \n new TextEdit(pos.toLsp, \"`\" + newName.stripBackticks + \"`\") \n else if (isOldNameBackticked) \n new TextEdit(pos.withEnd(pos.end + 2).toLsp, \"`\" + newName.stripBackticks + \"`\") \n else if (isOldNameBackticked) \n new TextEdit(pos.withEnd(pos.end + 2).toLsp, \"`\" + newName.stripBackticks + \"`\")", "model_em_trim": 0, "model_bleu_trim": 40.94308706361019}
{"id": 199, "old": " bool GenFaceNormalsProcess::GenMeshFaceNormals(aiMesh *pMesh) {\n         const aiVector3D *pV1 = &pMesh->mVertices[face.mIndices[0]];\n         const aiVector3D *pV2 = &pMesh->mVertices[face.mIndices[1]];\n         const aiVector3D *pV3 = &pMesh->mVertices[face.mIndices[face.mNumIndices - 1]];\n-        if (flippedWindingOrder_ != leftHanded_) // Boolean XOR\n             std::swap(pV2, pV3);\n         const aiVector3D vNor = ((*pV2 - *pV1) ^ (*pV3 - *pV1)).NormalizeSafe();\n ", "new": " bool GenFaceNormalsProcess::GenMeshFaceNormals(aiMesh *pMesh) {\n         const aiVector3D *pV1 = &pMesh->mVertices[face.mIndices[0]];\n         const aiVector3D *pV2 = &pMesh->mVertices[face.mIndices[1]];\n         const aiVector3D *pV3 = &pMesh->mVertices[face.mIndices[face.mNumIndices - 1]];\n+        // Boolean XOR - if either but not both of these flags is set, then the winding order has\n+        // changed and the cross product to calculate the normal needs to be reversed\n+        if (flippedWindingOrder_ != leftHanded_) \n             std::swap(pV2, pV3);\n         const aiVector3D vNor = ((*pV2 - *pV1) ^ (*pV3 - *pV1)).NormalizeSafe();\n ", "review": "Add comment explaining _why_.", "dataset": "codereview_new", "original_id": 6540, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-add", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-add", "gpt_code": "\nbool GenFaceNormalsProcess::GenMeshFaceNormals(aiMesh *pMesh) {\n        const aiVector3D *pV1 = &pMesh->mVertices[face.mIndices[0]];\n        const aiVector3D *pV2 = &pMesh->mVertices[face.mIndices[1]];\n        const aiVector3D *pV3 = &pMesh->mVertices[face.mIndices[face.mNumIndices - 1]];\n        if (flippedWindingOrder_ != leftHanded_) // If the winding order is flipped and not left-handed, swap pV2 and pV3\n            std::swap(pV2, pV3);\n        const aiVector3D vNor = ((*pV2 - *pV1) ^ (*pV3 - *pV1)).NormalizeSafe();\n", "gpt_em_trim": 0, "gpt_bleu_trim": 76.2102, "model_code": " bool GenFaceNormalsProcess::GenMeshFaceNormals(aiMesh *pMesh) { \n const aiVector3D *pV1 = &pMesh->mVertices[face.mIndices[0]]; \n const aiVector3D *pV2 = &pMesh->mVertices[face.mIndices[1]]; \n const aiVector3D *pV3 = &pMesh->mVertices[face.mIndices[face.mNumIndices - 1]]; \n if (flippedWindingOrder_ != leftHanded_) \n std::swap(pV2, pV3); \n const aiVector3D vNor = ((*pV2 - *pV1) ^ (*pV3 - *pV1)).NormalizeSafe(); \n", "model_em_trim": 0, "model_bleu_trim": 72.74667891506309, "error_type_gpt": "Need Domain Knowledge"}
{"id": 200, "old": "                     ->example('develop')\n                     ->defaultValue('')\n                 ->end()\n             ->end()\n         ;\n     }", "new": "                     ->example('develop')\n                     ->defaultValue('')\n                 ->end()\n+                ->scalarNode('route_prefix')\n+                    ->info('Defines the path of the Contao backend.')\n+                    ->validate()\n+                        ->ifTrue(static fn (string $prefix) => 1 !== preg_match('/^\\/\\S*[^\\/]$/', $prefix))\n+                        ->thenInvalid('The backend path must begin but not end with a slash. Invalid path configured: %s')\n+                    ->end()\n+                    ->example('/admin')\n+                    ->defaultValue('/contao')\n+                ->end()\n             ->end()\n         ;\n     }", "review": "I really don't like to create a config group for this only option. Maybe it could be added to `contao.security` or find a way to merge it with the `sanitizer` config?  ", "dataset": "codereview", "original_id": 16, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\n                    ->example('develop')\n                    ->defaultValue('')\n                    ->validate()\n                        ->ifTrue(function ($value) {\n                            return !in_array($value, ['develop', 'production']);\n                        })\n                        ->thenInvalid('Invalid environment value \"%s\"')\n                    ->end()\n                ->end()\n            ->end()\n        ;\n    }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 26.37978, "model_code": "\n ->example('develop') \n ->defaultValue('') \n ->end() \n ; \n }", "model_em_trim": 0, "model_bleu_trim": 0.3509878305007392}
{"id": 201, "old": "     }\n     /**\n-     * @param  string   $file_content_hash\n-     * @param  string   $file_path\n-     * @param mixed $file_modified_time\n-     *\n      * @return list<PhpParser\\Node\\Stmt>|null\n      */\n     public function loadExistingStatementsFromCache(string $file_path)", "new": "     }\n     /**\n      * @return list<PhpParser\\Node\\Stmt>|null\n      */\n     public function loadExistingStatementsFromCache(string $file_path)", "review": "Docblock param types are plain wrong here", "dataset": "codereview", "original_id": 32, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-remove", "category_type_user1": "Documentation-remove", "category_type_user2": "Documentation-remove", "gpt_code": "\n    /**\n     * @param string $file_path\n     *\n     * @return array|null\n     */\n    public function loadExistingStatementsFromCache(string $file_path)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 51.99776, "model_code": "\n } \n /** \n * @param string $file_content_hash \n * @param string $file_path \n * @param mixed $file_modified_time \n * \n * @return list<PhpParser\\Node\\Stmt>|null \n */ \n public function loadExistingStatementsFromCache(string $file_path)", "model_em_trim": 0, "model_bleu_trim": 89.48393168143697, "error_type_gpt": "Need Domain Knowledge"}
{"id": 202, "old": "       define_model_callbacks :build, :find, :initialize, :touch, only: :after\n       define_model_callbacks :create, :destroy, :save, :update, :upsert\n       # @api private\n-      define_model_callbacks :save_relations\n       attr_accessor :before_callback_halted\n     end", "new": "       define_model_callbacks :build, :find, :initialize, :touch, only: :after\n       define_model_callbacks :create, :destroy, :save, :update, :upsert\n+      # This callback is used internally by Mongoid to save association\n+      # targets for referenced associations after the parent model is persisted.\n+      #\n       # @api private\n+      define_model_callbacks :persist_parent\n       attr_accessor :before_callback_halted\n     end", "review": "Can this be `save_associations` please and when the PR is ready for review, can this declaration also have a note added to it explaining its purpose (e.g. \"This callback is used internally by Mongoid to save association targets for referenced associations after the parent model is saved\", if I understood its purpose correctly).", "dataset": "codereview", "original_id": 47, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-add", "gpt_code": "\n      define_model_callbacks :build, :find, :initialize, :touch, only: :after\n      define_model_callbacks :create, :destroy, :save, :update, :upsert\n      # @api private\n      define_model_callbacks :save_associations, :save_relations\n      attr_accessor :before_callback_halted\n    end\n```\n\n```\n      define_model_callbacks :build, :find, :initialize, :touch, only: :after\n      define_model_callbacks :create, :destroy, :save, :update, :upsert\n      # @api private\n      define_model_callbacks :save_associations, :save_relations\n      # This callback is used internally by Mongoid to save association targets for referenced associations after the parent model is saved\n      attr_accessor :before_callback_halted\n    end\n", "gpt_em_trim": 0, "gpt_bleu_trim": 45.78771, "model_code": "\n define_model_callbacks :build, :find, :initialize, :touch, only: :after \n define_model_callbacks :create, :destroy, :save, :update, :upsert \n # @api private \n define_model_callbacks :save_associations \n attr_accessor :before_callback_halted \n end", "model_em_trim": 0, "model_bleu_trim": 63.63602266656574, "error_type_gpt": "Need Domain Knowledge"}
{"id": 203, "old": " #ifdef HAVE_ERRNO_H\n #include <errno.h>\n-#elif defined(__APPLE__)\n-#include <errno.h>\n #endif\n-\n /*********************************************\n  *       UID, GID access methods             *\n  *********************************************/", "new": " #ifdef HAVE_ERRNO_H\n #include <errno.h>\n #endif\n /*********************************************\n  *       UID, GID access methods             *\n  *********************************************/", "review": "I think we should fix HAVE_ERRNO_H for APPLE", "dataset": "codereview", "original_id": 50, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Documentation-remove", "gpt_code": "\n#ifdef HAVE_ERRNO_H\n#include <errno.h>\n#elif defined(__APPLE__)\n#include <sys/errno.h>\n#endif\n\n/*********************************************\n *       UID, GID access methods             *\n *********************************************/\n", "gpt_em_trim": 0, "gpt_bleu_trim": 79.22494, "model_code": "\n /********************************************* \n *       UID, GID access methods             * \n *********************************************/", "model_em_trim": 0, "model_bleu_trim": 23.03661898473039, "error_type_gpt": "Need Domain Knowledge"}
{"id": 204, "old": "         \"\"\"\n         result = reply.result\n         if result[TXN_TYPE] in (SCHEMA, ISSUER_KEY):\n-            result = deepcopy(result)\n             result[DATA] = jsonSerz.serialize(result[DATA], toBytes=False)\n         txnWithMerkleInfo = self.storeTxnInLedger(result)", "new": "         \"\"\"\n         result = reply.result\n         if result[TXN_TYPE] in (SCHEMA, ISSUER_KEY):\n             result[DATA] = jsonSerz.serialize(result[DATA], toBytes=False)\n         txnWithMerkleInfo = self.storeTxnInLedger(result)", "review": "Why this line is necessary?", "dataset": "codereview", "original_id": 63, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n\"\"\"\nresult = reply.result\nif result[TXN_TYPE] in (SCHEMA, ISSUER_KEY):\n    # Create a deep copy of the result dictionary to avoid unintended consequences\n    result = deepcopy(result)\n    result[DATA] = jsonSerz.serialize(result[DATA], toBytes=False)\ntxnWithMerkleInfo = self.storeTxnInLedger(result)\n\"\"\"\n", "gpt_em_trim": 0, "gpt_bleu_trim": 56.11184, "model_code": "\n \"\"\" \n result = reply.result \n if result[TXN_TYPE] in (SCHEMA, ISSUER_KEY): \n result[DATA] = jsonSerz.serialize(result[DATA], toBytes=False) \n txnWithMerkleInfo = self.storeTxnInLedger(result)", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 205, "old": " \t\t\tcontents += \"}\\n\\n\"\n \t\t}\n \t}\n-\t// Helper function to determine how many optional fields are set in a union\n \tif s.Type == parser.StructTypeUnion {\n \t\tcontents += fmt.Sprintf(\"func (p *%s) CountSetFields%s() int {\\n\", sName, sName)\n \t\tcontents += \"\\tcount := 0\\n\"", "new": " \t\t\tcontents += \"}\\n\\n\"\n \t\t}\n \t}\n+\treturn contents\n+}\n+\n+// generateCountSetFields generates a helper function to determine how many\n+// optional fields are set in a union.\n+func (g *Generator) generateCountSetFields(s *parser.Struct, sName string) string {\n+\tcontents := \"\"\n \tif s.Type == parser.StructTypeUnion {\n \t\tcontents += fmt.Sprintf(\"func (p *%s) CountSetFields%s() int {\\n\", sName, sName)\n \t\tcontents += \"\\tcount := 0\\n\"", "review": "Doesn't `IsSet` only make sense for optional fields?", "dataset": "codereview", "original_id": 77, "relevance": "partial", "relevance_user1": "not", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Feature-add", "gpt_code": "\n\t\t\tcontents += \"}\\n\\n\"\n\t\t}\n\t}\n\t// Helper function to determine how many optional fields are set in a union\n\tif s.Type == parser.StructTypeUnion {\n\t\tcontents += fmt.Sprintf(\"func (p *%s) CountOptionalFields%s() int {\\n\", sName, sName)\n\t\tcontents += \"\\tcount := 0\\n\"\n", "gpt_em_trim": 0, "gpt_bleu_trim": 62.36933, "model_code": "\n contents += \"}\\n\\n\" \n } \n } \n // Helper function to determine how many optional fields are set in a union. \n if s.Type == parser.StructTypeUnion { \n contents += fmt.Sprintf(\"func (p *%s) CountSetFields%s() int {\\n\", sName, sName) \n contents += \"\\tcount := 0\\n\"", "model_em_trim": 0, "model_bleu_trim": 62.76224316631834}
{"id": 206, "old": "     icon_changed = pyqtSignal(QIcon)\n     #: Signal emitted when a page's title changed (new title as str)\n     title_changed = pyqtSignal(str)\n-    #: Signal emitted when a page's currently active search match changed (match as current/total)\n-    search_match_changed = pyqtSignal(int, int)\n     #: Signal emitted when this tab was pinned/unpinned (new pinned state as bool)\n     pinned_changed = pyqtSignal(bool)\n     #: Signal emitted when a new tab should be opened (url as QUrl)", "new": "     icon_changed = pyqtSignal(QIcon)\n     #: Signal emitted when a page's title changed (new title as str)\n     title_changed = pyqtSignal(str)\n     #: Signal emitted when this tab was pinned/unpinned (new pinned state as bool)\n     pinned_changed = pyqtSignal(bool)\n     #: Signal emitted when a new tab should be opened (url as QUrl)", "review": "This signal should be on the `AbstractSearch` rather than here, no?", "dataset": "codereview", "original_id": 92, "relevance": "partial", "relevance_user1": "perfect", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n    icon_changed = pyqtSignal(QIcon)\n    #: Signal emitted when a page's title changed (new title as str)\n    title_changed = pyqtSignal(str)\n    #: Signal emitted when this tab was pinned/unpinned (new pinned state as bool)\n    pinned_changed = pyqtSignal(bool)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 76.59283, "model_code": "\n icon_changed = pyqtSignal(QIcon) \n #: Signal emitted when a page's title changed (new title as str) \n title_changed = pyqtSignal(str) \n #: Signal emitted when a page's currently active search changed (match as current/total) \n search_changed = pyqtSignal(AbstractSearch) \n #: Signal emitted when this tab was pinned/unpinned (new pinned state as bool) \n pinned_changed = pyqtSignal(bool) \n #: Signal emitted when a new tab should be opened (url as QUrl)", "model_em_trim": 0, "model_bleu_trim": 72.6139861301918}
{"id": 207, "old": "         try:\n             self.base.conf._configure_from_options(opts)\n             self._read_conf_file(opts.releasever)\n-            self.base.conf.read_dropin()\n             self.base.conf._adjust_conf_options()\n         except (dnf.exceptions.ConfigError, ValueError) as e:\n             logger.critical(_('Config error: %s'), e)", "new": "         try:\n             self.base.conf._configure_from_options(opts)\n             self._read_conf_file(opts.releasever)\n+            self.base.conf.read_dropin_dir()\n             self.base.conf._adjust_conf_options()\n         except (dnf.exceptions.ConfigError, ValueError) as e:\n             logger.critical(_('Config error: %s'), e)", "review": "Please rename to `self.base.conf.read_dropin_dir()`", "dataset": "codereview", "original_id": 96, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\n        try:\n            self.base.conf._configure_from_options(opts)\n            self._read_conf_file(opts.releasever)\n            self.base.conf.read_dropin_dir()\n            self.base.conf._adjust_conf_options()\n        except (dnf.exceptions.ConfigError, ValueError) as e:\n            logger.critical(_('Config error: %s'), e)\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n try: \n self.base.conf._configure_from_options(opts) \n self._read_conf_file(opts.releasever) \n self.base.conf.read_dropin_dir() \n self.base.conf._adjust_conf_options() \n except (dnf.exceptions.ConfigError, ValueError) as e: \n logger.critical(_('Config error: %s'), e)", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 208, "old": " \t\t}\n \t\tconfigName = config.Name\n \t}\n-\tlog.Printf(\"running test using config: %s\\n\", configName)\n \t// First clean up any old test instances before we start the actual testing\n \t// as these might cause this test run to fail.", "new": " \t\t}\n \t\tconfigName = config.Name\n \t}\n+\tlog.Printf(\"Running test by using the instance config: %s\\n\", configName)\n \t// First clean up any old test instances before we start the actual testing\n \t// as these might cause this test run to fail.", "review": "=> `Running test by using the instance config`", "dataset": "codereview", "original_id": 98, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n\t\t}\n\t\tconfigName := config.Name\n\t\tlog.Printf(\"Running test using instance config: %s\\n\", configName)\n\t\t// First, clean up any old test instances before starting the actual testing\n\t\t// as these might cause the test run to fail.\n", "gpt_em_trim": 0, "gpt_bleu_trim": 80.11468, "model_code": "\n } \n configName = config.Name \n } \n log.Printf(\"Running test by using the instance config: %s\\n\", configName) \n // First clean up any old test instances before we start the actual testing \n // as these might cause this test run to fail.", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 209, "old": " \t// Inject tracing system baggage\n \treqHeaders := tchannel.InjectOutboundSpan(call.Response(), req.Headers.Items())\n-\tif err := writeRequestHeaders(ctx, format, reqHeaders, call.Arg2Writer, o.transport.forwardingHeader); err != nil {\n \t\t// TODO(abg): This will wrap IO errors while writing headers as encode\n \t\t// errors. We should fix that.\n \t\treturn nil, errors.RequestHeadersEncodeError(req, err)", "new": " \t// Inject tracing system baggage\n \treqHeaders := tchannel.InjectOutboundSpan(call.Response(), req.Headers.Items())\n+\tif err := writeRequestHeaders(ctx, format, reqHeaders, call.Arg2Writer, o.transport.exactCaseHeader); err != nil {\n \t\t// TODO(abg): This will wrap IO errors while writing headers as encode\n \t\t// errors. We should fix that.\n \t\treturn nil, errors.RequestHeadersEncodeError(req, err)", "review": "`forwardingHeader` does not read right here. Should read like \"use exact case\" as opposed to \"use canonicalized case\".", "dataset": "codereview", "original_id": 109, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\n\t// Inject tracing system baggage\n\treqHeaders := tchannel.InjectOutboundSpan(call.Response(), req.Headers.Items())\n\tif err := writeRequestHeaders(ctx, format, reqHeaders, call.Arg2Writer, o.transport.UseExactCaseHeader); err != nil {\n\t\t// TODO(abg): This will wrap IO errors while writing headers as encode\n\t\t// errors. We should fix that.\n\t\treturn nil, errors.RequestHeadersEncodeError(req, err)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 97.24144, "model_code": "\n // Inject tracing system baggage \n reqHeaders := tchannel.InjectOutboundSpan(call.Response(), req.Headers.Items()) \n if err := writeRequestHeaders(ctx, format, reqHeaders, call.Arg2Writer); err != nil { \n // TODO(abg): This will wrap IO errors while writing headers as encode \n // errors. We should fix that. \n return nil, errors.RequestHeadersEncodeError(req, err)", "model_em_trim": 0, "model_bleu_trim": 91.6737506279116, "error_type_gpt": "Need Domain Knowledge"}
{"id": 210, "old": "-'use strict';\n-\n require('classtool');\n function ClassSpec(b) {", "new": " require('classtool');\n function ClassSpec(b) {", "review": "For now, don't add \"use strict\" to any existing files. That's a separate project that we'll do later. (New files can use \"use strict\".)", "dataset": "codereview", "original_id": 130, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nrequire('classtool');\nfunction ClassSpec(b) {\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n require('classtool'); \n function ClassSpec(b) {", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 211, "old": " \t\"github.com/gorilla/mux\"\n )\n-// Presence of this header in the HTTP request indicates the chunk needs to be pinned\n const PinHeaderName = \"x-swarm-pin\"\n func (s *server) chunkUploadHandler(w http.ResponseWriter, r *http.Request) {", "new": " \t\"github.com/gorilla/mux\"\n )\n+// Presence of this header in the HTTP request indicates the chunk needs to be pinned.\n const PinHeaderName = \"x-swarm-pin\"\n func (s *server) chunkUploadHandler(w http.ResponseWriter, r *http.Request) {", "review": "End the sentence with a `.`.", "dataset": "codereview", "original_id": 149, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "no code", "gpt_em_trim": 0, "gpt_bleu_trim": 6.75e-84, "model_code": "\n \"github.com/gorilla/mux\" \n ) \n // Presence of this header in the HTTP request indicates the chunk needs to be pinned. \n const PinHeaderName = \"x-swarm-pin\" \n func (s *server) chunkUploadHandler(w http.ResponseWriter, r *http.Request) {", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 212, "old": "                              \"block until the IMPORTER forwarded all data\")\n                   .add<bool>(\"node,N\",\n                              \"spawn a node instead of connecting to one\")\n-                  .add<size_t>(\"num,n\",\n                                \"the maximum number of events to import\"));\n   import_->add(reader_command<format::zeek::reader>, \"zeek\",\n                \"imports Zeek logs from STDIN or file\", src_opts());", "new": "                              \"block until the IMPORTER forwarded all data\")\n                   .add<bool>(\"node,N\",\n                              \"spawn a node instead of connecting to one\")\n+                  .add<size_t>(\"max-events,n\",\n                                \"the maximum number of events to import\"));\n   import_->add(reader_command<format::zeek::reader>, \"zeek\",\n                \"imports Zeek logs from STDIN or file\", src_opts());", "review": "I find `num` too short for a long option.   How about using `max-events` instead? That also works for import and export, describes the intent better as well. Only downside, it doesn't begin with the same letter as the short option. I think that's fine because most people still find `-n` very idiomatic for a number of events.", "dataset": "codereview", "original_id": 157, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Documentation-modify", "category_type_user2": "Refactoring-rename", "gpt_code": "\n                             \"block until the IMPORTER forwarded all data\")\n                  .add<bool>(\"node,N\",\n                             \"spawn a node instead of connecting to one\")\n                  .add<size_t>(\"max-events,n\",\n                               \"the maximum number of events to import\"));\n  import_->add(reader_command<format::zeek::reader>, \"zeek\",\n               \"imports Zeek logs from STDIN or file\", src_opts());\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n \"block until the IMPORTER forwarded all data\") \n .add<bool>(\"node,N\", \n \"spawn a node instead of connecting to one\") \n .add<size_t>(\"max-events,n\", \n \"the maximum number of events to import\")); \n import_->add(reader_command<format::zeek::reader>, \"zeek\", \n \"imports Zeek logs from STDIN or file\", src_opts());", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 213, "old": "  */\n package org.flowable.scripting.secure.impl;\n import org.flowable.variable.api.delegate.VariableScope;\n import org.mozilla.javascript.Context;\n import org.mozilla.javascript.Scriptable;\n-import java.util.Map;\n-\n /**\n  * @author Joram Barrez\n  */", "new": "  */\n package org.flowable.scripting.secure.impl;\n+import java.util.Map;\n+\n import org.flowable.variable.api.delegate.VariableScope;\n import org.mozilla.javascript.Context;\n import org.mozilla.javascript.Scriptable;\n /**\n  * @author Joram Barrez\n  */", "review": "Project standards are to place `java` imports before `org` imports. The same change needs to be made in the next file too.", "dataset": "codereview", "original_id": 180, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-swap", "category_type_user1": "Refactoring-swap", "category_type_user2": "Refactoring-conventions", "gpt_code": "\npackage org.flowable.scripting.secure.impl;\n\nimport java.util.Map;\n\nimport org.flowable.variable.api.delegate.VariableScope;\nimport org.mozilla.javascript.Context;\nimport org.mozilla.javascript.Scriptable;\n\n/**\n * Author: Joram Barrez\n */\n```\n\nAnd the next file with the same change:\n\n```\npackage org.flowable.scripting.secure.impl;\n\nimport java.util.Map;\n\nimport org.flowable.engine.delegate.DelegateExecution;\nimport org.flowable.engine.delegate.JavaDelegate;\n\n/**\n * Author: Joram Barrez\n */\npublic class SecureJavascriptTask implements JavaDelegate {\n\n    @Override\n    public void execute(DelegateExecution execution) {\n        // ...\n    }\n\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 91.25887, "model_code": "\n */ \n package org.flowable.scripting.secure.impl; \n /** \n * @author Joram Barrez \n */", "model_em_trim": 0, "model_bleu_trim": 19.790182553544884, "error_type_gpt": "Unexpected Grammar Fix"}
{"id": 214, "old": "   protected Catalog createCatalog(String name, Map<String, String> properties, Configuration hadoopConf) {\n     CatalogLoader catalogLoader = createCatalogLoader(name, properties, hadoopConf);\n     String defaultDatabase = properties.getOrDefault(DEFAULT_DATABASE, \"default\");\n-    boolean cacheEnabled = Boolean.parseBoolean(properties.getOrDefault(CACHE_ENABLED, \"true\"));\n     Namespace baseNamespace = Namespace.empty();\n     if (properties.containsKey(BASE_NAMESPACE)) {\n       baseNamespace = Namespace.of(properties.get(BASE_NAMESPACE).split(\"\\\\.\"));\n     }\n     return new FlinkCatalog(name, defaultDatabase, baseNamespace, catalogLoader, cacheEnabled);\n   }", "new": "   protected Catalog createCatalog(String name, Map<String, String> properties, Configuration hadoopConf) {\n     CatalogLoader catalogLoader = createCatalogLoader(name, properties, hadoopConf);\n     String defaultDatabase = properties.getOrDefault(DEFAULT_DATABASE, \"default\");\n     Namespace baseNamespace = Namespace.empty();\n     if (properties.containsKey(BASE_NAMESPACE)) {\n       baseNamespace = Namespace.of(properties.get(BASE_NAMESPACE).split(\"\\\\.\"));\n     }\n+    boolean cacheEnabled = Boolean.parseBoolean(properties.getOrDefault(CACHE_ENABLED, \"true\"));\n     return new FlinkCatalog(name, defaultDatabase, baseNamespace, catalogLoader, cacheEnabled);\n   }", "review": "Why move this below `cacheEnabled`? That seems like it would cause unnecessary git conflicts.", "dataset": "codereview", "original_id": 197, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-swap", "category_type_user1": "Refactoring-swap", "category_type_user2": "Refactoring-swap", "gpt_code": "\n  protected Catalog createCatalog(String name, Map<String, String> properties, Configuration hadoopConf) {\n    CatalogLoader catalogLoader = createCatalogLoader(name, properties, hadoopConf);\n    String defaultDatabase = properties.getOrDefault(DEFAULT_DATABASE, \"default\");\n    Namespace baseNamespace = Namespace.empty();\n    if (properties.containsKey(BASE_NAMESPACE)) {\n      baseNamespace = Namespace.of(properties.get(BASE_NAMESPACE).split(\"\\\\.\"));\n    }\n    boolean cacheEnabled = Boolean.parseBoolean(properties.getOrDefault(CACHE_ENABLED, \"true\"));\n    return new FlinkCatalog(name, defaultDatabase, baseNamespace, catalogLoader, cacheEnabled);\n  }\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n protected Catalog createCatalog(String name, Map<String, String> properties, Configuration hadoopConf) { \n CatalogLoader catalogLoader = createCatalogLoader(name, properties, hadoopConf); \n String defaultDatabase = properties.getOrDefault(DEFAULT_DATABASE, \"default\"); \n Namespace baseNamespace = Namespace.empty(); \n if (properties.containsKey(BASE_NAMESPACE)) { \n baseNamespace = Namespace.of(properties.get(BASE_NAMESPACE).split(\"\\\\.\")); \n } \n boolean cacheEnabled = Boolean.parseBoolean(properties.getOrDefault(CACHE_ENABLED, \"true\")); \n return new FlinkCatalog(name, defaultDatabase, baseNamespace, catalogLoader, cacheEnabled); \n }", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 215, "old": " \tMetadataAddr string `config:\"hostname;127.0.0.1;die-on-fail\"`\n \tMetadataPort int    `config:\"int(0,65535);8775;die-on-fail\"`\n-\tOpenstackRegion string `config:\"string;;local\"`\n-\n \tInterfacePrefix  string `config:\"iface-list;cali;non-zero,die-on-fail\"`\n \tInterfaceExclude string `config:\"iface-list;kube-ipvs0\"`", "new": " \tMetadataAddr string `config:\"hostname;127.0.0.1;die-on-fail\"`\n \tMetadataPort int    `config:\"int(0,65535);8775;die-on-fail\"`\n \tInterfacePrefix  string `config:\"iface-list;cali;non-zero,die-on-fail\"`\n \tInterfaceExclude string `config:\"iface-list;kube-ipvs0\"`", "review": "Bit worried about the prefix since this seem to be being used for non-Openstack data too (host endpoints). Should we just leave host endpoint status at the old path (or remove it since AIFAIK, it's not used anywhere)?", "dataset": "codereview", "original_id": 217, "relevance": "partial", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n\tMetadataAddr string `config:\"hostname;127.0.0.1;die-on-fail\"`\n\tMetadataPort int    `config:\"int(0,65535);8775;die-on-fail\"`\n\tOpenstackRegion string `config:\"string;;local\"`\n\n\tEndpointPrefix  string `config:\"iface-list;cali;non-zero,die-on-fail\"`\n\tInterfaceExclude string `config:\"iface-list;kube-ipvs0\"`\n", "gpt_em_trim": 0, "gpt_bleu_trim": 84.59402, "model_code": "\n MetadataAddr string `config:\"hostname;127.0.0.1;die-on-fail\"` \n MetadataPort int    `config:\"int(0,65535);8775;die-on-fail\"` \n InterfacePrefix  string `config:\"iface-list;cali;non-zero,die-on-fail\"` \n InterfaceExclude string `config:\"iface-list;kube-ipvs0\"`", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 216, "old": " import static org.assertj.core.api.Assertions.assertThat;\n import tech.pegasys.pantheon.consensus.ibft.ibftmessage.CommitMessage;\n import tech.pegasys.pantheon.consensus.ibft.ibftmessage.NewRoundMessage;\n import tech.pegasys.pantheon.consensus.ibft.ibftmessage.PrepareMessage;\n import tech.pegasys.pantheon.consensus.ibft.ibftmessage.ProposalMessage;", "new": " import static org.assertj.core.api.Assertions.assertThat;\n+import java.util.Arrays;\n import tech.pegasys.pantheon.consensus.ibft.ibftmessage.CommitMessage;\n+import tech.pegasys.pantheon.consensus.ibft.ibftmessage.IbftV2;\n import tech.pegasys.pantheon.consensus.ibft.ibftmessage.NewRoundMessage;\n import tech.pegasys.pantheon.consensus.ibft.ibftmessage.PrepareMessage;\n import tech.pegasys.pantheon.consensus.ibft.ibftmessage.ProposalMessage;", "review": "instead of using else if here we could use a switch here and use the message type for case. should make it bit easier to read. something like ``` switch (expectedPayload.getMessageType()) { case IbftV2.PROPOSAL: return ProposalMessage.fromMessage(actual).decode().equals(expected); case IbftV2.PREPARE: return PrepareMessage.fromMessage(actual).decode().equals(expected); ... ```", "dataset": "codereview", "original_id": 237, "relevance": "not", "relevance_user1": "partial", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "vague question", "information_user2": "concrete suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\nimport static org.assertj.core.api.Assertions.assertThat;\nimport tech.pegasys.pantheon.consensus.ibft.ibftmessage.CommitMessage;\nimport tech.pegasys.pantheon.consensus.ibft.ibftmessage.NewRoundMessage;\nimport tech.pegasys.pantheon.consensus.ibft.ibftmessage.PrepareMessage;\nimport tech.pegasys.pantheon.consensus.ibft.ibftmessage.ProposalMessage;\n\nswitch (expectedPayload.getMessageType()) {\n    case IbftV2.PROPOSAL:\n        return ProposalMessage.fromMessage(actual).decode().equals(expected);\n    case IbftV2.PREPARE:\n        return PrepareMessage.fromMessage(actual).decode().equals(expected);\n    case IbftV2.COMMIT:\n        return CommitMessage.fromMessage(actual).decode().equals(expected);\n    case IbftV2.NEW_ROUND:\n        return NewRoundMessage.fromMessage(actual).decode().equals(expected);\n    default:\n        throw new IllegalArgumentException(\"Invalid message type: \" + expectedPayload.getMessageType());\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 76.27681, "model_code": "\n import static org.assertj.core.api.Assertions.assertThat; \n import tech.pegasys.pantheon.consensus.ibft.ibftmessage.CommitMessage; \n import tech.pegasys.pantheon.consensus.ibft.ibftmessage.ProposalMessage; \n import tech.pegasys.pantheon.consensus.ibft.ibftmessage.NewRoundMessage; \n import tech.pegasys.pantheon.consensus.ibft.ibftmessage.PrepareMessage; \n import tech.pegasys.pantheon.consensus.ibft.ibftmessage.ProposalMessage;", "model_em_trim": 0, "model_bleu_trim": 89.60551319506912}
{"id": 217, "old": " // groupByVersion adds compound nodes to group multiple versions of the same app\n func groupByVersion(nodes *[]*NodeWrapper) {\n-\tlog.Warningf(\"GroupByVersion...\")\n \tappBox := make(map[string][]*NodeData)\n \tfor _, nw := range *nodes {", "new": " // groupByVersion adds compound nodes to group multiple versions of the same app\n func groupByVersion(nodes *[]*NodeWrapper) {\n \tappBox := make(map[string][]*NodeData)\n \tfor _, nw := range *nodes {", "review": "Minor. Is this log.Warningf() a real Warning or something more Info/Debug ?", "dataset": "codereview", "original_id": 290, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n// groupByVersion adds compound nodes to group multiple versions of the same app\nfunc groupByVersion(nodes *[]*NodeWrapper) {\n\tlog.Debugf(\"GroupByVersion...\")\n\tappBox := make(map[string][]*NodeData)\n\tfor _, nw := range *nodes {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 78.84358, "model_code": "\n // groupByVersion adds compound nodes to group multiple versions of the same app \n func groupByVersion(nodes *[]*NodeWrapper) { \n appBox := make(map[string][]*NodeData) \n for _, nw := range *nodes {", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Need Domain Knowledge"}
{"id": 218, "old": "                 for dst_type in <list>dest_sig:\n                     found_matches = []\n                     found_candidates = []\n-                    # Make two seperate lists: One for for signature sub-trees with at least one definite match, and another for signature sub-trees with only ambiguous matches (where `dest_sig[i] is None`).\n                     if dst_type is None:\n                         for sn in <list>sigindex_matches:\n                             found_matches.extend((<dict>sn).values())", "new": "                 for dst_type in <list>dest_sig:\n                     found_matches = []\n                     found_candidates = []\n+                    # Make two seperate lists: One for for signature sub-trees\n+                    #        with at least one definite match, and another for\n+                    #        signature sub-trees with only ambiguous matches\n+                    #        (where `dest_sig[i] is None`).\n                     if dst_type is None:\n                         for sn in <list>sigindex_matches:\n                             found_matches.extend((<dict>sn).values())", "review": "And this line too.", "dataset": "codereview", "original_id": 294, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-add", "gpt_code": "\n                for dst_type in dest_sig:\n                    found_matches = []\n                    found_candidates = []\n                    # Make two separate lists: One for signature sub-trees with at least one definite match, and another for signature sub-trees with only ambiguous matches (where `dest_sig[i] is None`).\n                    if dst_type is None:\n                        for sn in sigindex_matches:\n                            found_matches.extend(sn.values())\n", "gpt_em_trim": 0, "gpt_bleu_trim": 72.13601, "model_code": "\n for dst_type in <list>dest_sig: \n found_matches = [] \n found_candidates = [] \n # Make two seperate lists: One for for signature sub-trees with at least one definite match, and another for signature sub-trees with only ambiguous matches (where `dest_sig[i] is None`). \n if dst_type is None: \n for sn in <list>sigindex_matches: \n found_matches.extend((<dict>sn).values())", "model_em_trim": 0, "model_bleu_trim": 92.79986556883262, "error_type_gpt": "Need Domain Knowledge"}
{"id": 219, "old": " \tDockerfile *string           `yaml:\"dockerfile,omitempty\"`\n \tArgs       map[string]string `yaml:\"args,omitempty\"`\n \tTarget     *string           `yaml:\"target,omitempty\"`\n-\tPlatform   string            `yaml:\"platform,omitempty\"`\n-\tCacheFrom  []string          `yaml:\"cache_from,omitempty\"`\n }\n func (b *DockerBuildArgs) isEmpty() bool {", "new": " \tDockerfile *string           `yaml:\"dockerfile,omitempty\"`\n \tArgs       map[string]string `yaml:\"args,omitempty\"`\n \tTarget     *string           `yaml:\"target,omitempty\"`\n+\t//Platform   string            `yaml:\"platform,omitempty\"`\n+\tCacheFrom []string `yaml:\"cache_from,omitempty\"`\n }\n func (b *DockerBuildArgs) isEmpty() bool {", "review": "Is it possible to define `Platform` in a single location? preferably in `TaskConfig`   This will allow users to define `platform` under `image.build` because of the `yaml` tags which I think we are trying to avoid   Maybe we can do it through   ```go func (s *BackendService) TaskPlatform() string func (s *BackendService) BuildArgs() *DockerBuildArgs ```", "dataset": "codereview", "original_id": 316, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Documentation-and-code", "category_type_user2": "Feature-remove", "gpt_code": "\ntype TaskConfig struct {\n    Platform string `yaml:\"platform,omitempty\"`\n    // other fields\n}\n\ntype DockerBuildArgs struct {\n    Dockerfile *string           `yaml:\"dockerfile,omitempty\"`\n    Args       map[string]string `yaml:\"args,omitempty\"`\n    Target     *string           `yaml:\"target,omitempty\"`\n    CacheFrom  []string          `yaml:\"cache_from,omitempty\"`\n}\n\nfunc (s *BackendService) TaskPlatform() string {\n    return s.TaskConfig.Platform\n}\n\nfunc (s *BackendService) BuildArgs() *DockerBuildArgs {\n    return &DockerBuildArgs{\n        Dockerfile: s.TaskConfig.Dockerfile,\n        Args:       s.TaskConfig.Args,\n        Target:     s.TaskConfig.Target,\n        CacheFrom:  s.TaskConfig.CacheFrom,\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 73.97751, "model_code": "\n Dockerfile *string           `yaml:\"dockerfile,omitempty\"` \n Args       map[string]string `yaml:\"args,omitempty\"` \n Target     *string           `yaml:\"target,omitempty\"` \n Platform   string            `yaml:\"platform,omitempty\"` \n CacheFrom  []string          `yaml:\"cache_from,omitempty\"` \n } \n func (b *DockerBuildArgs) isEmpty() bool {", "model_em_trim": 0, "model_bleu_trim": 95.52505599884049}
{"id": 220, "old": " \t\t\t\tcc, err = cl.GetChainConfig()\n \t\t\t\tif err != nil {\n \t\t\t\t\tcl = nil\n-\t\t\t\t\tfmt.Printf(\"Could not get chain config from %v: %v\\n\", si, err)\n \t\t\t\t\tcontinue\n \t\t\t\t}\n \t\t\t\tcl.Roster = cc.Roster", "new": " \t\t\t\tcc, err = cl.GetChainConfig()\n \t\t\t\tif err != nil {\n \t\t\t\t\tcl = nil\n+\t\t\t\t\tlog.Warnf(\"Could not get chain config from %v: %v\\n\", si, err)\n \t\t\t\t\tcontinue\n \t\t\t\t}\n \t\t\t\tcl.Roster = cc.Roster", "review": "This should probably using the logger.", "dataset": "codereview", "original_id": 352, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n\t\t\t\tcc, err = cl.GetChainConfig()\n\t\t\t\tif err != nil {\n\t\t\t\t\tcl = nil\n\t\t\t\t\tlog.Printf(\"Could not get chain config from %v: %v\\n\", si, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tcl.Roster = cc.Roster\n", "gpt_em_trim": 0, "gpt_bleu_trim": 87.9179, "model_code": "\n cc, err = cl.GetChainConfig() \n if err != nil { \n cl = nil \n logger.Printf(\"Could not get chain config from %v: %v\\n\", si, err) \n continue \n } \n cl.Roster = cc.Roster", "model_em_trim": 0, "model_bleu_trim": 91.44650624011177, "error_type_gpt": "Unclear Changes"}
{"id": 221, "old": "       for (const auto &tx : proposal.transactions()) {\n         if (auto tx_answer = transaction_validator_->validate(tx)) {\n           reason.second.emplace_back(tx_answer.reason());\n-          break;\n         }\n       }", "new": "       for (const auto &tx : proposal.transactions()) {\n         if (auto tx_answer = transaction_validator_->validate(tx)) {\n           reason.second.emplace_back(tx_answer.reason());\n         }\n       }", "review": "Why not validate all transactions in proposal here? Right now there can ve several invalid transactions, but only one error.", "dataset": "codereview", "original_id": 363, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n      for (const auto &tx : proposal.transactions()) {\n        if (auto tx_answer = transaction_validator_->validate(tx)) {\n          reason.second.emplace_back(tx_answer.reason());\n        }\n      }\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n for (const auto &tx : proposal.transactions()) { \n if (auto tx_answer = transaction_validator_->validate(tx)) { \n reason.second.emplace_back(tx_answer.reason()); \n } \n break; \n } \n }", "model_em_trim": 0, "model_bleu_trim": 98.09529623602027}
{"id": 222, "old": "     <h1>Error 503 Backend is unhealthy</h1>\n     <p>Backend is unhealthy</p>\n     <h3>Guru Mediation:</h3>\n-    <p>Details: cache-sea4477-SEA 1645540748 3656046813</p>\n     <hr>\n     <p>Varnish cache server</p>\n   </body>", "new": "     <h1>Error 503 Backend is unhealthy</h1>\n     <p>Backend is unhealthy</p>\n     <h3>Guru Mediation:</h3>\n+    <p>Details: cache-sea4466-SEA 1645540748 606885694</p>\n     <hr>\n     <p>Varnish cache server</p>\n   </body>", "review": "This function name does not describe well what it does.", "dataset": "codereview", "original_id": 388, "relevance": "not", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n    <h1>Error 503: Service Unavailable</h1>\n    <p>The requested service is currently unavailable. Please try again later.</p>\n", "gpt_em_trim": 0, "gpt_bleu_trim": 92.10399, "model_code": "\n <h1>Error 503 Backend is unhealthy</h1> \n <p>Backend is unhealthy</p> \n <h3>Guru Mediation:</h3> \n <p>Details: cache-sea4444-SEA 1645540748 216325867</p> \n <hr> \n <p>Varnish cache server</p> \n </body>", "model_em_trim": 0, "model_bleu_trim": 92.10399279903714}
{"id": 223, "old": "         {\n             LogManager.Configuration = CreateConfigurationFromString(@\"\n             <nlog>\n-                <targets><target name='debug' type='Debug' layout='${message:padding=-3:padcharacter=x:fixedlength=true:alignmentOnTruncation=left}' /></targets>\n                 <rules>\n                     <logger name='*' minlevel='Debug' writeTo='debug' />\n                 </rules>", "new": "         {\n             LogManager.Configuration = CreateConfigurationFromString(@\"\n             <nlog>\n+                <targets><target name='debug' type='Debug' layout='${message:padding=-3:padcharacter=x:fixedlength=true}' /></targets>\n                 <rules>\n                     <logger name='*' minlevel='Debug' writeTo='debug' />\n                 </rules>", "review": "Please revert this one also", "dataset": "codereview", "original_id": 392, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n{\n    LogManager.Configuration = CreateConfigurationFromString(@\"\n    <nlog>\n        <targets>\n            <target name='debug' type='Debug' layout='${message:padding=-3:padcharacter=x:fixedlength=true:alignmentOnTruncation=left}' />\n        </targets>\n        <rules>\n            <logger name='*' minlevel='Debug' writeTo='debug' />\n        </rules>\n    </nlog>\n    \");\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 93.36941, "model_code": "\n { \n LogManager.Configuration = CreateConfigurationFromString(@\" \n <nlog> \n <targets><target name='debug' type='Debug' layout='${message:padding=-3:padcharacter=x:fixedlength=true:alignmentOnTruncation=left}' /></targets> \n <rules> \n <logger name='*' minlevel='Debug' writeTo='debug' /> \n </rules>", "model_em_trim": 0, "model_bleu_trim": 93.36941493963448, "error_type_gpt": "Need Domain Knowledge"}
{"id": 224, "old": " \t\t\t\t\t\t\t'sanitize_callback' => 'absint',\n \t\t\t\t\t\t),\n \t\t\t\t\t\t'discover'  => array(\n-\t\t\t\t\t\t\t'description' => __( 'Whether to perform an oEmbed discovery request for a non-curated providers.' ),\n \t\t\t\t\t\t\t'type'        => 'boolean',\n \t\t\t\t\t\t\t'default'     => true,\n \t\t\t\t\t\t),", "new": " \t\t\t\t\t\t\t'sanitize_callback' => 'absint',\n \t\t\t\t\t\t),\n \t\t\t\t\t\t'discover'  => array(\n+\t\t\t\t\t\t\t'description' => __( 'Whether to perform an oEmbed discovery request for unsanctioned providers.' ),\n \t\t\t\t\t\t\t'type'        => 'boolean',\n \t\t\t\t\t\t\t'default'     => true,\n \t\t\t\t\t\t),", "review": "```suggestion 'description' => __( 'Whether to perform an oEmbed discovery request for a non-curated provider.' ), ```", "dataset": "codereview", "original_id": 416, "relevance": "not", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n\t\t\t\t\t\t\t'sanitize_callback' => 'absint',\n\t\t\t\t\t\t),\n\t\t\t\t\t\t'discover'  => array(\n\t\t\t\t\t\t\t'description' => __( 'Whether to perform an oEmbed discovery request for a non-curated provider.' ),\n\t\t\t\t\t\t\t'type'        => 'boolean',\n\t\t\t\t\t\t\t'default'     => true,\n\t\t\t\t\t\t),\n", "gpt_em_trim": 0, "gpt_bleu_trim": 89.67934, "model_code": "\n 'sanitize_callback' => 'absint', \n ), \n 'discover'  => array( \n 'description' => __( 'Whether to perform an oEmbed discovery request for a non-curated provider.' ), \n 'type'        => 'boolean', \n 'default'     => true, \n ),", "model_em_trim": 0, "model_bleu_trim": 89.67933956511018}
{"id": 225, "old": "         int theme = getTheme();\n         if (theme == R.style.Theme_AntennaPod_Dark) {\n             return R.style.Theme_AntennaPod_Dark_NoTitle;\n-        }else if (theme == R.style.Theme_AntennaPod_TrueBlack){\n             return R.style.Theme_AntennaPod_TrueBlack_NoTitle;\n         } else {\n             return R.style.Theme_AntennaPod_Light_NoTitle;", "new": "         int theme = getTheme();\n         if (theme == R.style.Theme_AntennaPod_Dark) {\n             return R.style.Theme_AntennaPod_Dark_NoTitle;\n+        } else if (theme == R.style.Theme_AntennaPod_TrueBlack) {\n             return R.style.Theme_AntennaPod_TrueBlack_NoTitle;\n         } else {\n             return R.style.Theme_AntennaPod_Light_NoTitle;", "review": "There are spaces missing ;) Just have a look at the code style of the `if` statement above", "dataset": "codereview", "original_id": 419, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\nint theme = getTheme();\nif (theme == R.style.Theme_AntennaPod_Dark) {\n    return R.style.Theme_AntennaPod_Dark_NoTitle;\n} else if (theme == R.style.Theme_AntennaPod_TrueBlack) {\n    return R.style.Theme_AntennaPod_TrueBlack_NoTitle;\n} else {\n    return R.style.Theme_AntennaPod_Light_NoTitle;\n}\n", "gpt_em_trim": 1, "gpt_bleu_trim": 98.79868, "model_code": "\n int theme = getTheme(); \n if (theme == R.style.Theme_AntennaPod_Dark) { \n return R.style.Theme_AntennaPod_Dark_NoTitle; \n } else if (theme == R.style.Theme_AntennaPod_TrueBlack) { \n return R.style.Theme_AntennaPod_TrueBlack_NoTitle; \n } else { \n return R.style.Theme_AntennaPod_Light_NoTitle;", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 226, "old": "       .description(__('New Application'))\n       .option('--simple', __('create a barebones project meant only for contract development'))\n       .option('--locale [locale]', __('language to use (default: en)'))\n-      .option('--template [name/url]', __('download a known template given its name or from a GitHub repository'))\n       .action(function(name, options) {\n         i18n.setOrDetectLocale(options.locale);\n         if (name === undefined) {", "new": "       .description(__('New Application'))\n       .option('--simple', __('create a barebones project meant only for contract development'))\n       .option('--locale [locale]', __('language to use (default: en)'))\n+      .option('--template [name/url]', __('download a template using a known name or a GitHub repository URL'))\n       .action(function(name, options) {\n         i18n.setOrDetectLocale(options.locale);\n         if (name === undefined) {", "review": "Suggested revision `download a template using a known name or a GitHub repository URL`", "dataset": "codereview", "original_id": 438, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n      .description(__('New Application'))\n      .option('--simple', __('create a barebones project meant only for contract development'))\n      .option('--locale [locale]', __('language to use (default: en)'))\n      .option('--template [name/url]', __('download a template using a known name or a GitHub repository URL'))\n      .action(function(name, options) {\n        i18n.setOrDetectLocale(options.locale);\n        if (name === undefined) {\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n .description(__('New Application')) \n .option('--simple', __('create a barebones project meant only for contract development')) \n .option('--locale [locale]', __('language to use (default: en)')) \n .option('--template [name/url]', __('download a template using a known name or a GitHub repository URL')) \n .action(function(name, options) { \n i18n.setOrDetectLocale(options.locale); \n if (name === undefined) {", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 227, "old": "     // bugs.\n     auto schedule_teardown = [&](const std::string& type_or_label) {\n       if (is_singleton(type_or_label)) {\n-        auto component = self->state.registry.remove(type_or_label);\n-        if (component) {\n           VAST_VERBOSE(\"{} schedules {} for shutdown\", self, type_or_label);\n           self->demonitor(component->actor);\n           scheduled_for_teardown.push_back(std::move(component->actor));", "new": "     // bugs.\n     auto schedule_teardown = [&](const std::string& type_or_label) {\n       if (is_singleton(type_or_label)) {\n+        if (auto component = self->state.registry.remove(type_or_label)) {\n           VAST_VERBOSE(\"{} schedules {} for shutdown\", self, type_or_label);\n           self->demonitor(component->actor);\n           scheduled_for_teardown.push_back(std::move(component->actor));", "review": "Nit: Exactly the same code is written as ``` auto component = self->state.registry.remove(type_or_label); if (component) { ``` here but ``` if (auto removed = self->state.registry.remove(component)) { ``` below", "dataset": "codereview", "original_id": 440, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\n    // bugs.\n    auto schedule_teardown = [&](const std::string& type_or_label) {\n      if (is_singleton(type_or_label)) {\n        if (auto component = self->state.registry.remove(type_or_label)) {\n          VAST_VERBOSE(\"{} schedules {} for shutdown\", self, type_or_label);\n          self->demonitor(component->actor);\n          scheduled_for_teardown.push_back(std::move(component->actor));\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n // bugs. \n auto schedule_teardown = [&](const std::string& type_or_label) { \n if (is_singleton(type_or_label)) { \n auto component = self->state.registry.remove(type_or_label); \n if (component) { \n VAST_VERBOSE(\"{} schedules {} for shutdown\", self, type_or_label); \n self->demonitor(component->actor); \n scheduled_for_teardown.push_back(std::move(component->actor));", "model_em_trim": 0, "model_bleu_trim": 94.7073708188097}
{"id": 228, "old": "         return $this;\n     }\n     public function loadTaxRuleWithoutProduct(TaxRule $taxRule, Country $country, State $state = null)\n     {\n         $this->product = null;", "new": "         return $this;\n     }\n+    /** @since 2.4 */\n     public function loadTaxRuleWithoutProduct(TaxRule $taxRule, Country $country, State $state = null)\n     {\n         $this->product = null;", "review": "Hello, Missing phpdoc `@since 2.4`", "dataset": "codereview", "original_id": 441, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-add", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-add", "gpt_code": "\n        return $this;\n    }\n\n    /**\n     * Loads a tax rule without a product.\n     *\n     * @since 2.4\n     *\n     * @param TaxRule $taxRule The tax rule to load.\n     * @param Country $country The country to load the tax rule for.\n     * @param State|null $state The state to load the tax rule for, or null if not applicable.\n     */\n    public function loadTaxRuleWithoutProduct(TaxRule $taxRule, Country $country, State $state = null)\n    {\n        $this->product = null;\n", "gpt_em_trim": 0, "gpt_bleu_trim": 50.44128, "model_code": "\n return $this; \n } \n /** \n * @since 2.4 \n */ \n public function loadTaxRuleWithoutProduct(TaxRule $taxRule, Country $country, State $state = null) \n { \n $this->product = null;", "model_em_trim": 0, "model_bleu_trim": 97.01619702903142, "error_type_gpt": "Unclear Location"}
{"id": 229, "old": " \tif ( $result = get_transient( 'rocket_sitemap_preload_complete' ) ) {\n \t\tdelete_transient( 'rocket_sitemap_preload_complete' ); ?>\n \t\t<div class=\"notice notice-success is-dismissible\">\n-\t\t\t<p><?php echo sprintf( __( 'Sitemap preload complete: %d pages not yet cached have been preloaded.', 'rocket' ), $result ); ?></p>\n \t\t</div>\n \t<?php\n \t}", "new": " \tif ( $result = get_transient( 'rocket_sitemap_preload_complete' ) ) {\n \t\tdelete_transient( 'rocket_sitemap_preload_complete' ); ?>\n \t\t<div class=\"notice notice-success is-dismissible\">\n+\t\t\t<p><?php printf( __( 'Sitemap preload complete: %d pages not yet cached have been preloaded.', 'rocket' ), $result ); ?></p>\n \t\t</div>\n \t<?php\n \t}", "review": "OK, I can overlook things about WPCS but not this x) ```echo sprintf();```", "dataset": "codereview", "original_id": 445, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nif ( $result = get_transient( 'rocket_sitemap_preload_complete' ) ) {\n\tdelete_transient( 'rocket_sitemap_preload_complete' );\n\t?>\n\t<div class=\"notice notice-success is-dismissible\">\n\t\t<p><?php printf( __( 'Sitemap preload complete: %d pages not yet cached have been preloaded.', 'rocket' ), $result ); ?></p>\n\t</div>\n\t<?php\n}\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n if ( $result = get_transient( 'rocket_sitemap_preload_complete' ) ) { \n delete_transient( 'rocket_sitemap_preload_complete' ); ?> \n <div class=\"notice notice-success is-dismissible\"> \n <p><?php echo sprintf( __( 'Sitemap preload complete: %d pages not yet cached have been preloaded.', 'rocket' ), $result ); ?></p> \n </div> \n <?php \n }", "model_em_trim": 0, "model_bleu_trim": 96.59532317701091}
{"id": 230, "old": "         try:\n             state = ext_handler.properties.state\n-            self.get_artifact_error_state.reset()\n             if self.last_etag == etag:\n                 if self.log_etag:\n                     ext_handler_i.logger.verbose(\"Incarnation {0} did not change, not processing GoalState\", etag)", "new": "         try:\n             state = ext_handler.properties.state\n             if self.last_etag == etag:\n                 if self.log_etag:\n                     ext_handler_i.logger.verbose(\"Incarnation {0} did not change, not processing GoalState\", etag)", "review": "the error state should be reset after the \"if decide_version\" (because download succeeded) now, since with this change we short-circuit the logic if there is not a new goal state, it seems to me that the error state is not needed and we should always report errors (since it is a new goal state) -- could you review the code to check if this is true?", "dataset": "codereview", "original_id": 456, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "concrete suggestion", "information_user2": "vague suggestion", "category_type": "Refactoring-swap", "category_type_user1": "Refactoring-swap", "category_type_user2": "Refactoring-swap", "gpt_code": "\n        try:\n            state = ext_handler.properties.state\n            if self.last_etag == etag:\n                if self.log_etag:\n                    ext_handler_i.logger.verbose(\"Incarnation {0} did not change, not processing GoalState\", etag)\n            else:\n                self.get_artifact_error_state.reset()\n                ext_handler_i.logger.verbose(\"New GoalState detected, processing GoalState\")\n                self.last_etag = etag\n                self.protocol.download_artifacts(state)\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n try: \n state = ext_handler.properties.state \n if self.last_etag == etag: \n if self.log_etag: \n ext_handler_i.logger.verbose(\"Incarnation {0} did not change, not processing GoalState\", etag)", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 231, "old": "  * This class was copied from Guava release 23.0 to replace the older Guava 14 version that had been used in Accumulo.\n  * It was annotated as Beta by Google, therefore unstable to use in a core Accumulo library.  We learned this the hard\n  * way when Guava version 20 deprecated the getHostText method and then removed the method all together in version 22.\n  *\n  * Unused methods and annotations were removed to reduce maintenance costs.\n  *", "new": "  * This class was copied from Guava release 23.0 to replace the older Guava 14 version that had been used in Accumulo.\n  * It was annotated as Beta by Google, therefore unstable to use in a core Accumulo library.  We learned this the hard\n  * way when Guava version 20 deprecated the getHostText method and then removed the method all together in version 22.\n+ * See ACCUMULO-4702\n  *\n  * Unused methods and annotations were removed to reduce maintenance costs.\n  *", "review": "could reference the issue number", "dataset": "codereview", "original_id": 467, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-add", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-add", "gpt_code": "\n * This class was copied from Guava release 23.0 to replace the older Guava 14 version that had been used in Accumulo.\n * It was annotated as Beta by Google, therefore unstable to use in a core Accumulo library.  We learned this the hard\n * way when Guava version 20 deprecated the getHostText method and then removed the method all together in version 22.\n *\n * Unused methods and annotations were removed to reduce maintenance costs.\n *\n * Issue #1234: Updated Guava version to 23.0 to avoid deprecation issues with getHostText method.\n", "gpt_em_trim": 0, "gpt_bleu_trim": 2.2e-88, "model_code": "\n * This class was copied from Guava release 23.0 to replace the older Guava 14 version that had been used in Accumulo. \n * It was annotated as Beta by Google, therefore unstable to use in a core Accumulo library.  We learned this the hard \n * way when Guava version 20 deprecated the getHostText method and then removed the method all together in version 22. \n * \n * Unused methods and annotations were removed to reduce maintenance costs. \n *", "model_em_trim": 0, "model_bleu_trim": 93.4235227684881, "error_type_gpt": "Need Domain Knowledge"}
{"id": 232, "old": "             var renderedSource = this.Source != null ? this.Source.Render(logEvent) : null;\n             var isCacheUpToDate = eventLogInstance != null && renderedSource == eventLogInstance.Source &&\n                                    eventLogInstance.Log == this.Log && eventLogInstance.MachineName == this.MachineName;\n-            eventLogInstance.MaximumKilobytes = Convert.ToInt64(this.MaxKilobytes.Render(logEvent));\n             if (!isCacheUpToDate)\n             {", "new": "             var renderedSource = this.Source != null ? this.Source.Render(logEvent) : null;\n             var isCacheUpToDate = eventLogInstance != null && renderedSource == eventLogInstance.Source &&\n                                    eventLogInstance.Log == this.Log && eventLogInstance.MachineName == this.MachineName;\n+            eventLogInstance.MaximumKilobytes = this.MaxKilobytes;\n             if (!isCacheUpToDate)\n             {", "review": "This is problematic when the value is empty/0 ;)", "dataset": "codereview", "original_id": 471, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n            var renderedSource = this.Source?.Render(logEvent);\n            var isCacheUpToDate = eventLogInstance != null && renderedSource == eventLogInstance.Source &&\n                                   eventLogInstance.Log == this.Log && eventLogInstance.MachineName == this.MachineName;\n            var maxKilobytes = this.MaxKilobytes?.Render(logEvent);\n            eventLogInstance.MaximumKilobytes = maxKilobytes != null ? Convert.ToInt64(maxKilobytes) : 0;\n            if (!isCacheUpToDate)\n            {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 62.53197, "model_code": "\n var renderedSource = this.Source != null ? this.Source.Render(logEvent) : null; \n var isCacheUpToDate = eventLogInstance != null && renderedSource == eventLogInstance.Source && \n eventLogInstance.Log == this.Log && eventLogInstance.MachineName == this.MachineName; \n if (!isCacheUpToDate) \n {", "model_em_trim": 0, "model_bleu_trim": 87.16221743324077, "error_type_gpt": "Need Domain Knowledge"}
{"id": 233, "old": " \t$widget_key = $wp_widget_factory->get_widget_key( $attributes['idBase'] );\n-\tif ( ! $widget_key ) {\n \t\treturn '';\n \t}", "new": " \t$widget_key = $wp_widget_factory->get_widget_key( $attributes['idBase'] );\n+\tif ( '' === $widget_key ) {\n \t\treturn '';\n \t}", "review": "```suggestion if ( '' === $widget_key ) { ``` Instead of an implicit loose comparison, I'd suggest testing specifically against the value of empty string. Why? - avoids type juggle in PHP - more readable and understandable by saying: if an empty string, bail out - future proof, especially giving how PHP is moving towards strict types Why empty string? The `get_widget_key` method returns a string. If the key doesn't exist, it returns an empty string.", "dataset": "codereview", "original_id": 499, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\n\t$widget_key = $wp_widget_factory->get_widget_key( $attributes['idBase'] );\n\tif ( '' === $widget_key ) {\n\t\treturn '';\n\t}\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n $widget_key = $wp_widget_factory->get_widget_key( $attributes['idBase'] ); \n if ( '' === $widget_key ) { \n return ''; \n }", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 234, "old": "           return item.action !== 'insert-page';\n         });\n       }\n-\n-      if (!args.edit && !args.contextMenu && self.apos.permissions.can(req, 'view-page')) {\n-        args.contextMenu = _.filter(self.options.contextMenu, function(item) {\n-          return item.action === 'copy-page'\n-        });\n-      }\n     }\n     if (args.contextMenu) {", "new": "           return item.action !== 'insert-page';\n         });\n       }\n     }\n     if (args.contextMenu) {", "review": "Is this about allowing the user to start from the page they want to copy? The idea is that you still start from the page you want to be the new parent, but there's a join inside the page settings modal which defaults to that page but can be used to pick a different source to copy from.", "dataset": "codereview", "original_id": 503, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n      if (!args.edit && !args.contextMenu && self.apos.permissions.can(req, 'view-page')) {\n        args.contextMenu = _.filter(self.options.contextMenu, function(item) {\n          return item.action === 'copy-page'\n        });\n        if (args.page) {\n          args.contextMenu.push({\n            action: 'copy-page',\n            label: 'Copy Page',\n            icon: 'copy',\n            pageId: args.page._id\n          });\n        }\n      }\n    }\n    if (args.contextMenu) {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 21.93893, "model_code": "\n return item.action !== 'insert-page'; \n }); \n } \n } \n if (args.contextMenu) {", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Need Domain Knowledge"}
{"id": 235, "old": "           opts.on '-r RUNNER', '--runner RUNNER',\n                   'Specify which test runner to use',\n-                  'supported runners: beaker, minitest' do |value|\n             @cmd_options[:runner] = value\n           end", "new": "           opts.on '-r RUNNER', '--runner RUNNER',\n                   'Specify which test runner to use',\n+                  'supported runners: beaker' do |value|\n             @cmd_options[:runner] = value\n           end", "review": "The minitest runner itself doesn't look like it runs at this point, is that true? I'm fine with that being the case, but I'd like to update this line that says it can until it's true.", "dataset": "codereview", "original_id": 517, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n          opts.on '-r RUNNER', '--runner RUNNER',\n                  'Specify which test runner to use',\n                  'supported runners: beaker' do |value|\n            @cmd_options[:runner] = value\n          end\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n opts.on '-r RUNNER', '--runner RUNNER', \n 'Specify which test runner to use', \n 'supported runners: beaker, minitest' do |value| \n @cmd_options[:runner] = value \n end", "model_em_trim": 0, "model_bleu_trim": 92.51691088144084}
{"id": 236, "old": "         foreach ($productsForRecalculations as $productForRecalculations) {\n             $this->productPriceRecalculationScheduler->scheduleProductForImmediateRecalculation($productForRecalculations);\n             $productForRecalculations->markForVisibilityRecalculation();\n-            $this->productChangeMessageProducer->productChanged($productForRecalculations);\n             $this->productAvailabilityRecalculationScheduler->scheduleProductForImmediateRecalculation($productForRecalculations);\n         }", "new": "         foreach ($productsForRecalculations as $productForRecalculations) {\n             $this->productPriceRecalculationScheduler->scheduleProductForImmediateRecalculation($productForRecalculations);\n             $productForRecalculations->markForVisibilityRecalculation();\n             $this->productAvailabilityRecalculationScheduler->scheduleProductForImmediateRecalculation($productForRecalculations);\n         }", "review": "Is this really necessary? It seems that the logic of *\"export all products linked via variant\"* is duplicated here. I'm not sure which class should be responsible for it...", "dataset": "codereview", "original_id": 518, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nforeach ($productsForRecalculations as $productForRecalculations) {\n    $this->productPriceRecalculationScheduler->scheduleProductForImmediateRecalculation($productForRecalculations);\n    $productForRecalculations->markForVisibilityRecalculation();\n    $this->productChangeMessageProducer->productChanged($productForRecalculations);\n    $this->productAvailabilityRecalculationScheduler->scheduleProductForImmediateRecalculation($productForRecalculations);\n\n    // Export all products linked via variant\n    if ($productForRecalculations->hasVariants()) {\n        foreach ($productForRecalculations->getVariants() as $variant) {\n            $this->productPriceRecalculationScheduler->scheduleProductForImmediateRecalculation($variant);\n            $variant->markForVisibilityRecalculation();\n            $this->productChangeMessageProducer->productChanged($variant);\n            $this->productAvailabilityRecalculationScheduler->scheduleProductForImmediateRecalculation($variant);\n        }\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 60.09402, "model_code": "\n foreach ($productsForRecalculations as $productForRecalculations) { \n $this->productPriceRecalculationScheduler->scheduleProductForImmediateRecalculation($productForRecalculations); \n $productForRecalculations->markForVisibilityRecalculation(); \n $this->productAvailabilityRecalculationScheduler->scheduleProductForImmediateRecalculation($productForRecalculations); \n }", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 237, "old": "             yield {\n                 'resource_id': violation.resource_id,\n                 'resource_type': violation.resource_type,\n-                'resource_name': violation.resource_name,\n                 'full_name': violation.full_name,\n                 'rule_index': violation.rule_index,\n                 'rule_name': violation.rule_name,", "new": "             yield {\n                 'resource_id': violation.resource_id,\n                 'resource_type': violation.resource_type,\n+                'resource_name': violation.resource_id,\n                 'full_name': violation.full_name,\n                 'rule_index': violation.rule_index,\n                 'rule_name': violation.rule_name,", "review": "Is it possible to have any other values for parent than kms_keyring?", "dataset": "codereview", "original_id": 552, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-modify", "gpt_code": "\n            yield {\n                'resource_id': violation.resource_id,\n                'resource_type': violation.resource_type,\n                'resource_name': violation.resource_name,\n                'full_name': violation.full_name,\n                'rule_index': violation.rule_index,\n                'rule_name': violation.rule_name,\n                'parent': violation.parent if violation.parent == 'kms_keyring' else None\n            }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 96.55747, "model_code": "\n yield { \n 'resource_id': violation.resource_id, \n 'resource_type': violation.resource_type, \n 'full_name': violation.full_name, \n 'rule_index': violation.rule_index, \n 'rule_name': violation.rule_name,", "model_em_trim": 0, "model_bleu_trim": 82.30996781253701}
{"id": 238, "old": "         if (!compress) {\n             return encode();\n         }\n-\n         ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n         DeflaterOutputStream outputStream = new DeflaterOutputStream(byteArrayOutputStream, new Deflater(Deflater.BEST_COMPRESSION));\n         DataVersion dataVersion = topicConfigSerializeWrapper.getDataVersion();", "new": "         if (!compress) {\n             return encode();\n         }\n+        long start = System.currentTimeMillis();\n         ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n         DeflaterOutputStream outputStream = new DeflaterOutputStream(byteArrayOutputStream, new Deflater(Deflater.BEST_COMPRESSION));\n         DataVersion dataVersion = topicConfigSerializeWrapper.getDataVersion();", "review": "I think `Deflater.BEST_COMPRESSION` may be even slower than GZIP. Do we care about compression speed?", "dataset": "codereview", "original_id": 556, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\n        if (!compress) {\n            return encode();\n        }\n\n        ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n        Deflater deflater = new Deflater();\n        deflater.setLevel(Deflater.DEFAULT_COMPRESSION);\n        DeflaterOutputStream outputStream = new DeflaterOutputStream(byteArrayOutputStream, deflater);\n        DataVersion dataVersion = topicConfigSerializeWrapper.getDataVersion();\n", "gpt_em_trim": 0, "gpt_bleu_trim": 75.99817, "model_code": "\n if (!compress) { \n return encode(); \n } \n ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); \n DeflaterOutputStream outputStream = new DeflaterOutputStream(byteArrayOutputStream, new Deflater(Deflater.BEST_COMPRESSION)); \n DataVersion dataVersion = topicConfigSerializeWrapper.getDataVersion();", "model_em_trim": 0, "model_bleu_trim": 80.5196912342092}
{"id": 239, "old": " \t\t\ttestRoutineComplete <- struct{}{}\n \t\t}()\n \t\tvar errw error\n-\t\tfor i := int64(1); i < 10000 && errw == nil; i++ {\n \t\t\terrw = acc.Atomic(func(tx *sql.Tx) error {\n \t\t\t\t_, err := tx.Exec(\"INSERT INTO t (a) VALUES (?)\", i)\n \t\t\t\treturn err", "new": " \t\t\ttestRoutineComplete <- struct{}{}\n \t\t}()\n \t\tvar errw error\n+\t\tfor i, timedLoop := int64(1), true; timedLoop && errw == nil; i++ {\n \t\t\terrw = acc.Atomic(func(tx *sql.Tx) error {\n \t\t\t\t_, err := tx.Exec(\"INSERT INTO t (a) VALUES (?)\", i)\n \t\t\t\treturn err", "review": "Now that you have `targetTestDurationTimer`, maybe it's reasonable to get rid of the `10000` constant altogether?", "dataset": "codereview", "original_id": 564, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n\t\t\ttestRoutineComplete <- struct{}{}\n\t\t}()\n\t\tvar errw error\n\t\ttargetDuration := time.Second * 10 // or whatever the target duration is\n\t\tstartTime := time.Now()\n\t\tfor i := int64(1); time.Since(startTime) < targetDuration && errw == nil; i++ {\n\t\t\terrw = acc.Atomic(func(tx *sql.Tx) error {\n\t\t\t\t_, err := tx.Exec(\"INSERT INTO t (a) VALUES (?)\", i)\n\t\t\t\treturn err\n\t\t\t})\n\t\t}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 65.38947, "model_code": "\n testRoutineComplete <- struct{}{} \n }() \n var errw error \n for i := int64(1); i < targetTestDurationTimer.C; i++ { \n errw = acc.Atomic(func(tx *sql.Tx) error { \n _, err := tx.Exec(\"INSERT INTO t (a) VALUES (?)\", i) \n return err", "model_em_trim": 0, "model_bleu_trim": 81.41697826507162}
{"id": 240, "old": "             if(currentProfile == null)\n                 return;\n             if(currentProfile.getUnits().equals(Constants.MMOL))\n-                tt = prefTT > 0  ? Profile.toMgdl(prefTT, Constants.MGDL) : 80d;\n             else\n                 tt = prefTT > 0  ? prefTT : 80d;\n             final double finalTT = tt;", "new": "             if(currentProfile == null)\n                 return;\n             if(currentProfile.getUnits().equals(Constants.MMOL))\n+                tt = prefTT > 0  ? Profile.toMgdl(prefTT, Constants.MMOL) : 80d;\n             else\n                 tt = prefTT > 0  ? prefTT : 80d;\n             final double finalTT = tt;", "review": "This must be `tt = prefTT > 0 ? Profile.toMgdl(prefTT, Constants.MMOL) : 80d;` as we are in the MMOL branch. It should be possible to reduce the whole if-else construct by something like `tt = prefTT > 0 ? Profile.toMgdl(prefTT, currentProfile.getUnits()) : 80d;`", "dataset": "codereview", "original_id": 581, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nif(currentProfile == null)\n    return;\ntt = prefTT > 0 ? Profile.toMgdl(prefTT, currentProfile.getUnits()) : 80d;\nfinal double finalTT = tt;\n", "gpt_em_trim": 0, "gpt_bleu_trim": 45.52892, "model_code": "\n if(currentProfile == null) \n return; \n tt = prefTT > 0  ? Profile.toMgdl(prefTT, Constants.MMOL) : 80d; \n final double finalTT = tt;", "model_em_trim": 0, "model_bleu_trim": 45.35904905913151}
{"id": 241, "old": " \t\t\t\treturn MPEGPS;\n \t\t\t}\n-\t\t\tif (supportSpec.match(MPEGTS, MPEG2, AC3) || supportSpec.match(MPEGTS, MPEG2, AAC)) {\n \t\t\t\treturn MPEGTS;\n \t\t\t}", "new": " \t\t\t\treturn MPEGPS;\n \t\t\t}\n+\t\t\tif (supportSpec.match(MPEGTS, MPEG2, AC3) || supportSpec.match(MPEGTS, H264, AAC)) {\n \t\t\t\treturn MPEGTS;\n \t\t\t}", "review": "Wouldn't it be (MPEGTS, H264, AAC)?", "dataset": "codereview", "original_id": 587, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n\t\t\t\treturn MPEGPS;\n\t\t\t}\n\t\t\tif (supportSpec.match(MPEGTS, H264, AAC)) {\n\t\t\t\treturn MPEGTS;\n\t\t\t}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 59.34875, "model_code": "\n return MPEGPS; \n } \n if (supportSpec.match(MPEGTS, MPEG2, AC3) || supportSpec.match(MPEGTS, MPEG2, AAC)) { \n return MPEGTS; \n }", "model_em_trim": 0, "model_bleu_trim": 92.327348399401, "error_type_gpt": "Unclear Location"}
{"id": 242, "old": " \t\t\t\t\t\t\t\t<span class=\"name\">{{html .Name}}</span>\n \t\t\t\t\t\t\t</a>\n \t\t\t\t\t\t</td>\n-\t\t\t\t\t\t{{- if .IsSymlink }}\n-\t\t\t\t\t\t<td data-order=\"-1\">symbolic link</td>\n-\t\t\t\t\t\t{{- else if .IsDir}}\n \t\t\t\t\t\t<td data-order=\"-1\">&mdash;</td>\n \t\t\t\t\t\t{{- else}}\n \t\t\t\t\t\t<td data-order=\"{{.Size}}\">{{.HumanSize}}</td>", "new": " \t\t\t\t\t\t\t\t<span class=\"name\">{{html .Name}}</span>\n \t\t\t\t\t\t\t</a>\n \t\t\t\t\t\t</td>\n+\t\t\t\t\t\t{{- if .IsDir}}\n \t\t\t\t\t\t<td data-order=\"-1\">&mdash;</td>\n \t\t\t\t\t\t{{- else}}\n \t\t\t\t\t\t<td data-order=\"{{.Size}}\">{{.HumanSize}}</td>", "review": "Instead of showing the words \"symbolic link\" under the \"Size\" column, how about we introduce new icons for symbolic link to file and symbolic link to directory?", "dataset": "codereview", "original_id": 592, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "concrete suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Feature-modify", "gpt_code": "\n\t\t\t\t\t\t\t\t<span class=\"name\">{{html .Name}}</span>\n\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t</td>\n\t\t\t\t\t\t{{- if .IsSymlink }}\n\t\t\t\t\t\t<td data-order=\"-1\"><i class=\"fa fa-link\"></i></td>\n\t\t\t\t\t\t{{- else if .IsDir}}\n\t\t\t\t\t\t<td data-order=\"-1\"><i class=\"fa fa-folder\"></i></td>\n\t\t\t\t\t\t{{- else}}\n\t\t\t\t\t\t<td data-order=\"{{.Size}}\">{{.HumanSize}}</td>\n", "gpt_em_trim": 0, "gpt_bleu_trim": 58.44895, "model_code": "\n <span class=\"name\">{{html .Name}}</span> \n </a> \n </td> \n {{- if .IsSymlink }} \n <td data-order=\"-1\">Symlink</td> \n {{- else if .IsDir}} \n <td data-order=\"-1\">&mdash;</td> \n {{- else}} \n <td data-order=\"{{.Size}}\">{{.HumanSize}}</td>", "model_em_trim": 0, "model_bleu_trim": 76.66820379207182}
{"id": 243, "old": " from bzt.modules.siege import SiegeExecutor, DataLogReader\n from tests import BZTestCase\n from tests.mocks import EngineEmul\n-from bzt.utils import is_windows\n-def tool_name():\n-    if is_windows():\n-        return 'siege.bat'\n-    else:\n-        return 'siege.sh'\n def get_res_path(resource):", "new": " from bzt.modules.siege import SiegeExecutor, DataLogReader\n from tests import BZTestCase\n from tests.mocks import EngineEmul\n+from bzt.utils import EXE_SUFFIX\n+TOOL_NAME = 'siege' + EXE_SUFFIX\n def get_res_path(resource):", "review": "Look how it's done everywhere with utils.EXE_SUFFIX constant.", "dataset": "codereview", "original_id": 594, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nfrom bzt.modules.siege import SiegeExecutor, DataLogReader\nfrom tests import BZTestCase\nfrom tests.mocks import EngineEmul\nfrom bzt.utils import is_windows, EXE_SUFFIX\n\ndef tool_name():\n    return 'siege' + EXE_SUFFIX\n\ndef get_res_path(resource):\n", "gpt_em_trim": 0, "gpt_bleu_trim": 75.64578, "model_code": "\n from bzt.modules.siege import SiegeExecutor, DataLogReader \n from tests import BZTestCase \n from tests.mocks import EngineEmul \n def tool_name(): \n return 'siege.bat' \n def get_res_path(resource):", "model_em_trim": 0, "model_bleu_trim": 62.32294198413504}
{"id": 244, "old": "     @Override\n     public void onClick(View view) {\n       if (eventListener != null && batchSelected.isEmpty() && messageRecord.isMms() && !((MmsMessageRecord) messageRecord).getSharedContacts().isEmpty()) {\n-        eventListener.onSharedContactDetailsClicked(((SharedContactView) view).getContact(), ((SharedContactView) view).getAvatarView());\n       } else {\n         passthroughClickListener.onClick(view);\n       }", "new": "     @Override\n     public void onClick(View view) {\n       if (eventListener != null && batchSelected.isEmpty() && messageRecord.isMms() && !((MmsMessageRecord) messageRecord).getSharedContacts().isEmpty()) {\n+        eventListener.onSharedContactDetailsClicked(((MmsMessageRecord) messageRecord).getSharedContacts().get(0), sharedContactStub.get().getAvatarView());\n       } else {\n         passthroughClickListener.onClick(view);\n       }", "review": "Why did this need to change?", "dataset": "codereview", "original_id": 619, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "no code", "gpt_em_trim": 0, "gpt_bleu_trim": 1.13e-88, "model_code": "\n @Override \n public void onClick(View view) { \n if (eventListener != null && batchSelected.isEmpty() && messageRecord.isMms() && !((MmsMessageRecord) messageRecord).getSharedContacts().isEmpty()) { \n eventListener.onSharedContactDetailsClicked(((SharedContactView) view).getContact(), ((SharedContactView) view).getAvatarView()); \n } else { \n passthroughClickListener.onClick(view); \n }", "model_em_trim": 0, "model_bleu_trim": 81.49686229247067, "error_type_gpt": "Need Domain Knowledge"}
{"id": 245, "old": " add_action( 'wp_head', 'wp_shortlink_wp_head', 10, 0 );\n add_action( 'wp_head', 'wp_custom_css_cb', 101 );\n add_action( 'wp_head', 'wp_site_icon', 99 );\n-add_action( 'wp_footer', 'wp_enqueue_global_styles', 1 );\n add_action( 'wp_footer', 'wp_print_footer_scripts', 20 );\n add_action( 'template_redirect', 'wp_shortlink_header', 11, 0 );\n add_action( 'wp_print_footer_scripts', '_wp_footer_scripts' );", "new": " add_action( 'wp_head', 'wp_shortlink_wp_head', 10, 0 );\n add_action( 'wp_head', 'wp_custom_css_cb', 101 );\n add_action( 'wp_head', 'wp_site_icon', 99 );\n add_action( 'wp_footer', 'wp_print_footer_scripts', 20 );\n add_action( 'template_redirect', 'wp_shortlink_header', 11, 0 );\n add_action( 'wp_print_footer_scripts', '_wp_footer_scripts' );", "review": "Can we move this next to the other action hook for `wp_enqueue_global_styles` so it's obvious that there're two of them?", "dataset": "codereview", "original_id": 667, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nadd_action( 'wp_head', 'wp_shortlink_wp_head', 10, 0 );\nadd_action( 'wp_head', 'wp_custom_css_cb', 101 );\nadd_action( 'wp_head', 'wp_site_icon', 99 );\nadd_action( 'wp_footer', 'wp_enqueue_global_styles', 1 );\nadd_action( 'wp_footer', 'wp_print_footer_scripts', 20 );\nadd_action( 'wp_footer', '_wp_footer_scripts' );\nadd_action( 'template_redirect', 'wp_shortlink_header', 11, 0 );\n", "gpt_em_trim": 0, "gpt_bleu_trim": 84.2349, "model_code": "\n add_action( 'wp_head', 'wp_shortlink_wp_head', 10, 0 ); \n add_action( 'wp_head', 'wp_custom_css_cb', 101 ); \n add_action( 'wp_head', 'wp_site_icon', 99 ); \n add_action( 'wp_footer', 'wp_print_footer_scripts', 20 ); \n add_action( 'template_redirect', 'wp_shortlink_header', 11, 0 ); \n add_action( 'wp_print_footer_scripts', '_wp_footer_scripts' );", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 246, "old": " \t\t}\n \t}()\n-\t_, err := client.ReadWriteTransaction(ctx, func(ctx context.Context, tx *ReadWriteTransaction) error {\n-\t\treturn nil\n-\t})\n-\tif err != nil {\n-\t\tt.Fatalf(\"Unexpected error during transaction: %v\", err)\n \t}\n }", "new": " \t\t}\n \t}()\n+\tif g, w := client.idleSessions.idleList.Len(), 1; g != w {\n+\t\tt.Fatalf(\"idle session count mismatch.\\nGot: %v\\nWant: %v\", g, w)\n \t}\n }", "review": "The current test case will hang indefinitely if a session leak would be re-introduced by accident in the future. The below suggestion would cause the test to fail fast instead. ```suggestion if g, w := client.idleSessions.idleList.Len(), 1; g != w { t.Fatalf(\"idle session count mismatch.\\nGot: %v\\nWant: %v\", g, w) } ```", "dataset": "codereview", "original_id": 705, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n}()\nif g, w := client.idleSessions.idleList.Len(), 1; g != w {\n    t.Fatalf(\"idle session count mismatch.\\nGot: %v\\nWant: %v\", g, w)\n}\n_, err := client.ReadWriteTransaction(ctx, func(ctx context.Context, tx *ReadWriteTransaction) error {\n    return nil\n})\nif err != nil {\n    t.Fatalf(\"Unexpected error during transaction: %v\", err)\n}\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 89.13766, "model_code": "\n } \n }() \n if g, w := client.idleSessions.idleList.Len(), 1; g != w { \n t.Fatalf(\"idle session count mismatch.\\nGot: %v\\nWant: %v\", g, w) \n } \n }", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Insignificant Omission"}
{"id": 247, "old": "  * @return {Promise<void>}\n  */\n export function act(cb) {\n-\t++actDepth;\n-\tif (actDepth > 1) {\n \t\t// If calls to `act` are nested, a flush happens only when the\n \t\t// outermost call returns. In the inner call, we just execute the\n \t\t// callback and return since the infrastructure for flushing has already", "new": "  * @return {Promise<void>}\n  */\n export function act(cb) {\n+\tif (++actDepth > 1) {\n \t\t// If calls to `act` are nested, a flush happens only when the\n \t\t// outermost call returns. In the inner call, we just execute the\n \t\t// callback and return since the infrastructure for flushing has already", "review": "Super super tiny nit: We can combine this line and the above one: ```js if (++actDepth > 1) { ... } ```", "dataset": "codereview", "original_id": 719, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\n * @return {Promise<void>}\n */\nexport function act(cb) {\n\tif (++actDepth > 1) {\n\t\t// If calls to `act` are nested, a flush happens only when the\n\t\t// outermost call returns. In the inner call, we just execute the\n\t\t// callback and return since the infrastructure for flushing has already\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n * @return {Promise<void>} \n */ \n export function act(cb) { \n if (++actDepth > 1) { \n // If calls to `act` are nested, a flush happens only when the \n // outermost call returns. In the inner call, we just execute the \n // callback and return since the infrastructure for flushing has already", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 248, "old": "   };\n });\n-console.log(rewards);\n-\n export const StakingRewardsStory = () => (\n   <StakingRewards\n     rewards={rewards}", "new": "   };\n });\n export const StakingRewardsStory = () => (\n   <StakingRewards\n     rewards={rewards}", "review": "Please remove it.", "dataset": "codereview", "original_id": 741, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Refactoring-conventions", "gpt_code": "\nexport const StakingRewardsStory = () => (\n  <StakingRewards\n    rewards={[]}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 66.38177, "model_code": "\n }; \n }); \n export const StakingRewardsStory = () => ( \n <StakingRewards \n rewards={rewards}", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 249, "old": " \t\t\t\tset\n \t\t\t\t\tlast_status = 'failed',\n \t\t\t\t\tlast_status_at = now(),\n-\t\t\t\t\tstatus_details = 'failed: contact method disabled',\n \t\t\t\t\tcycle_id = null,\n \t\t\t\t\tnext_retry_at = null\n \t\t\t\tfrom user_contact_methods cm", "new": " \t\t\t\tset\n \t\t\t\t\tlast_status = 'failed',\n \t\t\t\t\tlast_status_at = now(),\n+\t\t\t\t\tstatus_details = 'contact method disabled',\n \t\t\t\t\tcycle_id = null,\n \t\t\t\t\tnext_retry_at = null\n \t\t\t\tfrom user_contact_methods cm", "review": "We'll need to add the `user_id` here so we can use it for the log context.", "dataset": "codereview", "original_id": 754, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n\t\t\t\tset\n\t\t\t\t\tlast_status = 'failed',\n\t\t\t\t\tlast_status_at = now(),\n\t\t\t\t\tstatus_details = 'failed: contact method disabled',\n\t\t\t\t\tcycle_id = null,\n\t\t\t\t\tnext_retry_at = null,\n\t\t\t\t\tuser_id = cm.user_id\n\t\t\t\tfrom user_contact_methods cm\n", "gpt_em_trim": 0, "gpt_bleu_trim": 74.94055, "model_code": "\n set \n last_status = 'failed', \n last_status_at = now(), \n status_details = 'failed: contact method disabled', \n cycle_id = null, \n next_retry_at = null \n from user_contact_methods cm", "model_em_trim": 0, "model_bleu_trim": 92.96839796684331}
{"id": 250, "old": "       with open(QemuProcess.LOG_PATH) as f:\n         # Strip non-printable characters at beginning of qemu log\n         qemu_log = ''.join(c for c in f.read() if c in string.printable)\n-        # Only report the tail of the log; otherwise we would only end up seeing\n-        # the beginning of it once the logging library later truncates it to the\n-        # STACKDRIVER_LOG_MESSAGE_LIMIT.\n-        logs.log_warn(qemu_log[-64 * 1024:])\n     else:\n       logs.log_error('Qemu log not found in {}'.format(QemuProcess.LOG_PATH))", "new": "       with open(QemuProcess.LOG_PATH) as f:\n         # Strip non-printable characters at beginning of qemu log\n         qemu_log = ''.join(c for c in f.read() if c in string.printable)\n+        logs.log_warn(qemu_log[-undercoat.QEMU_LOG_LIMIT:])\n     else:\n       logs.log_error('Qemu log not found in {}'.format(QemuProcess.LOG_PATH))", "review": "nit: put `64 * 1024` in a constant in a suitable platforms/fuchsia/ module and use in both places.", "dataset": "codereview", "original_id": 762, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\nQEMU_LOG_TAIL_LIMIT = 64 * 1024\n\nwith open(QemuProcess.LOG_PATH) as f:\n    # Strip non-printable characters at beginning of qemu log\n    qemu_log = ''.join(c for c in f.read() if c in string.printable)\n    # Only report the tail of the log; otherwise we would only end up seeing\n    # the beginning of it once the logging library later truncates it to the\n    # STACKDRIVER_LOG_MESSAGE_LIMIT.\n    logs.log_warn(qemu_log[-QEMU_LOG_TAIL_LIMIT:])\nelse:\n    logs.log_error('Qemu log not found in {}'.format(QemuProcess.LOG_PATH))\n", "gpt_em_trim": 0, "gpt_bleu_trim": 65.82083, "model_code": "\n with open(QemuProcess.LOG_PATH) as f: \n # Strip non-printable characters at beginning of qemu log \n qemu_log = ''.join(c for c in f.read() if c in string.printable) \n logs.log_warn(qemu_log[-64 * 1024:]) \n else: \n logs.log_error('Qemu log not found in {}'.format(QemuProcess.LOG_PATH))", "model_em_trim": 0, "model_bleu_trim": 91.04757484423732}
{"id": 251, "old": " import org.apache.iceberg.io.CloseableIterable;\n public class FindFiles {\n   private static final DateTimeFormatter DATE_FORMAT = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\");\n   public static Builder in(Table table) {", "new": " import org.apache.iceberg.io.CloseableIterable;\n public class FindFiles {\n+  private FindFiles() {\n+  }\n+\n   private static final DateTimeFormatter DATE_FORMAT = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\");\n   public static Builder in(Table table) {", "review": "I assume `snapshotLog()` is in ascending order. Should we then `break` after the condition is not true anymore?", "dataset": "codereview", "original_id": 768, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "vague suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\nimport org.apache.iceberg.io.CloseableIterable;\n\npublic class FindFiles {\n  private static final DateTimeFormatter DATE_FORMAT = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\");\n\n  public static Builder in(Table table) {\n    CloseableIterable<DataFile> snapshotLog = table.snapshotLog();\n    Builder builder = new Builder(table);\n\n    try {\n      for (DataFile file : snapshotLog) {\n        if (file.timestampMillis() <= builder.asOfTimestampMillis) {\n          builder.addInputFile(file.path().toString());\n        } else {\n          break; // exit loop once we've passed the asOfTimestampMillis\n        }\n      }\n    } finally {\n      snapshotLog.close();\n    }\n\n    return builder;\n  }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 55.41283, "model_code": "\n import org.apache.iceberg.io.CloseableIterable; \n public class FindFiles implements Closeable { \n private static final DateTimeFormatter DATE_FORMAT = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\"); \n public static Builder in(Table table) {", "model_em_trim": 0, "model_bleu_trim": 85.90954338707324}
{"id": 252, "old": " \tcase types.UUIDKind:\n \t\treturn UuidType\n \tcase types.DecimalKind:\n-\t\treturn DecimalDefaultType\n \tdefault:\n \t\tpanic(fmt.Errorf(`no default type info for NomsKind \"%v\"`, kind.String()))\n \t}", "new": " \tcase types.UUIDKind:\n \t\treturn UuidType\n \tcase types.DecimalKind:\n+\t\treturn &decimalType{sql.MustCreateDecimalType(65, 30)}\n \tdefault:\n \t\tpanic(fmt.Errorf(`no default type info for NomsKind \"%v\"`, kind.String()))\n \t}", "review": "I'd return `&decimalType{sql.MustCreateDecimalType(65, 30)}` here as it's far wider than `10,5`", "dataset": "codereview", "original_id": 773, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n\tcase types.UUIDKind:\n\t\treturn UuidType\n\tcase types.DecimalKind:\n\t\treturn &decimalType{sql.MustCreateDecimalType(65, 30)}\n\tdefault:\n\t\tpanic(fmt.Errorf(`no default type info for NomsKind \"%v\"`, kind.String()))\n\t}\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n case types.UUIDKind: \n return UuidType \n case types.DecimalKind: \n return &decimalType{sql.MustCreateDecimalType(65, 30)} \n default: \n panic(fmt.Errorf(`no default type info for NomsKind \"%v\"`, kind.String())) \n }", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 253, "old": "     final MutableAccount account = updater.createAccount(ADDRESS);\n     account.setBalance(Wei.of(100000));\n     account.setCode(BytesValue.of(1, 2, 3));\n     account.setCode(BytesValue.of(3, 2, 1));\n     updater.commit();\n     assertEquals(BytesValue.of(3, 2, 1), worldState.get(ADDRESS).getCode());\n     assertEquals(\n         Hash.fromHexString(\"0xc14f5e30581de9155ea092affa665fad83bcd9f98e45c4a42885b9b36d939702\"),\n         worldState.rootHash());", "new": "     final MutableAccount account = updater.createAccount(ADDRESS);\n     account.setBalance(Wei.of(100000));\n     account.setCode(BytesValue.of(1, 2, 3));\n+    account.setVersion(Account.DEFAULT_VERSION);\n     account.setCode(BytesValue.of(3, 2, 1));\n     updater.commit();\n     assertEquals(BytesValue.of(3, 2, 1), worldState.get(ADDRESS).getCode());\n+    assertEquals(Account.DEFAULT_VERSION, worldState.get(ADDRESS).getVersion());\n     assertEquals(\n         Hash.fromHexString(\"0xc14f5e30581de9155ea092affa665fad83bcd9f98e45c4a42885b9b36d939702\"),\n         worldState.rootHash());", "review": "(Discussion) The version is not set in the test, should we set explicitly the version and check it ?", "dataset": "codereview", "original_id": 788, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\n    final MutableAccount account = updater.createAccount(ADDRESS);\n    account.setBalance(Wei.of(100000));\n    account.setCode(BytesValue.of(1, 2, 3));\n    account.setCode(BytesValue.of(3, 2, 1));\n    account.setVersion(1);\n    updater.commit();\n    assertEquals(BytesValue.of(3, 2, 1), worldState.get(ADDRESS).getCode());\n    assertEquals(\n        Hash.fromHexString(\"0xc14f5e30581de9155ea092affa665fad83bcd9f98e45c4a42885b9b36d939702\"),\n        worldState.rootHash());\n    assertEquals(1, worldState.get(ADDRESS).getVersion());\n", "gpt_em_trim": 0, "gpt_bleu_trim": 90.00118, "model_code": "\n final MutableAccount account = updater.createAccount(ADDRESS); \n account.setBalance(Wei.of(100000)); \n account.setCode(BytesValue.of(1, 2, 3)); \n updater.commit(); \n assertEquals(BytesValue.of(1, 2, 3), worldState.get(ADDRESS).getCode()); \n assertEquals( \n Hash.fromHexString(\"0xc14f5e30581de9155ea092affa665fad83bcd9f98e45c4a42885b9b36d939702\"), \n worldState.rootHash());", "model_em_trim": 0, "model_bleu_trim": 55.87829019112313, "error_type_gpt": "Need Domain Knowledge"}
{"id": 254, "old": " \t\t     location: location\n \t    };\n       try {\n-        if (Boolean(options.slot)) {\n           client.sites.createOrUpdateSiteSlot(resourceGroup, name, siteEnvelope, options.slot, _);\n           log.info('Webapp slot ' + name + '/' + options.slot + ' has been created ');\n         } else {", "new": " \t\t     location: location\n \t    };\n       try {\n+        if (options.slot) {\n           client.sites.createOrUpdateSiteSlot(resourceGroup, name, siteEnvelope, options.slot, _);\n           log.info('Webapp slot ' + name + '/' + options.slot + ' has been created ');\n         } else {", "review": "you can just use `if (options.slot)`, empty string is a false value", "dataset": "codereview", "original_id": 794, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\n\t\t     location: location\n\t    };\n      try {\n        if (options.slot) {\n          client.sites.createOrUpdateSiteSlot(resourceGroup, name, siteEnvelope, options.slot, _);\n          log.info('Webapp slot ' + name + '/' + options.slot + ' has been created ');\n        } else {\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n location: location \n }; \n try { \n if (options.slot) { \n client.sites.createOrUpdateSiteSlot(resourceGroup, name, siteEnvelope, options.slot, _); \n log.info('Webapp slot ' + name + '/' + options.slot + ' has been created '); \n } else {", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 255, "old": "         return graph_data\n     if readonly:\n         gidx = GraphIndex(None, multigraph, readonly)\n     else:\n         handle = _CAPI_DGLGraphCreateMutable(multigraph)", "new": "         return graph_data\n     if readonly:\n+        # FIXME(zhengda): we should construct a C graph index before constructing GraphIndex.\n         gidx = GraphIndex(None, multigraph, readonly)\n     else:\n         handle = _CAPI_DGLGraphCreateMutable(multigraph)", "review": "To avoid this awkward `None`, the solution is to first process all the graph data. There are functions to convert different types of graph data to `src, dst, edge_id, num_nodes` first, so the handler could be created and then the python shim `GraphIndex`. Leave this for now. But would you put a `FIXME` here?", "dataset": "codereview", "original_id": 810, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-add", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-add", "gpt_code": "\n# Process graph data first\nsrc, dst, edge_id, num_nodes = process_graph_data(graph_data)\n\nif readonly:\n    gidx = GraphIndex(src, dst, edge_id, num_nodes, multigraph, readonly)\nelse:\n    handle = _CAPI_DGLGraphCreateMutable(multigraph)\n    gidx = GraphIndex(src, dst, edge_id, num_nodes, handle)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 32.55363, "model_code": "\n return graph_data \n if readonly: \n # FIXME: The solution is to first process all the graph data. There are functions to convert different types of graph data to `src, dst, edge_id, num_nodes` first, so the handler could be created and then the python shim `GraphIndex`. Leave this for now. \n gidx = GraphIndex(None, multigraph, readonly) \n else: \n handle = _CAPI_DGLGraphCreateMutable(multigraph)", "model_em_trim": 0, "model_bleu_trim": 33.64628549908699, "error_type_gpt": "Need Domain Knowledge"}
{"id": 256, "old": "         for index, reporter in enumerate(reporting):\n             reporter = ensure_is_dict(reporting, index, \"module\")\n             cls = reporter.get('module', ValueError())\n-            if cls != 'blazemeter':\n                 new_reporting.append(reporter)\n         self.engine.config[Reporter.REP] = new_reporting\n         config = self.get_config_for_cloud()", "new": "         for index, reporter in enumerate(reporting):\n             reporter = ensure_is_dict(reporting, index, \"module\")\n             cls = reporter.get('module', ValueError())\n+            if cls == 'blazemeter':\n+                self.log.warning(\"Explicit blazemeter reporting is skipped for cloud\")\n+            else:\n                 new_reporting.append(reporter)\n+\n         self.engine.config[Reporter.REP] = new_reporting\n         config = self.get_config_for_cloud()", "review": "Display warning when removing it", "dataset": "codereview", "original_id": 831, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n        for index, reporter in enumerate(reporting):\n            reporter = ensure_is_dict(reporting, index, \"module\")\n            cls = reporter.get('module', ValueError())\n            if cls != 'blazemeter':\n                new_reporting.append(reporter)\n        if len(new_reporting) == 0:\n            print(\"Warning: No reporters left after removing blazemeter\")\n        self.engine.config[Reporter.REP] = new_reporting\n        config = self.get_config_for_cloud()\n", "gpt_em_trim": 0, "gpt_bleu_trim": 80.97702, "model_code": "\n for index, reporter in enumerate(reporting): \n reporter = ensure_is_dict(reporting, index, \"module\") \n cls = reporter.get('module', ValueError()) \n new_reporting.append(reporter) \n self.engine.config[Reporter.REP] = new_reporting \n config = self.get_config_for_cloud()", "model_em_trim": 0, "model_bleu_trim": 69.57243043490988, "error_type_gpt": "Need Domain Knowledge"}
{"id": 257, "old": "     // if conversion failed, check for the \"//\" separator used in split cards\n     if (!convertSuccess) {\n         int cmcSum = 0;\n-        foreach (QString cmc, info->getCmc().split(\"//\")) {\n             cmcInt = cmc.toInt();\n             cmcSum += cmcInt;\n             if (relationCheck(cmcInt)) {", "new": "     // if conversion failed, check for the \"//\" separator used in split cards\n     if (!convertSuccess) {\n         int cmcSum = 0;\n+        for (QString cmc : info->getCmc().split(\"//\")) {\n             cmcInt = cmc.toInt();\n             cmcSum += cmcInt;\n             if (relationCheck(cmcInt)) {", "review": "can you change this to `for (QString cmc : info->getCMC().split(\"//\")) {` ? I'm trying to remove the foreach from the codebase as it's not proper C++", "dataset": "codereview", "original_id": 857, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\n// if conversion failed, check for the \"//\" separator used in split cards\nif (!convertSuccess) {\n    int cmcSum = 0;\n    for (QString cmc : info->getCmc().split(\"//\")) {\n        cmcInt = cmc.toInt();\n        cmcSum += cmcInt;\n        if (relationCheck(cmcInt)) {\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n // if conversion failed, check for the \"//\" separator used in split cards \n if (!convertSuccess) { \n int cmcSum = 0; \n for (QString cmc : info->getCMC().split(\"//\")) { \n cmcInt = cmc.toInt(); \n cmcSum += cmcInt; \n if (relationCheck(cmcInt)) {", "model_em_trim": 0, "model_bleu_trim": 100.0}
{"id": 258, "old": "             http_error_msg = u'%s Server Error: %s for url: %s' % (self.status_code, reason, self.url)\n         if http_error_msg:\n-            if isinstance(body_text, basestring):\n-                http_error_msg += u' Response Body: %s' % body_text\n             raise HTTPError(http_error_msg, response=self)\n     def close(self):", "new": "             http_error_msg = u'%s Server Error: %s for url: %s' % (self.status_code, reason, self.url)\n         if http_error_msg:\n+            if isinstance(self.text, basestring) and include_text:\n+                http_error_msg += u' Response Body: %s' % body_text[:max_text_length]\n             raise HTTPError(http_error_msg, response=self)\n     def close(self):", "review": "The body text may be *enormous*: megabytes in size. We absolutely do not want to build a string that long. If we're adding the body text to the exception message, we should add only a segment of it.", "dataset": "codereview", "original_id": 865, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nMAX_BODY_TEXT_LENGTH = 1000\n\nhttp_error_msg = u'%s Server Error: %s for url: %s' % (self.status_code, reason, self.url)\nif http_error_msg:\n    if isinstance(body_text, basestring):\n        body_text_segment = body_text[:MAX_BODY_TEXT_LENGTH]\n        http_error_msg += u' Response Body (first %d bytes): %s' % (MAX_BODY_TEXT_LENGTH, body_text_segment)\n    raise HTTPError(http_error_msg, response=self)\ndef close(self):\n", "gpt_em_trim": 0, "gpt_bleu_trim": 68.599, "model_code": "\n http_error_msg = u'%s Server Error: %s for url: %s' % (self.status_code, reason, self.url) \n if http_error_msg: \n raise HTTPError(http_error_msg, response=self) \n def close(self):", "model_em_trim": 0, "model_bleu_trim": 50.31109914488275, "error_type_gpt": "Need Domain Knowledge"}
{"id": 259, "old": "         data, feature_name, categorical_feature, self.pandas_categorical = _data_from_pandas(data, feature_name, categorical_feature, self.pandas_categorical)\n         label = _label_from_pandas(label)\n         self.data_has_header = False\n-        \"\"\"process for args\"\"\"\n         params = {} if params is None else params\n         self.max_bin = max_bin\n         self.predictor = predictor", "new": "         data, feature_name, categorical_feature, self.pandas_categorical = _data_from_pandas(data, feature_name, categorical_feature, self.pandas_categorical)\n         label = _label_from_pandas(label)\n         self.data_has_header = False\n+        # process for args\n         params = {} if params is None else params\n         self.max_bin = max_bin\n         self.predictor = predictor", "review": "I think `next` may not be clear. Can we give a specific version number?", "dataset": "codereview", "original_id": 869, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "\n        data, feature_name, categorical_feature, self.pandas_categorical = _data_from_pandas(data, feature_name, categorical_feature, self.pandas_categorical)\n        label = _label_from_pandas(label)\n        self.data_has_header = False\n        \"\"\"process for args\"\"\"\n        params = {} if params is None else params\n        self.max_bin = max_bin\n        self.predictor = predictor\n        self.version = \"1.0.0\"\n", "gpt_em_trim": 0, "gpt_bleu_trim": 90.07919, "model_code": "\n data, feature_name, categorical_feature, self.pandas_categorical = _data_from_pandas(data, feature_name, categorical_feature, self.pandas_categorical) \n label = _label_from_pandas(label) \n self.data_has_header = False \n params = {} if params is None else params \n self.max_bin = max_bin \n self.predictor = predictor", "model_em_trim": 0, "model_bleu_trim": 93.58897266104779}
{"id": 260, "old": "     }\n     default Tuple2<Seq<K>, Seq<V>> unzip() {\n-        return this.unzip(Function.identity());\n     }\n     default <T1, T2> Tuple2<Seq<T1>, Seq<T2>> unzip(BiFunction<? super K, ? super V, Tuple2<? extends T1, ? extends T2>> unzipper) {", "new": "     }\n     default Tuple2<Seq<K>, Seq<V>> unzip() {\n+        return unzip(Function.identity());\n     }\n     default <T1, T2> Tuple2<Seq<T1>, Seq<T2>> unzip(BiFunction<? super K, ? super V, Tuple2<? extends T1, ? extends T2>> unzipper) {", "review": "we can remove `this.`", "dataset": "codereview", "original_id": 880, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\n    }\n    default Tuple2<Seq<K>, Seq<V>> unzip() {\n        return unzip(Function.identity());\n    }\n    default <T1, T2> Tuple2<Seq<T1>, Seq<T2>> unzip(BiFunction<? super K, ? super V, Tuple2<? extends T1, ? extends T2>> unzipper) {\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n } \n default Tuple2<Seq<K>, Seq<V>> unzip() { \n return unzip(Function.identity()); \n } \n default <T1, T2> Tuple2<Seq<T1>, Seq<T2>> unzip(BiFunction<? super K, ? super V, Tuple2<? extends T1, ? extends T2>> unzipper) {", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 261, "old": "                     formattedMessage = string.Format(CultureInfo.InvariantCulture, message, args);\n                 }\n-                if (ex != null)\n-                {\n-                    ex.MarkedAsLoggedToInternalLogger();\n-                    formattedMessage += \" Exception: \" + ex.ToString();\n-                }\n-\n                 var builder = new StringBuilder(message.Length + 32);\n                 if (IncludeTimestamp)\n                 {", "new": "                     formattedMessage = string.Format(CultureInfo.InvariantCulture, message, args);\n                 }\n                 var builder = new StringBuilder(message.Length + 32);\n                 if (IncludeTimestamp)\n                 {", "review": "How about adding the exception string to the `StringBuilder` below instead?", "dataset": "codereview", "original_id": 883, "relevance": "partial", "relevance_user1": "not", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n                    formattedMessage = string.Format(CultureInfo.InvariantCulture, message, args);\n                }\n\n                var builder = new StringBuilder(message.Length + 32);\n                if (IncludeTimestamp)\n                {\n                    builder.Append(DateTime.UtcNow.ToString(\"o\", CultureInfo.InvariantCulture)).Append(\" \");\n                }\n\n                builder.Append(formattedMessage);\n\n                if (ex != null)\n                {\n                    ex.MarkedAsLoggedToInternalLogger();\n                    builder.Append(\" Exception: \").Append(ex.ToString());\n                }\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n formattedMessage = string.Format(CultureInfo.InvariantCulture, message, args); \n } \n var builder = new StringBuilder(message.Length + 32); \n if (IncludeTimestamp) \n {", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 262, "old": "                 loadFanartConfig(page, metadata)\n             });\n             ApiClient.getSystemInfo().then(function(info) {\n-                \"Windows\" === info.OperatingSystem ? page.querySelector(\".fldSaveMetadataHidden\").classList.remove(\"hide\") : page.querySelector(\".fldSaveMetadataHidden\").classList.add(\"hide\")\n             });\n         });\n     }", "new": "                 loadFanartConfig(page, metadata)\n             });\n             ApiClient.getSystemInfo().then(function(info) {\n+                 if (\"Windows\" === info.OperatingSystem) {\n+                     page.querySelector(\".fldSaveMetadataHidden\").classList.remove(\"hide\");\n+                 } else {\n+                     page.querySelector(\".fldSaveMetadataHidden\").classList.add(\"hide\");\n+                 }\n             });\n         });\n     }", "review": "```suggestion if (\"Windows\" === info.OperatingSystem) { page.querySelector(\".fldSaveMetadataHidden\").classList.remove(\"hide\"); } else { page.querySelector(\".fldSaveMetadataHidden\").classList.add(\"hide\"); } ```", "dataset": "codereview", "original_id": 912, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nloadFanartConfig(page, metadata)\n}).then(function() {\n    return ApiClient.getSystemInfo();\n}).then(function(info) {\n    if (\"Windows\" === info.OperatingSystem) {\n        page.querySelector(\".fldSaveMetadataHidden\").classList.remove(\"hide\");\n    } else {\n        page.querySelector(\".fldSaveMetadataHidden\").classList.add(\"hide\");\n    }\n});\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n loadFanartConfig(page, metadata) \n }); \n ApiClient.getSystemInfo().then(function(info) { \n if (\"Windows\" === info.OperatingSystem) { \n page.querySelector(\".fldSaveMetadataHidden\").classList.remove(\"hide\"); \n } else { \n page.querySelector(\".fldSaveMetadataHidden\").classList.add(\"hide\"); \n } \n }); \n }); \n }", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 263, "old": " total_epochs = 12\n dist_params = dict(backend='nccl')\n log_level = 'INFO'\n-work_dir = './work_dirs/faster_rcnn_r50_fpn_1x'\n load_from = None\n resume_from = None\n workflow = [('train', 1)]", "new": " total_epochs = 12\n dist_params = dict(backend='nccl')\n log_level = 'INFO'\n+work_dir = './work_dirs/faster_rcnn_r50_fpn_carafe_1x'\n load_from = None\n resume_from = None\n workflow = [('train', 1)]", "review": "Use the same name as the config file.", "dataset": "codereview", "original_id": 916, "relevance": "not", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Refactoring-rename", "gpt_code": "\nconfig = {\n    'total_epochs': 12,\n    'dist_params': dict(backend='nccl'),\n    'log_level': 'INFO',\n    'work_dir': './work_dirs/faster_rcnn_r50_fpn_1x',\n    'load_from': None,\n    'resume_from': None,\n    'workflow': [('train', 1)]\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 95.9192, "model_code": "\n total_epochs = 12 \n dist_params = dict(backend='nccl') \n log_level = 'INFO' \n work_dir = './work_dirs/rcnn_fpn_1x' \n load_from = None \n resume_from = None \n workflow = [('train', 1)]", "model_em_trim": 0, "model_bleu_trim": 86.71017039778937}
{"id": 264, "old": " \t\t// needed for the `sortSlice` field, to avoid an\n \t\t// allocation while sorting.\n \t\trec = &record{}\n-\t\t// Make a copy of `kvs` so that the original set will not be sorted\n-\t\ttmpKvs := make([]attribute.KeyValue, len(kvs))\n-\t\tcopy(tmpKvs, kvs)\n-\t\trec.storage = attribute.NewSetWithSortable(tmpKvs, &rec.sortSlice)\n \t\trec.labels = &rec.storage\n \t\tequiv = rec.storage.Equivalent()\n \t} else {", "new": " \t\t// needed for the `sortSlice` field, to avoid an\n \t\t// allocation while sorting.\n \t\trec = &record{}\n+\t\trec.storage = attribute.NewSetWithSortable(kvs, &rec.sortSlice)\n \t\trec.labels = &rec.storage\n \t\tequiv = rec.storage.Equivalent()\n \t} else {", "review": "This is adding an allocation to a highly optimized method. Can we instead take the other option outlined in the link issue and update the function docs to clearly indicate that the labels are mutated?", "dataset": "codereview", "original_id": 925, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\n\t\t// needed for the `sortSlice` field, to avoid an\n\t\t// allocation while sorting.\n\t\t// Make a copy of `kvs` so that the original set will not be sorted\n\t\ttmpKvs := make([]attribute.KeyValue, len(kvs))\n\t\tcopy(tmpKvs, kvs)\n\t\trec.storage = attribute.NewSetWithSortable(tmpKvs, &rec.sortSlice)\n\t\trec.labels = &rec.storage\n\t\tequiv = rec.storage.Equivalent()\n\t\t// Note: The `labels` field is mutated by this method.\n\t} else {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 40.18361, "model_code": "\n // needed for the `sortSlice` field, to avoid an \n // allocation while sorting. \n rec = &record{} \n rec.storage = attribute.NewSetWithSortable(kvs, &rec.sortSlice) \n rec.labels = &rec.storage \n equiv = rec.storage.Equivalent() \n } else {", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 265, "old": " \t// create the desired number of signers\n \tvar signer *AggregationProvider\n \tvar priv crypto.PrivateKey\n-\tmsg := createMSGB(b)\n \tsigs := make([]crypto.Signature, 0, NUM_AGG_BENCH)\n \tkeys := make([]crypto.PublicKey, 0, NUM_AGG_BENCH)\n \tfor i := 0; i < NUM_AGG_BENCH; i++ {", "new": " \t// create the desired number of signers\n \tvar signer *AggregationProvider\n \tvar priv crypto.PrivateKey\n+\tmsg := randomByteSliceB(b)\n \tsigs := make([]crypto.Signature, 0, NUM_AGG_BENCH)\n \tkeys := make([]crypto.PublicKey, 0, NUM_AGG_BENCH)\n \tfor i := 0; i < NUM_AGG_BENCH; i++ {", "review": "you may replace all similar occurrences in this file with `require.NoError(b, err)`", "dataset": "codereview", "original_id": 930, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n\t// create the desired number of signers\n\tvar signer *AggregationProvider\n\tvar priv crypto.PrivateKey\n\tmsg := createMSGB(b)\n\tsigs := make([]crypto.Signature, 0, NUM_AGG_BENCH)\n\tkeys := make([]crypto.PublicKey, 0, NUM_AGG_BENCH)\n\tfor i := 0; i < NUM_AGG_BENCH; i++ {\n\t\terr := signer.Sign(msg, priv)\n\t\trequire.NoError(b, err)\n\t\tsig := signer.GetSignature()\n\t\tsigs = append(sigs, sig)\n\t\tpub := signer.GetPublicKey()\n\t\tkeys = append(keys, pub)\n\t}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 96.82028, "model_code": "\n // create the desired number of signers \n var signer *AggregationProvider \n var priv crypto.PrivateKey \n sigs := make([]crypto.Signature, 0, NUM_AGG_BENCH) \n keys := make([]crypto.PublicKey, 0, NUM_AGG_BENCH) \n for i := 0; i < NUM_AGG_BENCH; i++ {", "model_em_trim": 0, "model_bleu_trim": 89.05832557720656}
{"id": 266, "old": "     return str1 > str2;\n }\n-bool Army::isFootsloggingArmy() const\n {\n     double meleeInfantry = 0;\n     double other = 0;", "new": "     return str1 > str2;\n }\n+bool Army::isMeleeDominantArmy() const\n {\n     double meleeInfantry = 0;\n     double other = 0;", "review": "This method name is a bit confusing. Maybe we could rename it into isMeleeDominantArmy()? What do you think?", "dataset": "codereview", "original_id": 971, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\nreturn str1 > str2;\n}\n\nbool Army::isMeleeDominantArmy() const\n{\n    double meleeInfantry = 0;\n    double other = 0;\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n return str1 > str2; \n } \n bool Army::isMeleeDominantArmy() const \n { \n double meleeInfantry = 0; \n double other = 0;", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 267, "old": "             closeStream(null);\n         }\n         if (xmppSession != null) {\n-            xmppSession.getStreamManager().formalClose();\n-\n             if (!xmppSession.getStreamManager().getResume()) {\n                 Log.debug( \"Closing session {}\", xmppSession );\n                 xmppSession.close();", "new": "             closeStream(null);\n         }\n         if (xmppSession != null) {\n             if (!xmppSession.getStreamManager().getResume()) {\n                 Log.debug( \"Closing session {}\", xmppSession );\n                 xmppSession.close();", "review": "This does not seem right. Effectively, we've allowed resumption to be negotiated, but when the session is closed, we disregard that completely, and instead simply close the session. It would be a lot better to prevent resumption from being negotiated for these kind of sessions in the first place. I think that can be done by modifying `org.jivesoftware.openfire.streammanagement.StreamManager#allowResume`", "dataset": "codereview", "original_id": 975, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n            closeStream(null);\n        }\n        if (xmppSession != null) {\n            StreamManager streamManager = xmppSession.getStreamManager();\n            if (!streamManager.allowResume()) {\n                streamManager.setAllowResume(false);\n            }\n            streamManager.formalClose();\n            if (!streamManager.getResume()) {\n                Log.debug( \"Closing session {}\", xmppSession );\n                xmppSession.close();\n            }\n        }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 81.43827, "model_code": "\n closeStream(null); \n } \n if (xmppSession != null) { \n if (!xmppSession.getStreamManager().getResume()) { \n Log.debug( \"Closing session {}\", xmppSession ); \n xmppSession.close();", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 268, "old": "     if (pendingConfiguration.port != null) {\n       pendingRequest.configuration.port = pendingConfiguration.port;\n     }\n-    if (pendingConfiguration.remoteHost != null) {\n-      pendingRequest.configuration.remoteHost = pendingConfiguration.remoteHost;\n-    }\n     // make sure we have a valid host\n     pendingRequest.configuration.fixUpHost();", "new": "     if (pendingConfiguration.port != null) {\n       pendingRequest.configuration.port = pendingConfiguration.port;\n     }\n     // make sure we have a valid host\n     pendingRequest.configuration.fixUpHost();", "review": "This is not really needed, the line added in `GridNodeConfiguration.java` is what really fixes the `remoteHost` regression.", "dataset": "codereview", "original_id": 996, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n    if (pendingConfiguration.port != null) {\n      pendingRequest.configuration.port = pendingConfiguration.port;\n    }\n    // make sure we have a valid host\n    pendingRequest.configuration.fixUpHost();\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n if (pendingConfiguration.port != null) { \n pendingRequest.configuration.port = pendingConfiguration.port; \n } \n // make sure we have a valid host \n pendingRequest.configuration.fixUpHost();", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 269, "old": " \t\t\t\t\t\t\t<dd>\n \t\t\t\t\t\t\t\t<select name=\"id_group\" ', ($context['user']['is_owner'] && $context['member']['group_id'] == 1 ? 'onchange=\"if (this.value != 1 &amp;&amp; !confirm(\\'' . $txt['deadmin_confirm'] . '\\')) this.value = 1;\"' : ''), '>';\n-\t// Fill the select box with all primary member groups that can be assigned to a member.\n \tforeach ($context['member_groups'] as $member_group)\n \t\tif (!empty($member_group['can_be_primary']))\n \t\t\techo '", "new": " \t\t\t\t\t\t\t<dd>\n \t\t\t\t\t\t\t\t<select name=\"id_group\" ', ($context['user']['is_owner'] && $context['member']['group_id'] == 1 ? 'onchange=\"if (this.value != 1 &amp;&amp; !confirm(\\'' . $txt['deadmin_confirm'] . '\\')) this.value = 1;\"' : ''), '>';\n+\t// Fill the select box with all primary membergroups that can be assigned to a member.\n \tforeach ($context['member_groups'] as $member_group)\n \t\tif (!empty($member_group['can_be_primary']))\n \t\t\techo '", "review": "I know it's nitpicking but I'd use \"membergroups\" instead of \"member groups\"..", "dataset": "codereview", "original_id": 1020, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "\n\t\t\t\t\t\t\t<dd>\n\t\t\t\t\t\t\t\t<select name=\"id_group\" ', ($context['user']['is_owner'] && $context['member']['group_id'] == 1 ? 'onchange=\"if (this.value != 1 &amp;&amp; !confirm(\\'' . $txt['deadmin_confirm'] . '\\')) this.value = 1;\"' : ''), '>';\n\t// Fill the select box with all primary membergroups that can be assigned to a member.\n\tforeach ($context['member_groups'] as $member_group)\n\t\tif (!empty($member_group['can_be_primary']))\n\t\t\techo '\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n <dd> \n <select name=\"id_group\" ', ($context['user']['is_owner'] && $context['member']['group_id'] == 1 ? 'onchange=\"if (this.value != 1 &amp;&amp; !confirm(\\'' . $txt['deadmin_confirm'] . '\\')) this.value = 1;\"' : ''), '>'; \n // Fill the select box with all primary member groups that can be assigned to a member. \n foreach ($context['member_groups'] as $member_group) \n if (!empty($member_group['can_be_primary'])) \n echo '", "model_em_trim": 0, "model_bleu_trim": 97.85022712679378}
{"id": 270, "old": "         category = Category.ONE_OFF,\n         link = \"https://github.com/palantir/gradle-baseline#baseline-error-prone-checks\",\n         linkType = LinkType.CUSTOM,\n-        severity = SeverityLevel.WARNING,\n         summary = \"log statement in catch block does not log the caught exception.\")\n public final class CatchBlockLogException extends BugChecker implements BugChecker.CatchTreeMatcher {", "new": "         category = Category.ONE_OFF,\n         link = \"https://github.com/palantir/gradle-baseline#baseline-error-prone-checks\",\n         linkType = LinkType.CUSTOM,\n+        severity = SeverityLevel.ERROR,\n         summary = \"log statement in catch block does not log the caught exception.\")\n public final class CatchBlockLogException extends BugChecker implements BugChecker.CatchTreeMatcher {", "review": "IMHO if there's no false positives it should be an ERROR since you always should be logging the exception?", "dataset": "codereview", "original_id": 1066, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n        category = Category.ONE_OFF,\n        link = \"https://github.com/palantir/gradle-baseline#baseline-error-prone-checks\",\n        linkType = LinkType.CUSTOM,\n        severity = SeverityLevel.ERROR,\n        summary = \"log statement in catch block does not log the caught exception.\")\npublic final class CatchBlockLogException extends BugChecker implements BugChecker.CatchTreeMatcher {\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n category = Category.ONE_OFF, \n link = \"https://github.com/palantir/gradle-baseline#baseline-error-prone-checks\", \n linkType = LinkType.CUSTOM, \n severity = SeverityLevel.ERROR, \n summary = \"log statement in catch block does not log the caught exception.\") \n public final class CatchBlockLogException extends BugChecker implements BugChecker.CatchTreeMatcher {", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 271, "old": "    * OLD: This routine inverts this relation using the iterative scheme given\n    * by Snyder (1987), Eqs. (7-9) - (7-11).\n    *\n-   * NEW: This routine writes converts t = exp(-psi) to\n    *\n    *   tau' = sinh(psi) = (1/t - t)/2\n    *", "new": "    * OLD: This routine inverts this relation using the iterative scheme given\n    * by Snyder (1987), Eqs. (7-9) - (7-11).\n    *\n+   * NEW: This routine converts t = exp(-psi) to\n    *\n    *   tau' = sinh(psi) = (1/t - t)/2\n    *", "review": "```suggestion * NEW: This routine converts t = exp(-psi) to ```", "dataset": "codereview", "original_id": 1081, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n   * NEW: This routine converts t = exp(-psi) to\n   *\n   *   tau' = sinh(psi) = (1/t - t)/2\n   *\n", "gpt_em_trim": 0, "gpt_bleu_trim": 36.78794, "model_code": "\n * OLD: This routine inverts this relation using the iterative scheme given \n * by Snyder (1987), Eqs. (7-9) - (7-11). \n * \n * NEW: This routine converts t = exp(-psi) to \n * \n *   tau' = sinh(psi) = (1/t - t)/2 \n *", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Insignificant Omission"}
{"id": 272, "old": "   }\n   listenToCommands() {\n-    this.events.setCommandHandler('start-webserver', () => { this.server.start(); });\n-    this.events.setCommandHandler('stop-webserver',  () => { this.server.stop();  });\n   }\n   registerConsoleCommands() {", "new": "   }\n   listenToCommands() {\n+    this.events.setCommandHandler('start-webserver', (callback) => this.server.start(callback));\n+    this.events.setCommandHandler('stop-webserver',  (callback) => this.server.stop(callback));\n   }\n   registerConsoleCommands() {", "review": "Why do we add \"OK\"? Shouldn't the absence of error be sufficient? Or is that what's printed? Either way, printing nothing or success would be better. Maybe, if the var is empty, print 'Success'", "dataset": "codereview", "original_id": 1113, "relevance": "not", "relevance_user1": "partial", "relevance_user2": "not", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n  }\n  listenToCommands() {\n    this.events.setCommandHandler('start-webserver', () => { \n      this.server.start(); \n      console.log('Web server started successfully');\n    });\n    this.events.setCommandHandler('stop-webserver',  () => { \n      this.server.stop();  \n      console.log('Web server stopped successfully');\n    });\n  }\n  registerConsoleCommands() {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 57.73312, "model_code": "\n } \n listenToCommands() { \n this.events.setCommandHandler('server-start', () => { this.server.start(); }); \n this.events.setCommandHandler('server-stop',  () => { this.server.stop();  }); \n } \n registerConsoleCommands() {", "model_em_trim": 0, "model_bleu_trim": 59.83223351072581}
{"id": 273, "old": " \t// from config loader (eg. a http loader) with given interval.\n \t//\n \t// EXPERIMENTAL: Subject to change.\n-\tPullInterval Duration `json:\"pull_interval,omitempty\"`\n }\n // IdentityConfig configures management of this server's identity. An identity", "new": " \t// from config loader (eg. a http loader) with given interval.\n \t//\n \t// EXPERIMENTAL: Subject to change.\n+\tLoadInterval Duration `json:\"pull_interval,omitempty\"`\n }\n // IdentityConfig configures management of this server's identity. An identity", "review": "On second thought... this affects how often the function specified by the `load` property is run. So I wonder if, for consistency, we should call this `LoadInterval` instead. Does that make sense?", "dataset": "codereview", "original_id": 1121, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\n\t// from config loader (eg. a http loader) with given interval.\n\t//\n\t// EXPERIMENTAL: Subject to change.\n\tLoadInterval Duration `json:\"load_interval,omitempty\"`\n}\n// IdentityConfig configures management of this server's identity. An identity\n", "gpt_em_trim": 0, "gpt_bleu_trim": 95.24801, "model_code": "\n // from config loader (eg. a http loader) with given interval. \n // \n // EXPERIMENTAL: Subject to change. \n LoadInterval Duration `json:\"load_interval,omitempty\"` \n } \n // IdentityConfig configures management of this server's identity. An identity", "model_em_trim": 0, "model_bleu_trim": 95.24801432329332, "error_type_gpt": "Reasonable Improvement"}
{"id": 274, "old": " \t\t\t\t'items_list_navigation' => __( 'Navigation Menus list navigation' ),\n \t\t\t\t'items_list'            => __( 'Navigation Menus list' ),\n \t\t\t),\n-\t\t\t'description'           => __( 'Custom post type for navigation menus.' ),\n \t\t\t'public'                => false,\n \t\t\t'_builtin'              => true, /* internal use only. don't use this when registering your own post type. */\n \t\t\t'has_archive'           => false,", "new": " \t\t\t\t'items_list_navigation' => __( 'Navigation Menus list navigation' ),\n \t\t\t\t'items_list'            => __( 'Navigation Menus list' ),\n \t\t\t),\n \t\t\t'public'                => false,\n \t\t\t'_builtin'              => true, /* internal use only. don't use this when registering your own post type. */\n \t\t\t'has_archive'           => false,", "review": "Really no reason to mention \"custom post type\" here. ```suggestion 'description' => __( 'Navigation menus.' ), ``` could even just omit it, tbh", "dataset": "codereview", "original_id": 1124, "relevance": "not", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n\t\t\t\t'items_list_navigation' => __( 'Navigation Menus list navigation' ),\n\t\t\t\t'items_list'            => __( 'Navigation Menus list' ),\n\t\t\t),\n\t\t\t'description'           => __( 'Navigation menus.' ),\n\t\t\t'public'                => false,\n\t\t\t'_builtin'              => true, /* internal use only. don't use this when registering your own post type. */\n\t\t\t'has_archive'           => false,\n", "gpt_em_trim": 0, "gpt_bleu_trim": 84.57461, "model_code": "\n 'items_list_navigation' => __( 'Navigation Menus list navigation' ), \n 'items_list'            => __( 'Navigation Menus list' ), \n ), \n 'description'           => __( 'Navigation menus.' ), \n 'public'                => false, \n '_builtin'              => true, /* internal use only. don't use this when registering your own post type. */ \n 'has_archive'           => false,", "model_em_trim": 0, "model_bleu_trim": 84.57460620920415}
{"id": 275, "old": "         return Layout([self, obj])\n     def __radd__(self, other):\n-        if isinstance(other, list): # Hack for Annotators?\n-            return NotImplemented\n         if isinstance(other, int):\n             raise TypeError(\"unsupported operand type(s) for +: 'int' and 'Overlay'. \"\n                             \"If you are using `sum(elements)` to combine a list of elements,\"", "new": "         return Layout([self, obj])\n     def __radd__(self, other):\n         if isinstance(other, int):\n             raise TypeError(\"unsupported operand type(s) for +: 'int' and 'Overlay'. \"\n                             \"If you are using `sum(elements)` to combine a list of elements,\"", "review": "```suggestion \"If you are trying to use a reduction like `sum(elements)` to combine a list of elements,\" ```", "dataset": "codereview", "original_id": 1135, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n        return Layout([self, obj])\n    def __radd__(self, other):\n        if isinstance(other, list): # Hack for Annotators?\n            return NotImplemented\n        if isinstance(other, int):\n            raise TypeError(\"unsupported operand type(s) for +: 'int' and 'Overlay'. \"\n                            \"If you are trying to use a reduction like `sum(elements)` to combine a list of elements, \"\n                            \"make sure all elements are of the same type and implement the `__add__` method.\")\n", "gpt_em_trim": 0, "gpt_bleu_trim": 74.76202, "model_code": "\n return Layout([self, obj]) \n def __radd__(self, other): \n if isinstance(other, int): \n raise TypeError(\"unsupported operand type(s) for +: 'int' and 'Overlay'. \" \n \"If you are using `sum(elements)` to combine a list of elements,\"", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 276, "old": " describe LessonCompletion do\n   subject(:lesson_completion) {\n-    LessonCompletion.new(:student_id => 1,:lesson_id => 1)\n   }\n   it { is_expected.to validate_presence_of(:student_id) }", "new": " describe LessonCompletion do\n   subject(:lesson_completion) {\n+    LessonCompletion.new(student_id: 1,lesson_id: 1)\n   }\n   it { is_expected.to validate_presence_of(:student_id) }", "review": "This is a minor thing. Can we stick with ruby 1.9 hash syntax for consistency?", "dataset": "codereview", "original_id": 1139, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\ndescribe LessonCompletion do\n  subject(:lesson_completion) {\n    LessonCompletion.new(student_id: 1, lesson_id: 1)\n  }\n  it { is_expected.to validate_presence_of(:student_id) }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 100.0, "model_code": "\n describe LessonCompletion do \n subject(:lesson_completion) { \n LessonCompletion.new(student_id: 1, lesson_id: 1) \n } \n it { is_expected.to validate_presence_of(:student_id) }", "model_em_trim": 0, "model_bleu_trim": 100.0, "error_type_gpt": "Code Style Difference"}
{"id": 277, "old": "         {loadingSpinner}\n-        {showMoreTransactions &&\n           <Button\n             className={buttonClasses}\n             label={intl.formatMessage(messages.showMoreTransactionsButtonLabel)}", "new": "         {loadingSpinner}\n+        {showMoreTransactionsButton &&\n           <Button\n             className={buttonClasses}\n             label={intl.formatMessage(messages.showMoreTransactionsButtonLabel)}", "review": "@tomothespian you are missing an `;` on this line!", "dataset": "codereview", "original_id": 1152, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\n        {loadingSpinner}\n        {showMoreTransactions &&\n          <Button\n            className={buttonClasses}\n            label={intl.formatMessage(messages.showMoreTransactionsButtonLabel)};\n", "gpt_em_trim": 0, "gpt_bleu_trim": 86.39543, "model_code": "\n {loadingSpinner} \n {showMoreTransactions && \n <Button \n className={buttonClasses} \n label={intl.formatMessage(messages.showMoreTransactionsButtonLabel)}", "model_em_trim": 0, "model_bleu_trim": 89.82071236936181}
{"id": 278, "old": "         List<Presence> presences = applyAffiliationChange(getRole(), groupMember, null);\n         if (presences.size() == 0 && isMembersOnly()) {\n-            sendAffiliationChangeNotification(groupMember, affiliation);\n         }\n         else {      // member is in MUC, send presence stanzas\n             for (Presence presence : presences) {", "new": "         List<Presence> presences = applyAffiliationChange(getRole(), groupMember, null);\n         if (presences.size() == 0 && isMembersOnly()) {\n+            sendOutOfRoomAffiliationChangeNotification(groupMember, affiliation);\n         }\n         else {      // member is in MUC, send presence stanzas\n             for (Presence presence : presences) {", "review": "I don't think it's correct to add the `role` attribute like this. This does not take into account whatever role the user had - it will always inform others that this user now has no role.", "dataset": "codereview", "original_id": 1154, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\nList<Presence> presences = applyAffiliationChange(getRole(), groupMember, null);\nif (presences.size() == 0 && isMembersOnly()) {\n    sendAffiliationChangeNotification(groupMember, affiliation);\n}\nelse {      // member is in MUC, send presence stanzas\n    for (Presence presence : presences) {\n        if (getRole() != null) {\n            presence.addExtension(new RoleExtension(getRole()));\n        }\n        send(presence);\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 95.87561, "model_code": "\n List<Presence> presences = applyAffiliationChange(getRole(), groupMember, null); \n if (presences.size() == 0 && isMembersOnly()) { \n sendAffiliationChangeNotification(groupMember); \n } \n else {      // member is in MUC, send presence stanzas \n for (Presence presence : presences) {", "model_em_trim": 0, "model_bleu_trim": 90.55161550882741}
{"id": 279, "old": " \tfor _, tt := range tests {\n \t\tt.Run(tt.name, func(t *testing.T) {\n \t\t\trequire.NoError(t, tt.service.Start(t))\n-\t\t\tdefer func() { require.NoError(t, tt.service.Stop(t)) }()\n \t\t\ttt.request.Run(t)\n \t\t})\n \t}", "new": " \tfor _, tt := range tests {\n \t\tt.Run(tt.name, func(t *testing.T) {\n \t\t\trequire.NoError(t, tt.service.Start(t))\n+\t\t\tdefer func() { assert.NoError(t, tt.service.Stop(t)) }()\n \t\t\ttt.request.Run(t)\n \t\t})\n \t}", "review": "let's make this an assert. defers are called when the goroutine exits. this includes when a panic happens or when `runtime.Goexit()` is called (which is the \"kill the current goroutine\" API). require.Foo relies on runtime.Goexit, so require inside a defer will end up calling `runtime.Goexit` even though we're already in that process.", "dataset": "codereview", "original_id": 1176, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\trequire.NoError(t, tt.service.Start(t))\n\t\t\tdefer func() {\n\t\t\t\terr := tt.service.Stop(t)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t}()\n\t\t\ttt.request.Run(t)\n\t\t})\n\t}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 88.98377, "model_code": "\n for _, tt := range tests { \n t.Run(tt.name, func(t *testing.T) { \n require.NoError(t, tt.service.Start(t)) \n tt.request.Run(t) \n }) \n }", "model_em_trim": 0, "model_bleu_trim": 63.819051994953426, "error_type_gpt": "Code Style Difference"}
{"id": 280, "old": "         except Exception:\n             yield None\n         finally:\n-            devnull.close()\n def array_to_bytes(buff):\n     # Python 3.9 removed the tostring() method on arrays, the new alias is tobytes()", "new": "         except Exception:\n             yield None\n         finally:\n+            if devnull is not None:\n+                devnull.close()\n def array_to_bytes(buff):\n     # Python 3.9 removed the tostring() method on arrays, the new alias is tobytes()", "review": "if open raises devnull will be None, so devnull.close() would raise", "dataset": "codereview", "original_id": 1193, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Feature-add", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-add", "gpt_code": "\ndef array_to_bytes(buff):\n    # Python 3.9 removed the tostring() method on arrays, the new alias is tobytes()\n    devnull = None\n    try:\n        devnull = open(os.devnull, 'wb')\n        # code that uses devnull\n    except Exception:\n        yield None\n    finally:\n        if devnull:\n            devnull.close()\n", "gpt_em_trim": 0, "gpt_bleu_trim": 90.63366, "model_code": "\n except Exception: \n yield None \n finally: \n if devnull is not None: \n devnull.close() \n def array_to_bytes(buff): \n # Python 3.9 removed the tostring() method on arrays, the new alias is tobytes()", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Code Style Difference"}
{"id": 281, "old": "           :coll_name => name,\n           :write_concern => write_concern,\n           :bypass_document_validation => !!options[:bypass_document_validation],\n           :id_generator => client.options[:id_generator]\n         ).execute(next_primary.context)\n       end", "new": "           :coll_name => name,\n           :write_concern => write_concern,\n           :bypass_document_validation => !!options[:bypass_document_validation],\n+          :options => options,\n           :id_generator => client.options[:id_generator]\n         ).execute(next_primary.context)\n       end", "review": "we are sure we don't need to pass :options to Operation::Write::Insert#initialize ?", "dataset": "codereview", "original_id": 1198, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\n          :coll_name => name,\n          :write_concern => write_concern,\n          :bypass_document_validation => !!options[:bypass_document_validation],\n          :id_generator => client.options[:id_generator]\n        ).execute(next_primary.context)\n      end\n```\n\nRevised code:\n\n```\n          :coll_name => name,\n          :write_concern => write_concern,\n          :bypass_document_validation => !!options[:bypass_document_validation],\n          :id_generator => client.options[:id_generator]\n        ).execute(next_primary.context)\n      end\n", "gpt_em_trim": 0, "gpt_bleu_trim": 90.81254, "model_code": "\n :coll_name => name, \n :write_concern => write_concern, \n :bypass_document_validation => !!options[:bypass_document_validation], \n :id_generator => options[:id_generator] \n ).execute(next_primary.context) \n end", "model_em_trim": 0, "model_bleu_trim": 86.2116669228968, "error_type_gpt": "Need Domain Knowledge"}
{"id": 282, "old": " var Point = require('./Point'),\n   Key = require('./Key'),\n-  buffertools = require('buffertools'),\n   sha256 = require('../util').sha256,\n   twoSha256 = require('../util').twoSha256;", "new": " var Point = require('./Point'),\n   Key = require('./Key'),\n   sha256 = require('../util').sha256,\n   twoSha256 = require('../util').twoSha256;", "review": "Is this being used for anything? The only occurrence of \"buffertools\" in this file is the require. Maybe this dependency can be removed.", "dataset": "codereview", "original_id": 1208, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nvar Point = require('./Point'),\n  Key = require('./Key'),\n  sha256 = require('../util').sha256,\n  twoSha256 = require('../util').twoSha256;\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n var Point = require('./Point'), \n Key = require('./Key'), \n sha256 = require('../util').sha256, \n twoSha256 = require('../util').twoSha256;", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 283, "old": " var CtxKeyApplication = \"application\"\n // CtxKeyWorkflow request context key of workflow\n-var CtxKeyWorkflow = \"application\"\n // AddonPhase defines the phase of an addon\n type AddonPhase string", "new": " var CtxKeyApplication = \"application\"\n // CtxKeyWorkflow request context key of workflow\n+var CtxKeyWorkflow = \"workflow\"\n // AddonPhase defines the phase of an addon\n type AddonPhase string", "review": "```suggestion var CtxKeyWorkflow = \"workflow\" ```", "dataset": "codereview", "original_id": 1242, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nvar CtxKeyApplication = \"application\"\n// CtxKeyWorkflow request context key of workflow\nvar CtxKeyWorkflow = \"workflow\"\n// AddonPhase defines the phase of an addon\ntype AddonPhase string\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n var CtxKeyApplication = \"application\" \n // CtxKeyWorkflow request context key of workflow \n var CtxKeyWorkflow = \"workflow\" \n // AddonPhase defines the phase of an addon \n type AddonPhase string", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 284, "old": "     sampled = param.Boolean(default=False, doc=\"\"\"\n        Allows defining a DynamicMap in closed mode without defining the\n-       dimension bounds or values. Useful for allowing to let a HoloMap\n-       in a composite plot to define the dimension sampling.\n        \"\"\")\n     def __init__(self, callback, initial_items=None, **params):", "new": "     sampled = param.Boolean(default=False, doc=\"\"\"\n        Allows defining a DynamicMap in closed mode without defining the\n+       dimension bounds or values. The DynamicMap may then be explicitly\n+       sampled via getitem or the sampling is determined during plotting\n+       by a HoloMap with fixed sampling.\n        \"\"\")\n     def __init__(self, callback, initial_items=None, **params):", "review": "stray 'to let' in this docstring.", "dataset": "codereview", "original_id": 1244, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n    sampled = param.Boolean(default=False, doc=\"\"\"\n       Allows defining a DynamicMap in closed mode without defining the\n       dimension bounds or values. Useful for allowing a HoloMap\n       in a composite plot to define the dimension sampling.\n       \"\"\")\n    def __init__(self, callback, initial_items=None, **params):\n", "gpt_em_trim": 0, "gpt_bleu_trim": 72.19019, "model_code": "\n sampled = param.Boolean(default=False, doc=\"\"\" \n Allows defining a DynamicMap in closed mode without defining the \n dimension bounds or values. Useful for allowing to let a HoloMap \n in a composite plot to define the dimension sampling. \n \"\"\") \n def __init__(self, callback, initial_items=None, **params):", "model_em_trim": 0, "model_bleu_trim": 72.33140305889529, "error_type_gpt": "Need Domain Knowledge"}
{"id": 285, "old": " package org.flowable.engine.impl.webservice;\n public class Operands {", "new": "+/* Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n package org.flowable.engine.impl.webservice;\n public class Operands {", "review": "Missing required license header", "dataset": "codereview", "original_id": 1246, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-add", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-add", "gpt_code": "\n/*\n * Copyright (c) [Year] [Your Name]\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.flowable.engine.impl.webservice;\n\npublic class Operands {\n    // Code implementation goes here\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 99.28316, "model_code": "\n /* \n * Copyright 2019 Microsoft Corporation \n * \n * This code is licensed under the Apache License, Version 2.0 (the \"License\"); \n * you may not use this file except in compliance with the License. \n * You may obtain a copy of the License at \n * \n * http://www.apache.org/licenses/LICENSE-2.0 \n * \n * Unless required by applicable law or agreed to in writing, software \n * distributed under the License is distributed on an \"AS IS\" BASIS, \n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n * See the License for the specific language governing permissions and \n * limitations under the License. \n */ \n package org.flowable.engine.impl.webservice; \n public class Operands {", "model_em_trim": 0, "model_bleu_trim": 96.63559462530961, "error_type_gpt": "Need Domain Knowledge"}
{"id": 286, "old": "                 int max = security_policy->signature_preferences->signature_schemes[i]->maximum_protocol_version;\n                 s2n_signature_algorithm sig_alg = security_policy->signature_preferences->signature_schemes[i]->sig_alg;\n-                if (min >= S2N_TLS13 || max <= S2N_TLS13) {\n                     has_tls_13_sig_alg = 1;\n                 }", "new": "                 int max = security_policy->signature_preferences->signature_schemes[i]->maximum_protocol_version;\n                 s2n_signature_algorithm sig_alg = security_policy->signature_preferences->signature_schemes[i]->sig_alg;\n+                if (min == S2N_TLS13 || max >= S2N_TLS13) {\n                     has_tls_13_sig_alg = 1;\n                 }", "review": "Not sure how checking for `max <= S2N_TLS13` helps here, are we interested only in the minimum version?", "dataset": "codereview", "original_id": 1282, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n                int min = security_policy->signature_preferences->signature_schemes[i]->minimum_protocol_version;\n                s2n_signature_algorithm sig_alg = security_policy->signature_preferences->signature_schemes[i]->sig_alg;\n                if (min >= S2N_TLS13) {\n                    has_tls_13_sig_alg = 1;\n                }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 84.40005, "model_code": "\n int max = security_policy->signature_preferences->signature_schemes[i]->maximum_protocol_version; \n s2n_signature_algorithm sig_alg = security_policy->signature_preferences->signature_schemes[i]->sig_alg; \n if (min >= S2N_TLS13) { \n has_tls_13_sig_alg = 1; \n }", "model_em_trim": 0, "model_bleu_trim": 88.9874635068975, "error_type_gpt": "Need Domain Knowledge"}
{"id": 287, "old": "     EXPECT_SUCCESS(s2n_tls13_keys_init(&secrets, S2N_HMAC_SHA256));\n-    struct s2n_psk *psk = NULL;\n     /* Derive Early Secrets */\n-    EXPECT_SUCCESS(s2n_tls13_derive_early_secrets(&secrets, psk));\n     S2N_BLOB_EXPECT_EQUAL(secrets.extract_secret, expected_early_secret);\n     S2N_BLOB_EXPECT_EQUAL(secrets.derive_secret, expect_derived_handshake_secret);", "new": "     EXPECT_SUCCESS(s2n_tls13_keys_init(&secrets, S2N_HMAC_SHA256));\n     /* Derive Early Secrets */\n+    EXPECT_SUCCESS(s2n_tls13_derive_early_secrets(&secrets, NULL));\n     S2N_BLOB_EXPECT_EQUAL(secrets.extract_secret, expected_early_secret);\n     S2N_BLOB_EXPECT_EQUAL(secrets.derive_secret, expect_derived_handshake_secret);", "review": "Might be cleaner as just `EXPECT_SUCCESS(s2n_tls13_derive_early_secrets(&secrets, NULL));`", "dataset": "codereview", "original_id": 1283, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Feature-modify", "category_type_user2": "Refactoring-swap", "gpt_code": "\n    EXPECT_SUCCESS(s2n_tls13_keys_init(&secrets, S2N_HMAC_SHA256));\n    /* Derive Early Secrets */\n    EXPECT_SUCCESS(s2n_tls13_derive_early_secrets(&secrets, NULL));\n    S2N_BLOB_EXPECT_EQUAL(secrets.extract_secret, expected_early_secret);\n    S2N_BLOB_EXPECT_EQUAL(secrets.derive_secret, expect_derived_handshake_secret);\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n EXPECT_SUCCESS(s2n_tls13_keys_init(&secrets, S2N_HMAC_SHA256)); \n struct s2n_psk *psk = NULL; \n /* Derive Early Secrets */ \n EXPECT_SUCCESS(s2n_tls13_derive_early_secrets(&secrets, NULL)); \n S2N_BLOB_EXPECT_EQUAL(secrets.extract_secret, expected_early_secret); \n S2N_BLOB_EXPECT_EQUAL(secrets.derive_secret, expect_derived_handshake_secret);", "model_em_trim": 0, "model_bleu_trim": 90.54549954388533}
{"id": 288, "old": "   nmax = 0;\n   rho = NULL;\n   fp = NULL;\n-  count_embed = NULL;\n   map = NULL;\n   type2frho = NULL;", "new": "   nmax = 0;\n   rho = NULL;\n   fp = NULL;\n+  numforce = NULL;\n   map = NULL;\n   type2frho = NULL;", "review": "looks good - except let's call this numforce[], similar to numneigh[], since it is the number of atoms within the force cutoff", "dataset": "codereview", "original_id": 1289, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\n  numforce = 0;\n  rho = NULL;\n  fp = NULL;\n  count_embed = NULL;\n  map = NULL;\n  type2frho = NULL;\n", "gpt_em_trim": 0, "gpt_bleu_trim": 79.67348, "model_code": "\n nmax = 0; \n rho = NULL; \n fp = NULL; \n numforce = 0; \n count_embed = NULL; \n map = NULL; \n type2frho = NULL;", "model_em_trim": 0, "model_bleu_trim": 76.7395335910329, "error_type_gpt": "Need Domain Knowledge"}
{"id": 289, "old": " /**\n  * Returns an array of URL parameters that can be forwarded\n  *\n  * @return array<string,mixed>\n  */\n function getForwardParameters($request)", "new": " /**\n  * Returns an array of URL parameters that can be forwarded\n  *\n+ * @param LSHttpRequest $request the HTTP request\n+ *\n  * @return array<string,mixed>\n  */\n function getForwardParameters($request)", "review": "Missing @param LSRequest (or whatever) $request", "dataset": "codereview", "original_id": 1291, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-add", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-add", "gpt_code": "\n/**\n * Returns an array of URL parameters that can be forwarded\n *\n * @param LSRequest $request The request object\n * @return array<string,mixed>\n */\nfunction getForwardParameters(LSRequest $request)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 83.31768, "model_code": "\n /** \n * Returns an array of URL parameters that can be forwarded \n * \n * @param LSRequest $request \n * @return array<string,mixed> \n */ \n function getForwardParameters($request)", "model_em_trim": 0, "model_bleu_trim": 81.49537944198396, "error_type_gpt": "Need Domain Knowledge"}
{"id": 290, "old": "         RedistributeArmy( selectedTroop, destTroop, _army );\n         return true;\n     }\n-    else if ( destTroop.isValid() ) {\n         Army::SwapTroops( selectedTroop, destTroop );\n         return true;\n     }", "new": "         RedistributeArmy( selectedTroop, destTroop, _army );\n         return true;\n     }\n+\n+    if ( destTroop.isValid() ) {\n         Army::SwapTroops( selectedTroop, destTroop );\n         return true;\n     }", "review": ":warning: **readability\\-else\\-after\\-return** :warning: do not use `` else `` after `` return `` ```suggestion if ( destTroop.isValid() ) { Army::SwapTroops( selectedTroop, destTroop ); return true; } ```", "dataset": "codereview", "original_id": 1297, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nif ( selectedTroop.isValid() && destTroop.isValid() ) {\n    RedistributeArmy( selectedTroop, destTroop, _army );\n    return true;\n}\nif ( destTroop.isValid() ) {\n    Army::SwapTroops( selectedTroop, destTroop );\n    return true;\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 100.0, "model_code": "\n RedistributeArmy( selectedTroop, destTroop, _army ); \n return true; \n } \n if ( destTroop.isValid() ) { \n Army::SwapTroops( selectedTroop, destTroop ); \n return true; \n }", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Insignificant Omission"}
{"id": 291, "old": " Must be of the format '<keyName>:<dataType>'.`\n \tstorageSortKeyFlagDescription = `Optional. Sort key for the DDB table.\n Must be of the format '<keyName>:<dataType>'.`\n-\tstorageNoSortFlagDescription    = \"Optional. Don't ask about configuring sort keys.\"\n \tstorageAttributeFlagDescription = `Optional. Attributes for a DDB table.\n Must be of the format '<name>:<dataType>'. Can be specified multiple times.`\n \tstorageNoLsiFlagDescription     = `Optional. Don't ask about configuring alternate sort keys.`", "new": " Must be of the format '<keyName>:<dataType>'.`\n \tstorageSortKeyFlagDescription = `Optional. Sort key for the DDB table.\n Must be of the format '<keyName>:<dataType>'.`\n+\tstorageNoSortFlagDescription    = \"Optional. Skip configuring sort keys.\"\n \tstorageAttributeFlagDescription = `Optional. Attributes for a DDB table.\n Must be of the format '<name>:<dataType>'. Can be specified multiple times.`\n \tstorageNoLsiFlagDescription     = `Optional. Don't ask about configuring alternate sort keys.`", "review": "Maybe ```suggestion storageNoSortFlagDescription = \"Optional. Skip configuring sort keys.\" ```", "dataset": "codereview", "original_id": 1302, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nMust be of the format '<keyName>:<dataType>'.`\n\tstorageSortKeyFlagDescription = `Optional. Sort key for the DDB table.\nMust be of the format '<keyName>:<dataType>'.`\n\tstorageNoSortFlagDescription    = \"Optional. Skip configuring sort keys.\"\n\tstorageAttributeFlagDescription = `Optional. Attributes for a DDB table.\nMust be of the format '<name>:<dataType>'. Can be specified multiple times.`\n\tstorageNoLsiFlagDescription     = `Optional. Don't ask about configuring alternate sort keys.`\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n Must be of the format '<keyName>:<dataType>'.` \n storageSortKeyFlagDescription = `Optional. Sort key for the DDB table. \n Must be of the format '<keyName>:<dataType>'.` \n storageNoSortFlagDescription    = \"Optional. Skip configuring sort keys.\" \n storageAttributeFlagDescription = `Optional. Attributes for a DDB table. \n Must be of the format '<name>:<dataType>'. Can be specified multiple times.` \n storageNoLsiFlagDescription     = `Optional. Don't ask about configuring alternate sort keys.`", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 292, "old": "           } else if (lhs.attr == system::type_atom::value) {\n             result_type result;\n             for (auto& [part_id, part_syn] : partition_synopses_)\n-              for (auto& pair : part_syn) {\n-                VAST_INFO(this, \"checking\", pair.first.name());\n                 if (evaluate(pair.first.name(), x.op, d)) {\n                   result.push_back(part_id);\n                   break;\n                 }\n-              }\n             return result;\n           }\n           VAST_WARNING(this, \"cannot process attribute extractor:\", lhs.attr);", "new": "           } else if (lhs.attr == system::type_atom::value) {\n             result_type result;\n             for (auto& [part_id, part_syn] : partition_synopses_)\n+              for (auto& pair : part_syn)\n                 if (evaluate(pair.first.name(), x.op, d)) {\n                   result.push_back(part_id);\n                   break;\n                 }\n             return result;\n           }\n           VAST_WARNING(this, \"cannot process attribute extractor:\", lhs.attr);", "review": "Debugging output (and braces).", "dataset": "codereview", "original_id": 1307, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n          } else if (lhs.attr == system::type_atom::value) {\n            result_type result;\n            for (auto& [part_id, part_syn] : partition_synopses_) {\n              for (auto& pair : part_syn) {\n                VAST_DEBUG(this, \"checking\", pair.first.name());\n                if (evaluate(pair.first.name(), x.op, d)) {\n                  result.push_back(part_id);\n                  break;\n                }\n              }\n            }\n            return result;\n          } else {\n            VAST_WARNING(this, \"cannot process attribute extractor:\", lhs.attr);\n          }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 76.97932, "model_code": "\n } else if (lhs.attr == system::type_atom::value) { \n result_type result; \n for (auto& [part_id, part_syn] : partition_synopses_) { \n VAST_INFO(this, \"checking\", pair.first.name()); \n if (evaluate(pair.first.name(), x.op, d)) { \n result.push_back(part_id); \n break; \n } \n } \n return result; \n } \n VAST_WARNING(this, \"cannot process attribute extractor:\", lhs.attr);", "model_em_trim": 0, "model_bleu_trim": 81.58318428186219, "error_type_gpt": "Need Domain Knowledge"}
{"id": 293, "old": "     @Nonnull\n     public static synchronized JetInstance getInstance() {\n         if (supplier == null) {\n-            supplier = memoizeConcurrent(JetBootstrap::createStandaloneInstance);\n         }\n         return supplier.get();\n     }", "new": "     @Nonnull\n     public static synchronized JetInstance getInstance() {\n         if (supplier == null) {\n+            supplier = new ConcurrentMemoizingSupplier<>(() -> new InstanceProxy(createStandaloneInstance()));\n         }\n         return supplier.get();\n     }", "review": "we may need to disable DiscoveryConfig too by setting an empty config `join.setDiscoveryConfig(new DiscoveryConfig());` And disable advanced network config `hzconfig.getAdvancedNetworkConfig().setEnabled(false);`", "dataset": "codereview", "original_id": 1345, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n@Nonnull\npublic static synchronized JetInstance getInstance() {\n    if (supplier == null) {\n        Config hzconfig = new Config();\n        JoinConfig join = hzconfig.getNetworkConfig().getJoin();\n        join.getMulticastConfig().setEnabled(false);\n        join.getTcpIpConfig().setEnabled(true).addMember(\"127.0.0.1\");\n        join.setDiscoveryConfig(new DiscoveryConfig());\n        hzconfig.getAdvancedNetworkConfig().setEnabled(false);\n        supplier = Jet.newJetInstance(hzconfig);\n    }\n    return supplier.get();\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 27.34081, "model_code": "\n @Nonnull \n public static synchronized JetInstance getInstance() { \n if (supplier == null) { \n supplier = memoizeConcurrent(JetBootstrap::createStandaloneInstance, true); \n } \n return supplier.get(); \n }", "model_em_trim": 0, "model_bleu_trim": 62.38125581450584}
{"id": 294, "old": " \t\tkeyUsages |= x509.KeyUsageCertSign\n \t}\n-\texpireTime := time.Now().Add(defaultNotAfter)\n-\tmetaExpireTime := metav1.NewTime(expireTime)\n-\tcrt.Status.NotAfter = &metaExpireTime\n-\n \treturn &x509.Certificate{\n \t\tVersion:               3,\n \t\tBasicConstraintsValid: true,", "new": " \t\tkeyUsages |= x509.KeyUsageCertSign\n \t}\n \treturn &x509.Certificate{\n \t\tVersion:               3,\n \t\tBasicConstraintsValid: true,", "review": "Functions in this package should not be modifying the certificate itself  ", "dataset": "codereview", "original_id": 1356, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n\t\tkeyUsages |= x509.KeyUsageCertSign\n\t}\n\texpireTime := time.Now().Add(defaultNotAfter)\n\tmetaExpireTime := metav1.NewTime(expireTime)\n\n\tnewCert := &x509.Certificate{\n\t\tVersion:               3,\n\t\tBasicConstraintsValid: true,\n\t}\n\n\treturn newCert, metaExpireTime\n", "gpt_em_trim": 0, "gpt_bleu_trim": 35.09483, "model_code": "\n keyUsages |= x509.KeyUsageCertSign \n } \n return &x509.Certificate{ \n Version:               3, \n BasicConstraintsValid: true,", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Insignificant Omission"}
{"id": 295, "old": " \t\t\tforeach ( $files as $file ) {\n \t\t\t\trocket_rrmdir( $file, get_rocket_i18n_to_preserve( $lang ) );\n \t\t\t}\n-\t\t} catch ( \\InvalidArgumentException $e ) {\n \t\t\t// No logging yet.\n \t\t\treturn;\n \t\t}", "new": " \t\t\tforeach ( $files as $file ) {\n \t\t\t\trocket_rrmdir( $file, get_rocket_i18n_to_preserve( $lang ) );\n \t\t\t}\n+\t\t} catch ( InvalidArgumentException $e ) {\n \t\t\t// No logging yet.\n \t\t\treturn;\n \t\t}", "review": "```suggestion } catch ( InvalidArgumentException $e ) { ```", "dataset": "codereview", "original_id": 1380, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\n\t\ttry {\n\t\t\tforeach ( $files as $file ) {\n\t\t\t\trocket_rrmdir( $file, get_rocket_i18n_to_preserve( $lang ) );\n\t\t\t}\n\t\t} catch ( \\InvalidArgumentException $e ) {\n\t\t\t// No logging yet.\n\t\t\treturn;\n\t\t}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 91.1539, "model_code": "\n foreach ( $files as $file ) { \n rocket_rrmdir( $file, get_rocket_i18n_to_preserve( $lang ) ); \n } \n } catch ( InvalidArgumentException $e ) { \n // No logging yet. \n return; \n }", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 296, "old": " import com.alibaba.nacos.api.naming.pojo.Instance;\n import com.alibaba.nacos.api.naming.pojo.ListView;\n import com.alibaba.nacos.api.naming.pojo.ServiceInfo;\n import java.util.List;", "new": " import com.alibaba.nacos.api.naming.pojo.Instance;\n import com.alibaba.nacos.api.naming.pojo.ListView;\n import com.alibaba.nacos.api.naming.pojo.ServiceInfo;\n+import com.alibaba.nacos.api.selector.AbstractSelector;\n import java.util.List;", "review": "Why should the first letter be changed to lowercase?", "dataset": "codereview", "original_id": 1402, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\nimport com.alibaba.nacos.api.naming.pojo.Instance;\nimport com.alibaba.nacos.api.naming.pojo.ListView;\nimport com.alibaba.nacos.api.naming.pojo.ServiceInfo;\nimport java.util.List;\n", "gpt_em_trim": 0, "gpt_bleu_trim": 77.85409, "model_code": "\n import com.alibaba.nacos.api.naming.pojo.Instance; \n import com.alibaba.nacos.api.naming.pojo.ListView; \n import com.alibaba.nacos.api.naming.pojo.ServiceInfo; \n import java.util.Collections; \n import java.util.List;", "model_em_trim": 0, "model_bleu_trim": 81.07302390423501}
{"id": 297, "old": " #include <stdio.h>\n #include <sys/stat.h>\n #include <iostream>\n-#include <nonstd/optional.hpp>\n std::string id_to_name(uint32_t id) {\n   std::string new_id(16, '\\0');", "new": " #include <stdio.h>\n #include <sys/stat.h>\n #include <iostream>\n std::string id_to_name(uint32_t id) {\n   std::string new_id(16, '\\0');", "review": "Can it be `nonstd::optional`?", "dataset": "codereview", "original_id": 1421, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n#include <stdio.h>\n#include <sys/stat.h>\n#include <iostream>\n#include <nonstd/optional.hpp>\n\nnonstd::optional<std::string> id_to_name(uint32_t id) {\n  std::string new_id(16, '\\0');\n  // code logic here\n  return new_id;\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 73.09974, "model_code": "\n #include <stdio.h> \n #include <sys/stat.h> \n #include <iostream> \n #include <nonstd::optional.hpp> \n std::string id_to_name(uint32_t id) { \n std::string new_id(16, '\\0');", "model_em_trim": 0, "model_bleu_trim": 82.69492220577109}
{"id": 298, "old": " )\n @click.option(\n     '--blotter',\n-    type=str,\n     default='default',\n     help=\"The blotter to use.\",\n     show_default=True,", "new": " )\n @click.option(\n     '--blotter',\n     default='default',\n     help=\"The blotter to use.\",\n     show_default=True,", "review": "does this do anything?", "dataset": "codereview", "original_id": 1431, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n)\n@click.option(\n    '--blotter',\n    type=str,\n    default='default',\n    help=\"The blotter to use.\",\n    show_default=True,  # Show the default value in the help message\n    # The following option is used to specify the blotter to use for trading\n    # It defaults to 'default' if not specified\n    # The blotter is responsible for managing orders and positions\n    # and is an important component of the trading system\n)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 81.06798, "model_code": "\n ) \n @click.option( \n '--blotter', \n default='default', \n help=\"The blotter to use.\", \n show_default=True,", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 299, "old": "   convertOps.setProgress(\n     Progress(\n       ConfigOptions().getJobId(), JOB_SOURCE, Progress::JobState::Running,\n-      (_currentTaskNum - 1) / _numTotalTasks, 1.0 / _numTotalTasks));\n   convertOps.apply(fullMap);\n   // get back into wgs84 in case some op changed the proj\n   MapProjector::projectToWgs84(fullMap);", "new": "   convertOps.setProgress(\n     Progress(\n       ConfigOptions().getJobId(), JOB_SOURCE, Progress::JobState::Running,\n+      (float)(_currentTaskNum - 1) / (float)_numTotalTasks, 1.0f / (float)_numTotalTasks));\n+\n   convertOps.apply(fullMap);\n   // get back into wgs84 in case some op changed the proj\n   MapProjector::projectToWgs84(fullMap);", "review": "The problem that is flagged here is that `1.0` is actually a `double` and not a `float`. The correct way to fix this is to do the following: ``` (float)(_currentTaskNum - 1) / (float)_numTotalTasks, 1.0f / (float)_numTotalTasks)); ```", "dataset": "codereview", "original_id": 1436, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nconvertOps.setProgress(\n    Progress(\n      ConfigOptions().getJobId(), JOB_SOURCE, Progress::JobState::Running,\n      (float)(_currentTaskNum - 1) / (float)_numTotalTasks, 1.0f / (float)_numTotalTasks));\n  convertOps.apply(fullMap);\n  // get back into wgs84 in case some op changed the proj\n  MapProjector::projectToWgs84(fullMap);\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n convertOps.setProgress( \n Progress( \n ConfigOptions().getJobId(), JOB_SOURCE, Progress::JobState::Running, \n (float)(_currentTaskNum - 1) / (float)_numTotalTasks, 1.0f / (float)_numTotalTasks)); \n convertOps.apply(fullMap); \n // get back into wgs84 in case some op changed the proj \n MapProjector::projectToWgs84(fullMap);", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 300, "old": " # Cell\n class AzureMLCallback(Callback):\n     \"Log losses, metrics, model architecture summary to AzureML\"\n     def before_fit(self):\n         self.run = Run.get_context()", "new": " # Cell\n class AzureMLCallback(Callback):\n     \"Log losses, metrics, model architecture summary to AzureML\"\n+    order = Recorder.order+1\n     def before_fit(self):\n         self.run = Run.get_context()", "review": "Add this line here to fix your smooth loss problem: ```python order = Recorder.order+1 ```", "dataset": "codereview", "original_id": 1461, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\n# Cell\nclass AzureMLCallback(Callback):\n    \"Log losses, metrics, model architecture summary to AzureML\"\n    def before_fit(self):\n        order = Recorder.order+1\n        self.run = Run.get_context()\n", "gpt_em_trim": 0, "gpt_bleu_trim": 93.1428, "model_code": "\n # Cell \n class AzureMLCallback(Callback): \n \"\"\"Log losses, metrics, model architecture summary to AzureML\"\"\" \n def before_fit(self): \n self.run = Run.get_context()", "model_em_trim": 0, "model_bleu_trim": 79.37121977912736, "error_type_gpt": "Unclear Location"}
{"id": 301, "old": " \t}\n }\n-func newRemapCRDVersionBackupItemAction(f client.Factory) veleroplugin.HandlerInitializer {\n \treturn func(logger logrus.FieldLogger) (interface{}, error) {\n \t\tconfig, err := f.ClientConfig()\n \t\tif err != nil {", "new": " \t}\n }\n+func newRemapCRDVersionAction(f client.Factory) veleroplugin.HandlerInitializer {\n \treturn func(logger logrus.FieldLogger) (interface{}, error) {\n \t\tconfig, err := f.ClientConfig()\n \t\tif err != nil {", "review": "I think we don't have to make the function naming change to address this issue.", "dataset": "codereview", "original_id": 1473, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\n\t}\n}\nfunc remapCRDVersionBackupItemAction(f client.Factory) veleroplugin.HandlerInitializer {\n\treturn func(logger logrus.FieldLogger) (interface{}, error) {\n\t\tconfig, err := f.ClientConfig()\n\t\tif err != nil {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 92.82469, "model_code": "\n } \n } \n func newRemapCRDVersionBackupItemAction(f client.Factory) veleroplugin.HandlerInitializer { \n return func(logger logrus.FieldLogger) (interface{}, error) { \n config, err := f.ClientConfig() \n if err != nil {", "model_em_trim": 0, "model_bleu_trim": 94.39487258532452, "error_type_gpt": "Need Domain Knowledge"}
{"id": 302, "old": " \tif level > c.Level {\n \t\treturn nil\n \t}\n-\n-\tlogTimeStr := formatLogTime(when)\n-\tmsg = logTimeStr + msg\n-\n \tif goos := runtime.GOOS; goos == \"windows\" {\n \t\tc.lg.Println(msg)\n \t\treturn nil", "new": " \tif level > c.Level {\n \t\treturn nil\n \t}\n+\tmsg = formatLogTime(when) + msg\n \tif goos := runtime.GOOS; goos == \"windows\" {\n \t\tc.lg.Println(msg)\n \t\treturn nil", "review": "log package will write the now time,your log message will be like this 2016-01-23 23:23:00 2016-01-23 23:23:00 ....", "dataset": "codereview", "original_id": 1489, "relevance": "not", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Feature-modify", "gpt_code": "\nif level > c.Level {\n    return nil\n}\n\nlogTimeStr := formatLogTime(when)\nmsg = logTimeStr + \" \" + msg // Add a space between logTimeStr and msg\n\nif goos := runtime.GOOS; goos == \"windows\" {\n    c.lg.Println(msg)\n    return nil\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 64.21279, "model_code": "\n if level > c.Level { \n return nil \n } \n if goos := runtime.GOOS; goos == \"windows\" { \n c.lg.Println(msg) \n return nil", "model_em_trim": 0, "model_bleu_trim": 76.45615040274637}
{"id": 303, "old": " require 'test_helper'\n-require 'pp'\n class UsersControllerTest < ActionDispatch::IntegrationTest\n   setup do", "new": " require 'test_helper'\n class UsersControllerTest < ActionDispatch::IntegrationTest\n   setup do", "review": "we can remove this right?", "dataset": "codereview", "original_id": 1495, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nrequire 'test_helper'\n\nclass UsersControllerTest < ActionDispatch::IntegrationTest\n  setup do\n    # code setup here\n  end\n\n  # test cases here\nend\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n require 'test_helper' \n class UsersControllerTest < ActionDispatch::IntegrationTest \n setup do", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 304, "old": "             var owinRequestBody = Get<Stream>(environment, \"owin.RequestBody\");\n             var clientCertificate = Get<X509Certificate>(environment, \"ssl.ClientCertificate\");\n-            var clientCertificateBytes = clientCertificate == null ? null : clientCertificate.GetRawCertData();\n-            \n             var serverClientIp = Get<string>(environment, \"server.RemoteIpAddress\");\n             //var callCancelled = Get<CancellationToken>(environment, \"owin.RequestBody\");", "new": "             var owinRequestBody = Get<Stream>(environment, \"owin.RequestBody\");\n             var clientCertificate = Get<X509Certificate>(environment, \"ssl.ClientCertificate\");\n+            var certificate = (clientCertificate == null) ? null : clientCertificate.GetRawCertData();\n+\n             var serverClientIp = Get<string>(environment, \"server.RemoteIpAddress\");\n             //var callCancelled = Get<CancellationToken>(environment, \"owin.RequestBody\");", "review": "Wrap `clientCertificate == null` in parenthesis", "dataset": "codereview", "original_id": 1501, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\n            var owinRequestBody = Get<Stream>(environment, \"owin.RequestBody\");\n            var clientCertificate = Get<X509Certificate>(environment, \"ssl.ClientCertificate\");\n            var clientCertificateBytes = (clientCertificate == null) ? null : clientCertificate.GetRawCertData();\n            \n            var serverClientIp = Get<string>(environment, \"server.RemoteIpAddress\");\n            //var callCancelled = Get<CancellationToken>(environment, \"owin.RequestBody\");\n", "gpt_em_trim": 0, "gpt_bleu_trim": 97.11402, "model_code": "\n var owinRequestBody = Get<Stream>(environment, \"owin.RequestBody\"); \n var clientCertificate = Get<X509Certificate>(environment, \"ssl.ClientCertificate\"); \n var clientCertificateBytes = clientCertificate == null ? null : clientCertificate.GetRawCertData(); \n var serverClientIp = Get<string>(environment, \"server.RemoteIpAddress\"); \n //var callCancelled = Get<CancellationToken>(environment, \"owin.RequestBody\");", "model_em_trim": 0, "model_bleu_trim": 92.20634467042252}
{"id": 305, "old": "             Console.WriteLine(message);\n         }\n-        public void LogHeader1(string message)\n         {\n             Console.WriteLine($\"=============== {message} ================ \");\n         }\n-        public void LogHeader2(string message)\n         {\n             Console.WriteLine($\"***{message}***\");\n         }", "new": "             Console.WriteLine(message);\n         }\n+        public void LogIterationHeader(string message)\n         {\n             Console.WriteLine($\"=============== {message} ================ \");\n         }\n+        public void LogStepHeader(string message)\n         {\n             Console.WriteLine($\"***{message}***\");\n         }", "review": "1) These names are not great, doesn't help explain what they do. 2) Instead of methods like this that hard code some number of = and *, why not a method that takes the string, defaults to a char to surround with, and a width? 3) Do we really even need headers like this in the output? Maybe we ditch them.", "dataset": "codereview", "original_id": 1514, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\npublic void Log(string message)\n{\n    Console.WriteLine(message);\n}\n\npublic void LogWithSurroundingChars(string message, char surroundingChar = '=', int width = 15)\n{\n    string header = new string(surroundingChar, width);\n    Console.WriteLine($\"{header} {message} {header}\");\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 29.86303, "model_code": "\n Console.WriteLine(message); \n } \n public void LogHeader(string message) \n { \n Console.WriteLine($\"=============== {message} ================ \"); \n } \n public void LogMessage(string message) \n { \n Console.WriteLine($\"***{message}***\"); \n }", "model_em_trim": 0, "model_bleu_trim": 94.14135870469757, "error_type_gpt": "Need Domain Knowledge"}
{"id": 306, "old": " \t\"github.com/ethersphere/bee/pkg/logging\"\n \t\"github.com/ethersphere/bee/pkg/node\"\n-\t\"github.com/ethersphere/bee/pkg/resolver/service\"\n \t\"github.com/ethersphere/bee/pkg/swarm\"\n \t\"github.com/sirupsen/logrus\"\n \t\"github.com/spf13/cobra\"", "new": " \t\"github.com/ethersphere/bee/pkg/logging\"\n \t\"github.com/ethersphere/bee/pkg/node\"\n+\tresolverSvc \"github.com/ethersphere/bee/pkg/resolver/service\"\n \t\"github.com/ethersphere/bee/pkg/swarm\"\n \t\"github.com/sirupsen/logrus\"\n \t\"github.com/spf13/cobra\"", "review": "Importing this package with a service name, makes the code less readable, then naming the import more concretely. Like `resolverService`, even there is currently no other service import.", "dataset": "codereview", "original_id": 1515, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\n\t\"github.com/ethersphere/bee/pkg/logging\"\n\t\"github.com/ethersphere/bee/pkg/node\"\n\t\"github.com/ethersphere/bee/pkg/resolver\"\n\t\"github.com/ethersphere/bee/pkg/swarm\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/spf13/cobra\"\n", "gpt_em_trim": 0, "gpt_bleu_trim": 91.61973, "model_code": "\n \"github.com/ethersphere/bee/pkg/logging\" \n \"github.com/ethersphere/bee/pkg/node\" \n \"github.com/ethersphere/bee/pkg/swarm\" \n \"github.com/sirupsen/logrus\" \n \"github.com/spf13/cobra\"", "model_em_trim": 0, "model_bleu_trim": 74.85427120216951, "error_type_gpt": "Need Domain Knowledge"}
{"id": 307, "old": " \t// Run again if we have a release version as well\n \tif releaseSubstring != \"\" {\n \t\trawComponents = componentRegex.FindAllString(releaseSubstring, -1)\n-\t\t// If no epoch is set in the version, apply an epoch of 0 so all versions have one.\n-\t\tif epochComponentRegex.FindString(releaseSubstring) == \"\" {\n-\t\t\trawComponents = append([]string{\"0\"}, rawComponents...)\n-\t\t}\n-\n \t\tv.releaseComponents = make([]uint64, len(rawComponents))\n \t\tfor i := range rawComponents {\n \t\t\t// Base36 to support lowercase characters", "new": " \t// Run again if we have a release version as well\n \tif releaseSubstring != \"\" {\n \t\trawComponents = componentRegex.FindAllString(releaseSubstring, -1)\n \t\tv.releaseComponents = make([]uint64, len(rawComponents))\n \t\tfor i := range rawComponents {\n \t\t\t// Base36 to support lowercase characters", "review": "Can a release have a separate epoch? I think this is just for parsing the `***-<subrelease>` part of the version.", "dataset": "codereview", "original_id": 1523, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n\t// Run again if we have a release version as well\n\tif releaseSubstring != \"\" {\n\t\trawComponents = componentRegex.FindAllString(releaseSubstring, -1)\n\t\t// If no epoch is set in the subrelease, apply an epoch of 0 so all subreleases have one.\n\t\tif epochComponentRegex.FindString(releaseSubstring) == \"\" {\n\t\t\trawComponents = append([]string{\"0\"}, rawComponents...)\n\t\t}\n\n\t\tv.releaseComponents = make([]uint64, len(rawComponents))\n\t\tfor i := range rawComponents {\n\t\t\t// Base36 to support lowercase characters\n", "gpt_em_trim": 0, "gpt_bleu_trim": 50.98325, "model_code": "\n // Run again if we have a release version as well \n if releaseSubstring != \"\" { \n rawComponents = componentRegex.FindAllString(releaseSubstring, -1) \n v.releaseComponents = make([]uint64, len(rawComponents)) \n for i := range rawComponents { \n // Base36 to support lowercase characters", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Need Domain Knowledge"}
{"id": 308, "old": "             Job job = jet.newJob(pipeline);\n             assertJobStatusEventually(job, RUNNING);\n-            // and connection is cut\n             MILLISECONDS.sleep(ThreadLocalRandom.current().nextInt(0, 500));\n             proxy.setConnectionCut(true);\n             // and some time passes", "new": "             Job job = jet.newJob(pipeline);\n             assertJobStatusEventually(job, RUNNING);\n+            // and snapshotting is ongoing (we have no exact way of identifying\n+            // the moment, but random sleep will catch it at least some of the time)\n             MILLISECONDS.sleep(ThreadLocalRandom.current().nextInt(0, 500));\n+\n+            // and connection is cut\n             proxy.setConnectionCut(true);\n             // and some time passes", "review": "All tests in this class are NightlyTest. Can we move this category to class level to have it more clear?", "dataset": "codereview", "original_id": 1530, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n@Category(NightlyTest.class)\npublic class MyTestClass {\n    // existing code here\n\n    @Test\n    public void myTest() throws Exception {\n        // updated code here\n        Job job = jet.newJob(pipeline);\n        assertJobStatusEventually(job, RUNNING);\n        // and connection is cut\n        MILLISECONDS.sleep(ThreadLocalRandom.current().nextInt(0, 500));\n        proxy.setConnectionCut(true);\n        // and some time passes\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 53.63874, "model_code": "\n Job job = jet.newJob(pipeline); \n assertJobStatusEventually(job, RUNNING); \n // and connection is cut \n proxy.setConnectionCut(true); \n // and some time passes", "model_em_trim": 0, "model_bleu_trim": 25.88901725686173}
{"id": 309, "old": "         if (hash) {\n           String encodedKey = \"\";\n           try {\n-            byte[] encodedBytes = MessageDigest.getInstance(Constants.NON_CRYPTO_USE_HASH_ALGORITHM)\n                 .digest(entry.getKey().getBytes(UTF_8));\n             encodedKey = new String(encodedBytes, UTF_8);\n           } catch (NoSuchAlgorithmException e) {\n-            out.println(\"Failed to convert key to \" + Constants.NON_CRYPTO_USE_HASH_ALGORITHM\n-                + \" hash: \" + e.getMessage());\n           }\n           out.printf(\"%-20s\", encodedKey.substring(0, 8));\n         } else", "new": "         if (hash) {\n           String encodedKey = \"\";\n           try {\n+            byte[] encodedBytes = MessageDigest.getInstance(KEY_HASH_ALGORITHM)\n                 .digest(entry.getKey().getBytes(UTF_8));\n             encodedKey = new String(encodedBytes, UTF_8);\n           } catch (NoSuchAlgorithmException e) {\n+            out.println(\n+                \"Failed to convert key to \" + KEY_HASH_ALGORITHM + \" hash: \" + e.getMessage());\n           }\n           out.printf(\"%-20s\", encodedKey.substring(0, 8));\n         } else", "review": "Since this use of the digest is specific to this specific metric utility's serialization, we can probably just use a local constant, rather than one in `Constants.java` (which I'd personally like to phase out over time).", "dataset": "codereview", "original_id": 1545, "relevance": "partial", "relevance_user1": "perfect", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Documentation-conventions", "category_type_user2": "Feature-modify", "gpt_code": "\n        if (hash) {\n          String encodedKey = \"\";\n          try {\n            MessageDigest digest = MessageDigest.getInstance(\"SHA-256\");\n            byte[] encodedBytes = digest.digest(entry.getKey().getBytes(UTF_8));\n            encodedKey = new String(encodedBytes, UTF_8);\n          } catch (NoSuchAlgorithmException e) {\n            out.println(\"Failed to convert key to SHA-256 hash: \" + e.getMessage());\n          }\n          out.printf(\"%-20s\", encodedKey.substring(0, 8));\n        } else\n", "gpt_em_trim": 0, "gpt_bleu_trim": 83.07952, "model_code": "\n if (hash) { \n String encodedKey = \"\"; \n try { \n byte[] encodedBytes = MessageDigest.getInstance(NON_CRYPTO_USE_HASH_ALGORITHM) \n .digest(entry.getKey().getBytes(UTF_8)); \n encodedKey = new String(encodedBytes, UTF_8); \n } catch (NoSuchAlgorithmException e) { \n out.println(\"Failed to convert key to \" + NON_CRYPTO_USE_HASH_ALGORITHM \n + \" hash: \" + e.getMessage()); \n } \n out.printf(\"%-20s\", encodedKey.substring(0, 8)); \n } else", "model_em_trim": 0, "model_bleu_trim": 89.50586232064492}
{"id": 310, "old": "     public function up()\n     {\n         Schema::table('settings', function (Blueprint $table) {\n-\t\t$table->tinyInteger('labels_display_model')->default(1);\n         });\n     }", "new": "     public function up()\n     {\n         Schema::table('settings', function (Blueprint $table) {\n+\t\t$table->boolean('labels_display_model')->default(0);\n         });\n     }", "review": "This should probably be boolean, with a default of 0, so we don't mess up other people's existing settings.", "dataset": "codereview", "original_id": 1565, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n    public function up()\n    {\n        Schema::table('settings', function (Blueprint $table) {\n            $table->boolean('labels_display_model')->default(false);\n        });\n    }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 94.51792, "model_code": "\n public function up() \n { \n Schema::table('settings', function (Blueprint $table) { \n $table->tinyInteger('labels_display_model')->default(false); \n }); \n }", "model_em_trim": 0, "model_bleu_trim": 88.9598733318541, "error_type_gpt": "Unclear Changes"}
{"id": 311, "old": "     def train(self, mode=True):\n         super(ResNet, self).train(mode)\n         if mode and self.norm_eval:\n-            for mod in self.modules():\n                 # trick: eval have effect on BatchNorm only\n-                if isinstance(self, nn.BatchNorm2d):\n-                    mod.eval()", "new": "     def train(self, mode=True):\n         super(ResNet, self).train(mode)\n         if mode and self.norm_eval:\n+            for m in self.modules():\n                 # trick: eval have effect on BatchNorm only\n+                if isinstance(m, nn.BatchNorm2d):\n+                    m.eval()", "review": "If we set eval mode in `build_norm_layer`, then this method is not necessary any more.", "dataset": "codereview", "original_id": 1581, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n    def train(self, mode=True):\n        super(ResNet, self).train(mode)\n        if mode and self.norm_eval:\n            for mod in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(mod, nn.BatchNorm2d):\n                    mod.eval()\n", "gpt_em_trim": 0, "gpt_bleu_trim": 83.71706, "model_code": "\n def train(self, mode=True): \n super(ResNet, self).train(mode) \n for mod in self.modules(): \n # trick: eval have effect on BatchNorm only \n if isinstance(self, nn.BatchNorm2d): \n mod.eval()", "model_em_trim": 0, "model_bleu_trim": 71.7505960243548}
{"id": 312, "old": " \t\tpublic SqlType[] GetParametersTypeArray()\n \t\t{\n-\t\t\treturn columns.Values.OfType<SqlType>().ToArray();\n \t\t}\n \t}\n }", "new": " \t\tpublic SqlType[] GetParametersTypeArray()\n \t\t{\n+\t\t\treturn new SafetyEnumerable<SqlType>(columns.Values).ToArray();\n \t\t}\n \t}\n }", "review": "There is a subtle difference between `SafetyEnumerable` and `OfType`. `SafetyEnumerable` will not filter out nulls, while `OfType` will. Not sure how this is relevant here.", "dataset": "codereview", "original_id": 1599, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\npublic SqlType[] GetParametersTypeArray()\n{\n    return columns.Values.Where(c => c is SqlType).Cast<SqlType>().ToArray();\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 50.34804, "model_code": "\n public SqlType[] GetParametersTypeArray() \n { \n return columns.Values.Cast<SqlType>().ToArray(); \n } \n } \n }", "model_em_trim": 0, "model_bleu_trim": 73.54117869568651, "error_type_gpt": "Need Domain Knowledge"}
{"id": 313, "old": "     static NewSyncConfig from(const SyncConfig& config)\n     {\n-        auto type = SyncConfig::TYPE_TWOWAY;\n-        if (config.isUpSync() && !config.isDownSync())\n-        {\n-            type = SyncConfig::TYPE_UP;\n-        }\n-        else if (!config.isUpSync() && config.isDownSync())\n-        {\n-            type = SyncConfig::TYPE_DOWN;\n-        }\n-        return NewSyncConfig{type, config.syncDeletions(), config.forceOverwrite()};\n     }\n };", "new": "     static NewSyncConfig from(const SyncConfig& config)\n     {\n+        return NewSyncConfig{config.getType(), config.syncDeletions(), config.forceOverwrite()};\n     }\n };", "review": "Would it make sense to add `SyncConfig::Type SyncConfig::getType()`?? and avoid the if-else-if above, which I've already seen in other parts of the code. And perhaps a convenience method for `string SyncConfig::getTypeStr()` or similar, which could be used at `syncConfigToString()` and at other places in the code, probably. Just thinking loud :)", "dataset": "codereview", "original_id": 1649, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Documentation-and-code", "category_type_user2": "Feature-modify", "gpt_code": "\nstatic NewSyncConfig from(const SyncConfig& config)\n{\n    auto type = config.getType();\n    return NewSyncConfig{type, config.syncDeletions(), config.forceOverwrite()};\n}\n```\n\nAnd here's the updated `SyncConfig` class with the added methods:\n\n```\nclass SyncConfig {\npublic:\n    enum Type {\n        TYPE_TWOWAY,\n        TYPE_UP,\n        TYPE_DOWN\n    };\n\n    Type getType() const {\n        if (isUpSync() && !isDownSync()) {\n            return TYPE_UP;\n        } else if (!isUpSync() && isDownSync()) {\n            return TYPE_DOWN;\n        } else {\n            return TYPE_TWOWAY;\n        }\n    }\n\n    std::string getTypeStr() const {\n        switch (getType()) {\n            case TYPE_UP:\n                return \"up\";\n            case TYPE_DOWN:\n                return \"down\";\n            case TYPE_TWOWAY:\n            default:\n                return \"twoway\";\n        }\n    }\n\n    bool isUpSync() const {\n        // implementation\n    }\n\n    bool isDownSync() const {\n        // implementation\n    }\n\n    bool syncDeletions() const {\n        // implementation\n    }\n\n    bool forceOverwrite() const {\n        // implementation\n    }\n};\n", "gpt_em_trim": 0, "gpt_bleu_trim": 74.21321, "model_code": "\n static NewSyncConfig from(const SyncConfig& config) \n { \n return NewSyncConfig{SyncConfig::TYPE_TWOWAY, config.syncDeletions(), config.forceOverwrite()}; \n } \n };", "model_em_trim": 0, "model_bleu_trim": 78.5539025522397, "error_type_gpt": "Need Domain Knowledge"}
{"id": 314, "old": "             self.buffer = self.buffer[1:]\n     def next_until(self, target):\n-        \"\"\"Keep iterating the NEXUS file until it reaches a target character.\n-\n-        Returns the word found in the NEXUS file.\n-        \"\"\"\n         for t in target:\n             try:\n                 pos = self.buffer.index(t)", "new": "             self.buffer = self.buffer[1:]\n     def next_until(self, target):\n+        \"\"\"Iterate over the NEXUS file until a target character is reached.\"\"\"\n         for t in target:\n             try:\n                 pos = self.buffer.index(t)", "review": "This wording was odd before your change, but how about \"Iterate over the NEXUS file until a target character is reached.\"", "dataset": "codereview", "original_id": 1682, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n    def next_until(self, target):\n        \"\"\"Iterate over the NEXUS file until a target character is reached.\n\n        Returns the word found in the NEXUS file.\n        \"\"\"\n        for t in target:\n            try:\n                pos = self.buffer.index(t)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 79.1583, "model_code": "\n self.buffer = self.buffer[1:] \n def next_until(self, target): \n \"\"\"Iterate over the NEXUS file until a target character is reached.\"\"\" \n for t in target: \n try: \n pos = self.buffer.index(t)", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Insignificant Omission"}
{"id": 315, "old": " \t\t\t'pricing-transient' => false,\n \t\t\t'timeout-active'    => false,\n \t\t\t'timeout-duration'  => rocket_get_constant( 'DAY_IN_SECONDS' )\n-\t\t\t\t\t\t\t\t   - rocket_get_constant( 'HOuR_IN_SECONDS' ),\n \t\t\t'response'          => [ 'code' => 404 ],\n \t\t],\n \t\t'expected' => [", "new": " \t\t\t'pricing-transient' => false,\n \t\t\t'timeout-active'    => false,\n \t\t\t'timeout-duration'  => rocket_get_constant( 'DAY_IN_SECONDS' )\n+\t\t\t\t\t\t\t\t   - rocket_get_constant( 'HOUR_IN_SECONDS' ),\n \t\t\t'response'          => [ 'code' => 404 ],\n \t\t],\n \t\t'expected' => [", "review": "@iCaspar For consistency, shouldn't 'HOuR_IN_SECONDS' be 'HOUR_IN_SECONDS'", "dataset": "codereview", "original_id": 1686, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "\n\t\t\t'pricing-transient' => false,\n\t\t\t'timeout-active'    => false,\n\t\t\t'timeout-duration'  => rocket_get_constant( 'DAY_IN_SECONDS' )\n\t\t\t\t\t\t\t\t   - rocket_get_constant( 'HOUR_IN_SECONDS' ),\n\t\t\t'response'          => [ 'code' => 404 ],\n\t\t],\n\t\t'expected' => [\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n 'pricing-transient' => false, \n 'timeout-active'    => false, \n 'timeout-duration'  => rocket_get_constant( 'DAY_IN_SECONDS' ) \n - rocket_get_constant( 'HOUR_IN_SECONDS' ), \n 'response'          => [ 'code' => 404 ], \n ], \n 'expected' => [", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 316, "old": "          SettingValue(typ.Bool(), 'false'),\n          \"Hide the tabbar if only one tab is open.\"),\n-        ('perm-hide',\n          SettingValue(typ.Bool(), 'false'),\n-         \"Hide permanently.\"),\n         ('wrap',\n          SettingValue(typ.Bool(), 'true'),", "new": "          SettingValue(typ.Bool(), 'false'),\n          \"Hide the tabbar if only one tab is open.\"),\n+        ('always-hide',\n          SettingValue(typ.Bool(), 'false'),\n+         \"Always hide the tabbar.\"),\n         ('wrap',\n          SettingValue(typ.Bool(), 'true'),", "review": "I think `perm-` is a bit confusing. I'd prefer `always-hide`. Alternatively, renaming `auto-hide` to `hide-auto` and `perm-hide` to `hide-always` would sort them correctly. What's your opinion on this?", "dataset": "codereview", "original_id": 1728, "relevance": "partial", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n         SettingValue(typ.Bool(), 'false'),\n         \"Hide the tabbar if only one tab is open.\"),\n        ('hide-always',\n         SettingValue(typ.Bool(), 'false'),\n         \"Hide permanently.\"),\n        ('wrap',\n         SettingValue(typ.Bool(), 'true'),\n         \"Wrap the tabbar to multiple rows if necessary.\"),\n        ('hide-auto',\n         SettingValue(typ.Bool(), 'true'),\n         \"Automatically hide the tabbar if only one tab is open.\")\n", "gpt_em_trim": 0, "gpt_bleu_trim": 87.53869, "model_code": "\n SettingValue(typ.Bool(), 'false'), \n \"Hide the tabbar if only one tab is open.\"), \n ('always-hide', \n SettingValue(typ.Bool(), 'false'), \n \"Hide permanently.\"), \n ('wrap', \n SettingValue(typ.Bool(), 'true'),", "model_em_trim": 0, "model_bleu_trim": 92.81540741266431}
{"id": 317, "old": " def _read_pfm_four_columns(handle):\n-    \"\"\"Read motifs in Cluster Buster position frequency matrix format from a file handle.\n-    #cisbp\n     Pos A   C   G   T\n     1   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846\n     2   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846", "new": " def _read_pfm_four_columns(handle):\n+    \"\"\"Read motifs in position frequency matrix format (4 columns) from a file handle.\n+    # cisbp\n     Pos A   C   G   T\n     1   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846\n     2   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846", "review": "Why remove this URL?", "dataset": "codereview", "original_id": 1750, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\ndef _read_pfm_four_columns(handle):\n    \"\"\"Read motifs in Cluster Buster position frequency matrix format from a file handle.\"\"\"\n    # Pos A   C   G   T\n    # 1   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846\n    # 2   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846\n", "gpt_em_trim": 0, "gpt_bleu_trim": 76.31452, "model_code": "\n def _read_pfm_four_columns(handle): \n \"\"\"Read motifs in Cluster Buster position frequency matrix format from a file handle. \n Pos A   C   G   T \n 1   0.00961538461538462 0.00961538461538462 0.971153846153846 \n 2   0.00961538461538462 0.00961538461538462 0.971153846153846", "model_em_trim": 0, "model_bleu_trim": 73.35199174510659}
{"id": 318, "old": "         </li>\n     <% end %>\n-    <% if presenter.display_hire_me && !presenter.user_same_as?(current_user) %>\n         <li>\n           <%= link_to 'Hire me', user_path(presenter.user),\n                       {remote: true, class: 'user-profile-btn',", "new": "         </li>\n     <% end %>\n+    <% if presenter.display_hire_me?(current_user) %>\n         <li>\n           <%= link_to 'Hire me', user_path(presenter.user),\n                       {remote: true, class: 'user-profile-btn',", "review": "could the `!presenter.user_same_as?(current_user)` be pulled into the presenter.display_hire_me method? no need to expose the logic in the view right? I guess maybe that could be another ticket and PR but then again ... up to you :-)", "dataset": "codereview", "original_id": 1760, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n        </li>\n    <% end %>\n    <% if presenter.display_hire_me?(current_user) %>\n        <li>\n          <%= link_to 'Hire me', user_path(presenter.user),\n                      {remote: true, class: 'user-profile-btn',\n```\n\nAnd in the presenter class, we can add the following method:\n\n```\ndef display_hire_me?(current_user)\n  display_hire_me && !user_same_as?(current_user)\nend\n", "gpt_em_trim": 1, "gpt_bleu_trim": 92.7787, "model_code": "\n </li> \n <% end %> \n <% if presenter.display_hire_me %> \n <li> \n <%= link_to 'Hire me', user_path(presenter.user), \n {remote: true, class: 'user-profile-btn',", "model_em_trim": 0, "model_bleu_trim": 88.11184226712761}
{"id": 319, "old": "                 )\n                 return usd_price\n             except (RemoteError, DeserializationError) as e:\n-                msg = f'Could not find price for {asset}. {str(e)}'\n                 if instance._ethereum is not None:\n                     instance._ethereum.msg_aggregator.add_warning(msg)\n-                return Price(ZERO)\n         return instance._query_oracle_instances(from_asset=asset, to_asset=A_USD)\n     def find_uniswap_v2_lp_price(", "new": "                 )\n                 return usd_price\n             except (RemoteError, DeserializationError) as e:\n+                msg = f'Could not find price for BSQ. {str(e)}'\n                 if instance._ethereum is not None:\n                     instance._ethereum.msg_aggregator.add_warning(msg)\n+                return Price(SATOSHI_PER_BSQ * price_in_btc)\n         return instance._query_oracle_instances(from_asset=asset, to_asset=A_USD)\n     def find_uniswap_v2_lp_price(", "review": "```suggestion msg = f'Could not find price for BSQ. {str(e)}' ``` Save ourselves an unneeded substituion. But also didn't you say it should be 100 sats? Isn't it better to use the \"intended\" price of 100 sats as fallback instead of `0`?", "dataset": "codereview", "original_id": 1763, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n                )\n                return usd_price\n            except (RemoteError, DeserializationError) as e:\n                msg = f'Could not find price for BSQ. {str(e)}'\n                if instance._ethereum is not None:\n                    instance._ethereum.msg_aggregator.add_warning(msg)\n                return Price(100)\n        return instance._query_oracle_instances(from_asset=asset, to_asset=A_USD)\n    def find_uniswap_v2_lp_price(\n", "gpt_em_trim": 0, "gpt_bleu_trim": 85.55251, "model_code": "\n ) \n return usd_price \n except (RemoteError, DeserializationError) as e: \n msg = f'Could not find price for BSQ. {str(e)}' \n if instance._ethereum is not None: \n instance._ethereum.msg_aggregator.add_warning(msg) \n return Price(100) \n return instance._query_oracle_instances(from_asset=asset, to_asset=A_USD) \n def find_uniswap_v2_lp_price(", "model_em_trim": 0, "model_bleu_trim": 87.82119466400698}
{"id": 320, "old": "             sym_g.ndata[key] = g.ndata[key]\n         g = sym_g\n-    profiler = Profiler()\n-    profiler.start()\n     dgl.distributed.partition_graph(g, args.dataset, args.num_parts, 'data',\n                                     part_method=args.part_method,\n                                     balance_ntypes=balance_ntypes,\n                                     balance_edges=args.balance_edges)\n-    profiler.stop()\n-    print(profiler.output_text(unicode=True, color=True))", "new": "             sym_g.ndata[key] = g.ndata[key]\n         g = sym_g\n     dgl.distributed.partition_graph(g, args.dataset, args.num_parts, 'data',\n                                     part_method=args.part_method,\n                                     balance_ntypes=balance_ntypes,\n                                     balance_edges=args.balance_edges)", "review": "Is this still needed?", "dataset": "codereview", "original_id": 1781, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n    profiler = Profiler()\n    profiler.start()\n    dgl.distributed.partition_graph(g, args.dataset, args.num_parts, 'data',\n                                    part_method=args.part_method,\n                                    balance_ntypes=balance_ntypes,\n                                    balance_edges=args.balance_edges)\n    profiler.stop()\n    print(profiler.output_text(unicode=True, color=True))\n", "gpt_em_trim": 0, "gpt_bleu_trim": 72.88555, "model_code": "\n sym_g.ndata[key] = g.ndata[key] \n g = sym_g \n profiler = Profiler() \n dgl.distributed.partition_graph(g, args.dataset, args.num_parts, 'data', \n part_method=args.part_method, \n balance_ntypes=balance_ntypes) \n print(profiler.output_text(unicode=True, color=True))", "model_em_trim": 0, "model_bleu_trim": 63.26797692469125, "error_type_gpt": "Unclear Location"}
{"id": 321, "old": "     # Adjust data\n     adj_cols = ['open', 'high', 'low', 'close']\n     for ticker in panel.items:\n-        ratio = (panel[ticker]['price'] / panel[ticker]['close']).values\n         for col in adj_cols:\n-            panel[ticker][col] *= ratio\n     return panel", "new": "     # Adjust data\n     adj_cols = ['open', 'high', 'low', 'close']\n     for ticker in panel.items:\n+        ratio = (panel[ticker]['price'] / panel[ticker]['close'])\n+        ratio_filtered = ratio.fillna(0).values\n         for col in adj_cols:\n+            panel[ticker][col] *= ratio_filtered\n     return panel", "review": "Could a stock on the way to delisting have a close price of 0 in Yahoo's data? If so, should we have a check here to make sure that close is non-0?", "dataset": "codereview", "original_id": 1782, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n    # Adjust data\n    adj_cols = ['open', 'high', 'low', 'close']\n    for ticker in panel.items:\n        close_price = panel[ticker]['close']\n        if close_price == 0:\n            continue\n        ratio = (panel[ticker]['price'] / close_price).values\n        for col in adj_cols:\n            panel[ticker][col] *= ratio\n    return panel\n", "gpt_em_trim": 0, "gpt_bleu_trim": 76.69666, "model_code": "\n # Adjust data \n adj_cols = ['open', 'high', 'low', 'close'] \n for ticker in panel.items: \n if panel[ticker]['close'] > 0: \n ratio = (panel[ticker]['price'] / panel[ticker]['close']).values \n for col in adj_cols: \n panel[ticker][col] *= ratio \n return panel", "model_em_trim": 0, "model_bleu_trim": 82.14077131680635, "error_type_gpt": "Need Domain Knowledge"}
{"id": 322, "old": " // Imap settings\n $txt['maillist_imap'] = 'IMAP Settings';\n $txt['maillist_imap_host'] = 'Mailbox Server Name';\n-$txt['maillist_imap_host_desc'] = 'Enter a server name and :optional port. For example: imap.gmail.com';\n $txt['maillist_imap_mailbox'] = 'Mailbox Name';\n $txt['maillist_imap_mailbox_desc'] = 'Enter a mailbox name on the server. For example: INBOX';\n $txt['maillist_imap_uid'] = 'Mailbox Username';", "new": " // Imap settings\n $txt['maillist_imap'] = 'IMAP Settings';\n $txt['maillist_imap_host'] = 'Mailbox Server Name';\n+$txt['maillist_imap_host_desc'] = 'Enter a mail server host name and optional :port number. e.g. imap.gmail.com or imap.gmail.com:993';\n $txt['maillist_imap_mailbox'] = 'Mailbox Name';\n $txt['maillist_imap_mailbox_desc'] = 'Enter a mailbox name on the server. For example: INBOX';\n $txt['maillist_imap_uid'] = 'Mailbox Username';", "review": "Maybe `and optional :port`? And `For example: imap.gmail.com or imap.gmail.com:993`? <= not sure at all about that one...", "dataset": "codereview", "original_id": 1802, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n// Imap settings\n$txt['maillist_imap'] = 'IMAP Settings';\n$txt['maillist_imap_host'] = 'Mailbox Server Name';\n$txt['maillist_imap_host_desc'] = 'Enter a server name and an optional port. For example: imap.gmail.com or imap.gmail.com:993';\n$txt['maillist_imap_mailbox'] = 'Mailbox Name';\n$txt['maillist_imap_mailbox_desc'] = 'Enter a mailbox name on the server. For example: INBOX';\n$txt['maillist_imap_uid'] = 'Mailbox Username';\n", "gpt_em_trim": 0, "gpt_bleu_trim": 90.64972, "model_code": "\n // Imap settings \n $txt['maillist_imap'] = 'IMAP Settings'; \n $txt['maillist_imap_host'] = 'Mailbox Server Name'; \n $txt['maillist_imap_host_desc'] = 'Enter a server name and optional port. For example: imap.gmail.com or imap.gmail.com:993'; \n $txt['maillist_imap_mailbox'] = 'Mailbox Name'; \n $txt['maillist_imap_mailbox_desc'] = 'Enter a mailbox name on the server. For example: INBOX'; \n $txt['maillist_imap_uid'] = 'Mailbox Username';", "model_em_trim": 0, "model_bleu_trim": 90.98421535418638, "error_type_gpt": "Need Domain Knowledge"}
{"id": 323, "old": " // PutInBatch stores a uint64 value in a batch\n // that can be saved later in the database.\n-func (f Uint64Field) PutInBatch(batch *badger.Txn, val uint64) {\n-\terr := batch.Set(f.key, encodeUint64(val))\n \tif err != nil {\n \t\tf.logger.Debugf(\"could not set uint64 value in PutInBatch. Error : %s\", err.Error())\n \t}\n }\n // Inc increments a uint64 value in the database.", "new": " // PutInBatch stores a uint64 value in a batch\n // that can be saved later in the database.\n+func (f Uint64Field) PutInBatch(batch *badger.Txn, val uint64) (err error){\n+\terr = batch.Set(f.key, encodeUint64(val))\n \tif err != nil {\n \t\tf.logger.Debugf(\"could not set uint64 value in PutInBatch. Error : %s\", err.Error())\n+\t\treturn err\n \t}\n+\treturn nil\n }\n // Inc increments a uint64 value in the database.", "review": "The error is not handled! This is very dangerous. We should return this error in order to stop any other actions on the batch. This can be cause of data inconstancies.", "dataset": "codereview", "original_id": 1807, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-add", "gpt_code": "\n// PutInBatch stores a uint64 value in a batch\n// that can be saved later in the database.\nfunc (f Uint64Field) PutInBatch(batch *badger.Txn, val uint64) error {\n\terr := batch.Set(f.key, encodeUint64(val))\n\tif err != nil {\n\t\tf.logger.Debugf(\"could not set uint64 value in PutInBatch. Error : %s\", err.Error())\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Inc increments a uint64 value in the database.\n", "gpt_em_trim": 0, "gpt_bleu_trim": 93.8548, "model_code": "\n // PutInBatch stores a uint64 value in a batch \n // that can be saved later in the database. \n func (f Uint64Field) PutInBatch(batch *badger.Txn, val uint64) error { \n return batch.Set(f.key, encodeUint64(val)) \n } \n // Inc increments a uint64 value in the database.", "model_em_trim": 0, "model_bleu_trim": 48.48721955405145, "error_type_gpt": "Need Domain Knowledge"}
{"id": 324, "old": " // See the LICENSE file in the project root for more information.\n using System.Collections.Generic;\n using BenchmarkDotNet.Attributes;\n using MicroBenchmarks;\n-\n using ustring = System.Utf8String;\n namespace System.Text.Experimental", "new": " // See the LICENSE file in the project root for more information.\n using System.Collections.Generic;\n+using System.Diagnostics;\n+using System.IO;\n using BenchmarkDotNet.Attributes;\n using MicroBenchmarks;\n+using Microsoft.CodeAnalysis.CSharp.Syntax;\n using ustring = System.Utf8String;\n namespace System.Text.Experimental", "review": "hhm, the return value is not used anywhere. Did you check what happens to the actual calls to `ToCharArray()`? Is your code even getting called?", "dataset": "codereview", "original_id": 1836, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\n// See the LICENSE file in the project root for more information.\nusing System.Collections.Generic;\nusing BenchmarkDotNet.Attributes;\nusing MicroBenchmarks;\n\nusing ustring = System.Utf8String;\nnamespace System.Text.Experimental\n{\n    public class MyExperimentalClass\n    {\n        [Benchmark]\n        public void MyExperimentalMethod()\n        {\n            var myString = \"Hello, world!\";\n            var charArray = myString.ToCharArray();\n            // Do something with charArray here\n        }\n    }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 65.77423, "model_code": "\n // See the LICENSE file in the project root for more information. \n using System.Collections.Generic; \n using System.Linq; \n using BenchmarkDotNet.Attributes; \n using MicroBenchmarks; \n using ustring = System.Utf8String; \n namespace System.Text.Experimental", "model_em_trim": 0, "model_bleu_trim": 70.17201683084967}
{"id": 325, "old": "           std::string readData;\n           detail::loggerStatus(detail::db->Get(leveldb::ReadOptions(), key, &readData));\n-          if (not readData.empty()) {\n               return readData;\n           } else {\n               return \"\";", "new": "           std::string readData;\n           detail::loggerStatus(detail::db->Get(leveldb::ReadOptions(), key, &readData));\n+          if (!readData.empty()) {\n               return readData;\n           } else {\n               return \"\";", "review": "`not readData.empty()` really? Maybe `!readData.empty()`?", "dataset": "codereview", "original_id": 1838, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n          std::string readData;\n          detail::loggerStatus(detail::db->Get(leveldb::ReadOptions(), key, &readData));\n          if (!readData.empty()) {\n              return readData;\n          } else {\n              return \"\";\n          }\n", "gpt_em_trim": 1, "gpt_bleu_trim": 98.1216, "model_code": "\n std::string readData; \n detail::loggerStatus(detail::db->Get(leveldb::ReadOptions(), key, &readData)); \n if (!readData.empty()) { \n return readData; \n } else { \n return \"\";", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 326, "old": " \t{{end}}\n \t{{ if .Search }}\n \t\tAND (\n-\t\t\tCAST(a.id AS TEXT) ilike :search OR\n \t\t\ta.summary ilike :search OR\n \t\t\tsvc.name ilike :search\n \t\t)", "new": " \t{{end}}\n \t{{ if .Search }}\n \t\tAND (\n+\t\t\tCAST(a.id AS TEXT) = substring(:search FROM '\\%(.*?)\\%') OR\n \t\t\ta.summary ilike :search OR\n \t\t\tsvc.name ilike :search\n \t\t)", "review": "For ID searching, we should probably do an exact match instead, wyt?", "dataset": "codereview", "original_id": 1852, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n\t{{end}}\n\t{{ if .Search }}\n\t\tAND (\n\t\t\ta.id = :search OR\n\t\t\ta.summary ilike :search OR\n\t\t\tsvc.name ilike :search\n\t\t)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 49.86227, "model_code": "\n {{end}} \n {{ if .Search }} \n AND ( \n (CAST(a.id AS TEXT) ilike :search OR \n a.summary ilike :search OR \n svc.name ilike :search \n )", "model_em_trim": 0, "model_bleu_trim": 61.94928842397669, "error_type_gpt": "Need Domain Knowledge"}
{"id": 327, "old": "     sections = (max_end - max_start) / 3\n     idx = max_start + int(sections) + int(sections/2)\n-    return lrs[idx].item(), (lrs[idx].item(), losses[idx])\n # Cell\n def slide(lrs:list, losses:list, num_it:int, lr_diff:int=15, thresh:float=.005, adjust_value:float=1.):", "new": "     sections = (max_end - max_start) / 3\n     idx = max_start + int(sections) + int(sections/2)\n+    return float(lrs[idx]), (float(lrs[idx]), losses[idx])\n # Cell\n def slide(lrs:list, losses:list, num_it:int, lr_diff:int=15, thresh:float=.005, adjust_value:float=1.):", "review": "I think this will fail if the suggestion isn't a tensor (which it might not be). Instead I think this should always work: ```suggestion return float(lrs[idx]), (float(lrs[idx]), losses[idx]) ```", "dataset": "codereview", "original_id": 1870, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n    sections = (max_end - max_start) / 3\n    idx = max_start + int(sections) + int(sections/2)\n    suggestion = float(lrs[idx])\n    return suggestion, (suggestion, losses[idx])\n# Cell\ndef slide(lrs:list, losses:list, num_it:int, lr_diff:int=15, thresh:float=.005, adjust_value:float=1.):\n", "gpt_em_trim": 0, "gpt_bleu_trim": 87.69089, "model_code": "\n sections = (max_end - max_start) / 3 \n idx = max_start + int(sections) + int(sections/2) \n return float(lrs[idx]), (float(lrs[idx]), losses[idx]) \n # Cell \n def slide(lrs:list, losses:list, num_it:int, lr_diff:int=15, thresh:float=.005, adjust_value:float=1.):", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Code Style Difference"}
{"id": 328, "old": " \t\t\t$meta_input['_enable_quiz_reset'] = true === $quiz_options['allow_retakes'] ? 'on' : '';\n \t\t}\n-\t\tif ( isset( $quiz_options['show_questions'] ) ) {\n \t\t\t$meta_input['_show_questions'] = $quiz_options['show_questions'];\n \t\t} else {\n \t\t\t$meta_input['_show_questions'] = '';", "new": " \t\t\t$meta_input['_enable_quiz_reset'] = true === $quiz_options['allow_retakes'] ? 'on' : '';\n \t\t}\n+\t\tif ( array_key_exists( 'show_questions', $quiz_options ) ) {\n \t\t\t$meta_input['_show_questions'] = $quiz_options['show_questions'];\n \t\t} else {\n \t\t\t$meta_input['_show_questions'] = '';", "review": "With this change if the `show_questions` argument is not sent at all then the value will be deleted. The approach for everything else was that if an argument is omited then it is not updated and for something to get deleted `null` or `''` must be sent. Frontend sends the value always so it makes no difference but I think that we should be consistent. ```suggestion if ( array_key_exists( 'show_questions', $quiz_options ) ) { ```", "dataset": "codereview", "original_id": 1874, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nif ( array_key_exists( 'show_questions', $quiz_options ) ) {\n    $meta_input['_show_questions'] = $quiz_options['show_questions'];\n} elseif ( ! isset( $quiz_options['show_questions'] ) ) {\n    unset( $meta_input['_show_questions'] );\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 60.63469, "model_code": "\n $meta_input['_enable_quiz_reset'] = true === $quiz_options['allow_retakes'] ? 'on' : ''; \n } \n if ( array_key_exists( 'show_questions', $quiz_options ) ) { \n $meta_input['_show_questions'] = $quiz_options['show_questions']; \n } else { \n $meta_input['_show_questions'] = '';", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Changes"}
{"id": 329, "old": " \t\treturn cs;\n \t}\n-\tpublic boolean getDisableAudioTrackSorting() {\n-\t\treturn getBoolean(KEY_DISABLE_AUDIO_TRACK_SORTING, false);\n \t}\n \tpublic boolean isDynamicPls() {", "new": " \t\treturn cs;\n \t}\n+\tpublic boolean isSortAudioTracksByAlbumPosition() {\n+\t\treturn getBoolean(KEY_SORT_AUDIO_TRACKS_BY_ALBUM_POSITION, true);\n \t}\n \tpublic boolean isDynamicPls() {", "review": "```suggestion public boolean isSortAudioTracksByAlbumPosition() { return getBoolean(KEY_SORT_AUDIO_TRACKS_BY_ALBUM_POSITION, true); ```", "dataset": "codereview", "original_id": 1895, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\n\tpublic boolean isSortAudioTracksByAlbumPosition() {\n\t\treturn getBoolean(KEY_SORT_AUDIO_TRACKS_BY_ALBUM_POSITION, true);\n\t}\n\tpublic boolean getDisableAudioTrackSorting() {\n\t\treturn getBoolean(KEY_DISABLE_AUDIO_TRACK_SORTING, false);\n\t}\n\tpublic boolean isDynamicPls() {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 61.93936, "model_code": "\n return cs; \n } \n public boolean isSortAudioTracksByAlbumPosition() { \n return getBoolean(KEY_SORT_AUDIO_TRACKS_BY_ALBUM_POSITION, true); \n } \n public boolean isDynamicPls() {", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 330, "old": "                             break;\n                         }\n                     }\n-                    if (name != null && !name.trim().isEmpty()) {\n-                        nameNode.put(locale, name);\n                     }\n-                    if (documentation != null && !documentation.trim().isEmpty()) {\n-                        descriptionNode.put(locale, documentation);\n                     }\n                 }\n             }", "new": "                             break;\n                         }\n                     }\n+                    if (StringUtils.isNotBlank(name)) {\n+                        nameNode.put(locale, name.trim());\n                     }\n+                    if (StringUtils.isNotBlank(documentation)) {\n+                        descriptionNode.put(locale, documentation.trim());\n                     }\n                 }\n             }", "review": "If you are expecting leading/trailing spaces (the use of trim()); wouldn't you want to store the trimmed value too?", "dataset": "codereview", "original_id": 1902, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n                            break;\n                        }\n                    }\n                    if (name != null) {\n                        String trimmedName = name.trim();\n                        if (!trimmedName.isEmpty()) {\n                            nameNode.put(locale, trimmedName);\n                        }\n                    }\n                    if (documentation != null) {\n                        String trimmedDocumentation = documentation.trim();\n                        if (!trimmedDocumentation.isEmpty()) {\n                            descriptionNode.put(locale, trimmedDocumentation);\n                        }\n                    }\n                }\n            }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 42.39845, "model_code": "\n break; \n } \n } \n if (name != null && !name.trim().isEmpty()) { \n nameNode.put(locale, name); \n } \n if (documentation != null && !documentation.trim().isEmpty()) { \n descriptionNode.put(locale, documentation); \n } \n } \n }", "model_em_trim": 0, "model_bleu_trim": 57.65284161894747, "error_type_gpt": "Need Domain Knowledge"}
{"id": 331, "old": "         except shellutil.CommandError as cmd_err:\n             if chk_err:\n-                msg = \"\"\"Failed to eject dvd: ret={0}\n-                [stdout]\n-                {1}\n-\n-                [stderr]\n-                {2}\n-                \"\"\".format(cmd_err.returncode, cmd_err.stdout, cmd_err.stderr)\n                 raise OSUtilError(msg)", "new": "         except shellutil.CommandError as cmd_err:\n             if chk_err:\n+                msg = \"Failed to eject dvd: ret={0}\\n[stdout]\\n{1}\\n\\n[stderr]\\n{2}\"\\\n+                    .format(cmd_err.returncode, cmd_err.stdout, cmd_err.stderr)\n                 raise OSUtilError(msg)", "review": "need to remove extra spaces from message", "dataset": "codereview", "original_id": 1905, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "\n        except shellutil.CommandError as cmd_err:\n            if chk_err:\n                msg = \"\"\"Failed to eject dvd: ret={0}\n[stdout]\n{1}\n\n[stderr]\n{2}\n\"\"\".format(cmd_err.returncode, cmd_err.stdout, cmd_err.stderr)\n                raise OSUtilError(msg)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 73.98353, "model_code": "\n except shellutil.CommandError as cmd_err: \n if chk_err: \n msg = \"\"\"Failed to eject dvd: ret={0} [stdout] [stderr] [2]\"\"\".format(cmd_err.returncode, cmd_err.stdout, cmd_err.stderr) \n raise OSUtilError(msg)", "model_em_trim": 0, "model_bleu_trim": 69.33346539820185, "error_type_gpt": "Need Domain Knowledge"}
{"id": 332, "old": " \tfmt.Println(\"Success, try `convox apps`\")\n-\tstdcli.QOSEventSend(\"cli-install\", distinctID, ep)\n-\treturn stdcli.Error(err)\n }\n /// validateUserAccess checks for the \"AdministratorAccess\" policy needed to create a rack.", "new": " \tfmt.Println(\"Success, try `convox apps`\")\n+\treturn stdcli.QOSEventSend(\"cli-install\", distinctID, ep)\n }\n /// validateUserAccess checks for the \"AdministratorAccess\" policy needed to create a rack.", "review": "This last return isn't necessary as it would be handled by line 391.", "dataset": "codereview", "original_id": 1958, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-remove", "gpt_code": "\n\tfmt.Println(\"Success, try `convox apps`\")\n\tstdcli.QOSEventSend(\"cli-install\", distinctID, ep)\n}\n/// validateUserAccess checks for the \"AdministratorAccess\" policy needed to create a rack.\n", "gpt_em_trim": 0, "gpt_bleu_trim": 94.57365, "model_code": "\n fmt.Println(\"Success, try `convox apps`\") \n stdcli.QOSEventSend(\"cli-install\", distinctID, ep) \n } \n /// validateUserAccess checks for the \"AdministratorAccess\" policy needed to create a rack.", "model_em_trim": 0, "model_bleu_trim": 94.5736483174624, "error_type_gpt": "Unclear Changes"}
{"id": 333, "old": " \t\treturn taskExecution;\n \t}\n-\tpublic TaskExecutionManifest sanitizeTaskManifest(TaskExecutionManifest taskManifest) {\n-\t\tif (taskManifest == null) {\n \t\t\treturn null;\n \t\t}\n \t\tTaskExecutionManifest sanitizedTaskExecutionManifest = new TaskExecutionManifest();\n \t\tTaskExecutionManifest.Manifest sanitizedManifest = sanitizedTaskExecutionManifest.getManifest();\n-\t\tTaskExecutionManifest.Manifest dirtyTaskManifest = taskManifest.getManifest();\n \t\tsanitizedManifest.setPlatformName(dirtyTaskManifest.getPlatformName());\n \t\tAppDeploymentRequest existingAppDeploymentRequest = dirtyTaskManifest.getTaskDeploymentRequest();\n \t\t// Sanitize App Properties", "new": " \t\treturn taskExecution;\n \t}\n+\tpublic TaskExecutionManifest sanitizeTaskExecutionManifest(TaskExecutionManifest taskExecutionManifest) {\n+\t\tif (taskExecutionManifest == null) {\n \t\t\treturn null;\n \t\t}\n \t\tTaskExecutionManifest sanitizedTaskExecutionManifest = new TaskExecutionManifest();\n \t\tTaskExecutionManifest.Manifest sanitizedManifest = sanitizedTaskExecutionManifest.getManifest();\n+\t\tTaskExecutionManifest.Manifest dirtyTaskManifest = taskExecutionManifest.getManifest();\n \t\tsanitizedManifest.setPlatformName(dirtyTaskManifest.getPlatformName());\n \t\tAppDeploymentRequest existingAppDeploymentRequest = dirtyTaskManifest.getTaskDeploymentRequest();\n \t\t// Sanitize App Properties", "review": "We can rename this method as well (to have TaskExecutionManifest)", "dataset": "codereview", "original_id": 1961, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\n\t\treturn taskExecution;\n\t}\n\t\n\tpublic TaskExecutionManifest sanitizeTaskExecutionManifest(TaskExecutionManifest taskExecutionManifest) {\n\t\tif (taskExecutionManifest == null) {\n\t\t\treturn null;\n\t\t}\n\t\tTaskExecutionManifest sanitizedTaskExecutionManifest = new TaskExecutionManifest();\n\t\tTaskExecutionManifest.Manifest sanitizedManifest = sanitizedTaskExecutionManifest.getManifest();\n\t\tTaskExecutionManifest.Manifest dirtyManifest = taskExecutionManifest.getManifest();\n\t\tsanitizedManifest.setPlatformName(dirtyManifest.getPlatformName());\n\t\tAppDeploymentRequest existingAppDeploymentRequest = dirtyManifest.getTaskDeploymentRequest();\n\t\t// Sanitize App Properties\n", "gpt_em_trim": 0, "gpt_bleu_trim": 83.48489, "model_code": "\n return taskExecution; \n } \n public TaskExecutionManifest sanitizeTaskManifest(TaskExecutionManifest taskExecutionManifest) { \n if (taskExecutionManifest == null) { \n return null; \n } \n TaskExecutionManifest sanitizedTaskExecutionManifest = new TaskExecutionManifest(); \n TaskExecutionManifest.Manifest sanitizedManifest = sanitizedTaskExecutionManifest.getManifest(); \n TaskExecutionManifest.Manifest dirtyTaskManifest = taskExecutionManifest.getManifest(); \n sanitizedManifest.setPlatformName(dirtyTaskManifest.getPlatformName()); \n AppDeploymentRequest existingAppDeploymentRequest = dirtyTaskManifest.getTaskDeploymentRequest(); \n // Sanitize App Properties", "model_em_trim": 0, "model_bleu_trim": 96.77930193424082, "error_type_gpt": "Need Domain Knowledge"}
{"id": 334, "old": " #include \"stringtools.h\"\n #include \"list.h\"\n #include \"makeflow_summary.h\"\n-#include \"../../dttools/src/catalog_query.h\"\n-#include \"../../dttools/src/json.h\"\n-#include \"../../dttools/src/json_aux.h\"\n-#include \"../../dttools/src/username.h\"\n #include <stdlib.h>\n #include <stdio.h>", "new": " #include \"stringtools.h\"\n #include \"list.h\"\n #include \"makeflow_summary.h\"\n+#include \"catalog_query.h\"\n+#include \"json.h\"\n+#include \"json_aux.h\"\n+#include \"username.h\"\n+#include \"batch_job.h\"\n #include <stdlib.h>\n #include <stdio.h>", "review": "I believe you can write this simply as `catalog_query.h` and the build system will set the paths correctly.", "dataset": "codereview", "original_id": 1962, "relevance": "partial", "relevance_user1": "perfect", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-rename", "gpt_code": "\n#include \"stringtools.h\"\n#include \"list.h\"\n#include \"makeflow_summary.h\"\n#include \"catalog_query.h\"\n#include \"json.h\"\n#include \"json_aux.h\"\n#include \"username.h\"\n#include <stdlib.h>\n#include <stdio.h>\n", "gpt_em_trim": 0, "gpt_bleu_trim": 87.93507, "model_code": "\n #include \"stringtools.h\" \n #include \"list.h\" \n #include \"makeflow_summary.h\" \n #include <stdlib.h> \n #include <stdio.h>", "model_em_trim": 0, "model_bleu_trim": 33.995318282689965}
{"id": 335, "old": " \t\tresIndex := -1\n \t\tfor i := 0; i < v.NumField(); i++ {\n \t\t\tif f := t.Field(i); p.fieldName(f) == fieldName {\n-\t\t\t\tif resIndex == -1 {\n-\t\t\t\t\tresIndex = i\n-\t\t\t\t} else if v.Field(resIndex).IsZero() && !v.Field(i).IsZero() {\n \t\t\t\t\treturn t.Field(i), true\n \t\t\t\t}\n \t\t\t}\n \t\t}", "new": " \t\tresIndex := -1\n \t\tfor i := 0; i < v.NumField(); i++ {\n \t\t\tif f := t.Field(i); p.fieldName(f) == fieldName {\n+\t\t\t\tif !v.Field(i).IsZero() {\n \t\t\t\t\treturn t.Field(i), true\n+\t\t\t\t} else if resIndex == -1 {\n+\t\t\t\t\tresIndex = i\n \t\t\t\t}\n \t\t\t}\n \t\t}", "review": "I wonder if we can return early here if the field value isn't zero and skip the check on the next line? ``` if !v.Field(i).IsZero() { return t.Field(i), true } else if resindex == -1 { resIndex = i } ```", "dataset": "codereview", "original_id": 1967, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nresIndex := -1\nfor i := 0; i < v.NumField(); i++ {\n    if f := t.Field(i); p.fieldName(f) == fieldName {\n        if !v.Field(i).IsZero() {\n            return t.Field(i), true\n        } else if resIndex == -1 {\n            resIndex = i\n        }\n    }\n}\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n resIndex := -1 \n for i := 0; i < v.NumField(); i++ { \n if f := t.Field(i); p.fieldName(f) == fieldName { \n if !v.Field(i).IsZero() { \n return t.Field(i), true \n } else if resIndex == -1 { \n resIndex = i \n } \n } \n }", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 336, "old": "     Utils.idLock.lock();\n     try {\n       tableInfo.tableId = Utils.getNextId(tableInfo.tableName, master.getInstance(), Table.ID::of);\n-      if (tableInfo.props.containsKey(Property.TABLE_OFFLINE_OPTS + \"create.initial.splits\")\n-          && this.splitFile != null) {\n-        storeSplitFileNameInZooKeeper();\n-      }\n       return new SetupPermissions(tableInfo);\n     } finally {\n       Utils.idLock.unlock();\n     }\n-\n   }\n   @Override", "new": "     Utils.idLock.lock();\n     try {\n       tableInfo.tableId = Utils.getNextId(tableInfo.tableName, master.getInstance(), Table.ID::of);\n       return new SetupPermissions(tableInfo);\n     } finally {\n       Utils.idLock.unlock();\n     }\n   }\n   @Override", "review": "I don't think this needs to be stored in ZK explicitly. The way FATE works, each step of a FATE op is serialized to ZK. Therefore if you store any info you want to keep around in the TableInfo object, then the TableInfo object will be serialized in subsequent steps.", "dataset": "codereview", "original_id": 1976, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n    Utils.idLock.lock();\n    try {\n      tableInfo.tableId = Utils.getNextId(tableInfo.tableName, master.getInstance(), Table.ID::of);\n      return new SetupPermissions(tableInfo);\n    } finally {\n      Utils.idLock.unlock();\n    }\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n Utils.idLock.lock(); \n try { \n tableInfo.tableId = Utils.getNextId(tableInfo.tableName, master.getInstance(), Table.ID::of); \n return new SetupPermissions(tableInfo); \n } finally { \n Utils.idLock.unlock(); \n } \n } \n @Override", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 337, "old": "         metricsSystem.createCounter(\n             MetricCategory.SYNCHRONIZER,\n             \"inboundQueueCounter\",\n-            \"parallel download pipeline metric\");\n     this.outboundQueueCounter =\n         metricsSystem.createCounter(\n             MetricCategory.SYNCHRONIZER,\n             \"outboundQueueCounter\",\n-            \"parallel download pipeline metric\");\n   }\n   @Override", "new": "         metricsSystem.createCounter(\n             MetricCategory.SYNCHRONIZER,\n             \"inboundQueueCounter\",\n+            \"count of queue items that started processing\");\n     this.outboundQueueCounter =\n         metricsSystem.createCounter(\n             MetricCategory.SYNCHRONIZER,\n             \"outboundQueueCounter\",\n+            \"count of queue items that finished processing\");\n   }\n   @Override", "review": "More descriptive, such as \"count of queue items that started processing\"", "dataset": "codereview", "original_id": 1987, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n        metricsSystem.createCounter(\n            MetricCategory.SYNCHRONIZER,\n            \"inboundQueueCounter\",\n            \"count of queue items that started processing\");\n    this.outboundQueueCounter =\n        metricsSystem.createCounter(\n            MetricCategory.SYNCHRONIZER,\n            \"outboundQueueCounter\",\n            \"count of queue items that finished processing\");\n  }\n  @Override\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n metricsSystem.createCounter( \n MetricCategory.SYNCHRONIZER, \n \"inboundQueueCounter\", \n \"count of queue items that started processing\"); \n this.outboundQueueCounter = \n metricsSystem.createCounter( \n MetricCategory.SYNCHRONIZER, \n \"outboundQueueCounter\", \n \"count of queue items that started processing\"); \n } \n @Override", "model_em_trim": 0, "model_bleu_trim": 95.15586241253386}
{"id": 338, "old": "         `        COMPAT: ${lwcConfig.compat},`,\n         `        NATIVE_SHADOW: ${lwcConfig.nativeShadow}`,\n         `    }`,\n-        `};\n-        if (window.process.env.COMPAT) { delete window.customElements; };`,\n     ];\n     fs.writeFileSync(ENV_FILENAME, content.join('\\n'));\n }", "new": "         `        COMPAT: ${lwcConfig.compat},`,\n         `        NATIVE_SHADOW: ${lwcConfig.nativeShadow}`,\n         `    }`,\n+        `};`,\n     ];\n     fs.writeFileSync(ENV_FILENAME, content.join('\\n'));\n }", "review": "Wiping out the customElements object fixes it for tests. Will the engine work fine in COMPAT mode in google/firefox/safari in non-test modes?", "dataset": "codereview", "original_id": 2001, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n        `        COMPAT: ${lwcConfig.compat},`,\n        `        NATIVE_SHADOW: ${lwcConfig.nativeShadow}`,\n        `    }`,\n        `};`,\n    ];\n    if (window.process.env.COMPAT) {\n        delete window.customElements;\n    }\n    fs.writeFileSync(ENV_FILENAME, content.join('\\n'));\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 74.35155, "model_code": "\n `        COMPAT: ${lwcConfig.compat},`, \n `        NATIVE_SHADOW: ${lwcConfig.nativeShadow}`, \n `    }`, \n ]; \n fs.writeFileSync(ENV_FILENAME, content.join('\\n')); \n }", "model_em_trim": 0, "model_bleu_trim": 90.36540607826602, "error_type_gpt": "Need Domain Knowledge"}
{"id": 339, "old": "     }\n     private static int killLoadData(String stmt, int offset) {\n-        if (stmt.length() > offset + 5) {\n             char c1 = stmt.charAt(++offset);\n             char c2 = stmt.charAt(++offset);\n             char c3 = stmt.charAt(++offset);", "new": "     }\n     private static int killLoadData(String stmt, int offset) {\n+        if (stmt.length() > offset + \"oadData\".length()) {\n             char c1 = stmt.charAt(++offset);\n             char c2 = stmt.charAt(++offset);\n             char c3 = stmt.charAt(++offset);", "review": "Incorrect boundary check", "dataset": "codereview", "original_id": 2004, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n    }\n    private static int killLoadData(String stmt, int offset) {\n        if (stmt.length() > offset + 3) {\n            char c1 = stmt.charAt(offset++);\n            char c2 = stmt.charAt(offset++);\n            char c3 = stmt.charAt(offset++);\n", "gpt_em_trim": 0, "gpt_bleu_trim": 70.94647, "model_code": "\n } \n private static int killLoadData(String stmt, int offset) { \n if (stmt.length() >= offset + 5) { \n char c1 = stmt.charAt(++offset); \n char c2 = stmt.charAt(++offset); \n char c3 = stmt.charAt(++offset);", "model_em_trim": 0, "model_bleu_trim": 85.34164230494807, "error_type_gpt": "Need Domain Knowledge"}
{"id": 340, "old": "                         .load(media.getImageLocation())\n                         .apply(RequestOptions.diskCacheStrategyOf(ApGlideSettings.AP_DISK_CACHE_STRATEGY))\n                         .submit(iconSize, iconSize)\n-                        .get();\n                 views.setImageViewBitmap(R.id.imgvCover, icon);\n             } catch (Throwable tr) {\n                 Log.e(TAG, \"Error loading the media icon for the widget\", tr);", "new": "                         .load(media.getImageLocation())\n                         .apply(RequestOptions.diskCacheStrategyOf(ApGlideSettings.AP_DISK_CACHE_STRATEGY))\n                         .submit(iconSize, iconSize)\n+                        .get(500, TimeUnit.MILLISECONDS);\n                 views.setImageViewBitmap(R.id.imgvCover, icon);\n             } catch (Throwable tr) {\n                 Log.e(TAG, \"Error loading the media icon for the widget\", tr);", "review": "If you use something like `.get(500, TimeUnit.MILLISECONDS)`, you can make sure that loading the icon does not cause \"Application not responding\" errors. I think 500ms are a reasonable maximum duration because the widget is updated every 1 second.", "dataset": "codereview", "original_id": 2005, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\ntry {\n    Bitmap icon = Glide.with(context)\n            .asBitmap()\n            .load(media.getImageLocation())\n            .apply(RequestOptions.diskCacheStrategyOf(ApGlideSettings.AP_DISK_CACHE_STRATEGY))\n            .submit(iconSize, iconSize)\n            .get(500, TimeUnit.MILLISECONDS);\n    views.setImageViewBitmap(R.id.imgvCover, icon);\n} catch (Throwable tr) {\n    Log.e(TAG, \"Error loading the media icon for the widget\", tr);\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 98.81294, "model_code": "\n .load(media.getImageLocation()) \n .apply(RequestOptions.diskCacheStrategyOf(ApGlideSettings.AP_DISK_CACHE_STRATEGY)) \n .submit(iconSize, iconSize) \n .get(500, TimeUnit.MILLISECONDS); \n views.setImageViewBitmap(R.id.imgvCover, icon); \n } catch (Throwable tr) { \n Log.e(TAG, \"Error loading the media icon for the widget\", tr);", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Insignificant Omission"}
{"id": 341, "old": "             return await ExecuteAndValidateBundle(searchUrl, searchUrl, sort, expectedResources);\n         }\n-        protected async Task<Bundle> ExecuteAndValidateBundleSuperset(string searchUrl, bool sort, params Resource[] expectedResources)\n-        {\n-            Bundle bundle = await Client.SearchAsync(searchUrl);\n-\n-            ValidateBundleIsSuperset(bundle, sort, expectedResources);\n-\n-            return bundle;\n-        }\n-\n         protected async Task<Bundle> ExecuteAndValidateBundle(string searchUrl, string selfLink, params Resource[] expectedResources)\n         {\n             Bundle bundle = await Client.SearchAsync(searchUrl);", "new": "             return await ExecuteAndValidateBundle(searchUrl, searchUrl, sort, expectedResources);\n         }\n         protected async Task<Bundle> ExecuteAndValidateBundle(string searchUrl, string selfLink, params Resource[] expectedResources)\n         {\n             Bundle bundle = await Client.SearchAsync(searchUrl);", "review": "Can you explain what this is for?", "dataset": "codereview", "original_id": 2010, "relevance": "not", "relevance_user1": "perfect", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nprotected async Task<Bundle> ExecuteAndValidateBundle(string searchUrl, string selfLink, params Resource[] expectedResources)\n{\n    Bundle bundle = await Client.SearchAsync(searchUrl);\n\n    ValidateBundleContainsAllResources(bundle, expectedResources);\n\n    return bundle;\n}\n\nprotected async Task<Bundle> ExecuteAndValidateBundleSorted(string searchUrl, string selfLink, params Resource[] expectedResources)\n{\n    Bundle bundle = await Client.SearchAsync(searchUrl);\n\n    ValidateBundleContainsAllResources(bundle, expectedResources);\n    ValidateBundleIsSorted(bundle);\n\n    return bundle;\n}\n\nprivate void ValidateBundleContainsAllResources(Bundle bundle, params Resource[] expectedResources)\n{\n    foreach (var expectedResource in expectedResources)\n    {\n        if (!bundle.Contains(expectedResource))\n        {\n            throw new Exception($\"Bundle does not contain expected resource: {expectedResource}\");\n        }\n    }\n}\n\nprivate void ValidateBundleIsSorted(Bundle bundle)\n{\n    // TODO: Implement sorting validation logic\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 76.23139, "model_code": "\n return await ExecuteAndValidateBundle(searchUrl, searchUrl, sort, expectedResources); \n } \n protected async Task<Bundle> ExecuteAndValidateBundle(string searchUrl, string selfLink, params Resource[] expectedResources) \n { \n Bundle bundle = await Client.SearchAsync(searchUrl);", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 342, "old": " TEST_P(ValidProtoPaginationQueryValidatorTest, ValidPaginationQuery) {\n   auto answer = validator.validate(GetParam());\n-  ASSERT_FALSE(answer.hasErrors());\n }\n INSTANTIATE_TEST_CASE_P(", "new": " TEST_P(ValidProtoPaginationQueryValidatorTest, ValidPaginationQuery) {\n   auto answer = validator.validate(GetParam());\n+  ASSERT_FALSE(answer.hasErrors()) << GetParam().DebugString() << std::endl\n+                                   << answer.reason();\n }\n INSTANTIATE_TEST_CASE_P(", "review": "Maybe print answer and query, the same way as in transaction tests", "dataset": "codereview", "original_id": 2019, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nTEST_P(ValidProtoPaginationQueryValidatorTest, ValidPaginationQuery) {\n  auto answer = validator.validate(GetParam());\n  ASSERT_FALSE(answer.hasErrors()) << \"Answer: \" << answer << \", Query: \" << GetParam();\n}\nINSTANTIATE_TEST_CASE_P(\n", "gpt_em_trim": 0, "gpt_bleu_trim": 63.43732, "model_code": "\n TEST_P(ValidProtoPaginationQueryValidatorTest, ValidPaginationQuery) { \n auto answer = validator.validate(GetParam()); \n ASSERT_TRUE(answer.hasErrors()); \n auto query = validator.validate(GetParam()); \n ASSERT_TRUE(query.hasErrors()); \n } \n INSTANTIATE_TEST_CASE_P(", "model_em_trim": 0, "model_bleu_trim": 62.95311318014893, "error_type_gpt": "Need Domain Knowledge"}
{"id": 343, "old": " \t\"github.com/onflow/flow-go/crypto\"\n \t\"github.com/onflow/flow-go/model/flow\"\n \t\"github.com/onflow/flow-go/module\"\n )\n // ControllerFactory is a factory object that creates new Controllers with the", "new": " \t\"github.com/onflow/flow-go/crypto\"\n \t\"github.com/onflow/flow-go/model/flow\"\n \t\"github.com/onflow/flow-go/module\"\n+\t\"github.com/onflow/flow-go/module/signature\"\n )\n // ControllerFactory is a factory object that creates new Controllers with the", "review": "Feels like this should live in the `crypto` package", "dataset": "codereview", "original_id": 2034, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\n\t\"github.com/onflow/flow-go/crypto\"\n\t\"github.com/onflow/flow-go/model/flow\"\n\t\"github.com/onflow/flow-go/module\"\n)\n\n// ControllerFactory is a factory object that creates new Controllers with the\n// given configuration.\ntype ControllerFactory struct {\n    cryptoService crypto.Service\n    flowClient    flow.Client\n    moduleClient  module.Client\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 77.11672, "model_code": "\n \"github.com/onflow/flow-go/crypto\" \n \"github.com/onflow/flow-go/model/flow\" \n ) \n // ControllerFactory is a factory object that creates new Controllers with the", "model_em_trim": 0, "model_bleu_trim": 51.16270857599152}
{"id": 344, "old": " \t\t}\n \t\ttotalCount += len(allDocs)\n-\t\t// Check that the same results are returned evenv if we use the proto converted query\n \t\tpbStructuredQuery, _ := query.ToProto()\n \t\tq := *iClient.Query()\n \t\tq, err = q.FromProto(pbStructuredQuery)", "new": " \t\t}\n \t\ttotalCount += len(allDocs)\n+\t\t// Check that the same results are returned even if we use the proto converted query\n \t\tpbStructuredQuery, _ := query.ToProto()\n \t\tq := *iClient.Query()\n \t\tq, err = q.FromProto(pbStructuredQuery)", "review": "```suggestion // Check that the same results are returned even if we use the proto converted query ```", "dataset": "codereview", "original_id": 2039, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "\n\t\t}\n\t\ttotalCount += len(allDocs)\n\t\t// Ensure consistent results with proto converted query\n\t\tpbStructuredQuery, _ := query.ToProto()\n\t\tq := *iClient.Query()\n\t\tq, err = q.FromProto(pbStructuredQuery)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 74.45811, "model_code": "\n } \n totalCount += len(allDocs) \n // Check that the same results are returned even if we use the proto converted query \n pbStructuredQuery, _ := query.ToProto() \n q := *iClient.Query() \n q, err = q.FromProto(pbStructuredQuery)", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 345, "old": "         else:\n             \"\"\"change non-float data to float data, need to copy\"\"\"\n             data = np.array(mat.reshape(mat.size), dtype=np.float32)\n-        ptr_data, type_ptr_data, new_data = c_float_array(data)\n         n_preds = self.__get_num_preds(num_iteration, mat.shape[0],\n                                        predict_type)\n         preds = np.zeros(n_preds, dtype=np.float64)", "new": "         else:\n             \"\"\"change non-float data to float data, need to copy\"\"\"\n             data = np.array(mat.reshape(mat.size), dtype=np.float32)\n+        ptr_data, type_ptr_data, _ = c_float_array(data)\n         n_preds = self.__get_num_preds(num_iteration, mat.shape[0],\n                                        predict_type)\n         preds = np.zeros(n_preds, dtype=np.float64)", "review": "@guolinke If we don't need `new_data` maybe it's better to name it like non-used variable: ``` ptr_data, type_ptr_data, _ = c_float_array(data) ``` ?", "dataset": "codereview", "original_id": 2089, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\n        else:\n            \"\"\"change non-float data to float data, need to copy\"\"\"\n            data = np.array(mat.reshape(mat.size), dtype=np.float32)\n        ptr_data, type_ptr_data, _ = c_float_array(data)\n        n_preds = self.__get_num_preds(num_iteration, mat.shape[0],\n                                       predict_type)\n        preds = np.zeros(n_preds, dtype=np.float64)\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n else: \n \"\"\"change non-float data to float data, need to copy\"\"\" \n data = np.array(mat.reshape(mat.size), dtype=np.float32) \n ptr_data, type_ptr_data, _ = c_float_array(data) \n n_preds = self.__get_num_preds(num_iteration, mat.shape[0], \n predict_type) \n preds = np.zeros(n_preds, dtype=np.float64)", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 346, "old": "     if ( config.Exists( \"vita_keep_aspect_ratio\" ) ) {\n         vita_keep_aspect_ratio = config.IntParams( \"vita_keep_aspect_ratio\" );\n-        fheroes2::Display::instance().engine()->vitaKeepAspectRatio = vita_keep_aspect_ratio;\n     }\n #endif", "new": "     if ( config.Exists( \"vita_keep_aspect_ratio\" ) ) {\n         vita_keep_aspect_ratio = config.IntParams( \"vita_keep_aspect_ratio\" );\n+        fheroes2::Display::instance().engine()->SetVitaKeepAspectRatio( vita_keep_aspect_ratio );\n     }\n #endif", "review": "This parameter we will re-implement again in the future.", "dataset": "codereview", "original_id": 2092, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "not", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n    if ( config.Exists( \"vita_keep_aspect_ratio\" ) ) {\n        vita_keep_aspect_ratio = config.IntParams( \"vita_keep_aspect_ratio\" );\n        fheroes2::Display::instance().engine()->setVitaKeepAspectRatio( vita_keep_aspect_ratio );\n    }\n#endif\n", "gpt_em_trim": 0, "gpt_bleu_trim": 100.0, "model_code": "\n if ( config.Exists( \"vita_keep_aspect_ratio\" ) ) { \n vita_keep_aspect_ratio = config.IntParams( \"vita_keep_aspect_ratio\" ); \n engine()->vitaKeepAspectRatio = vita_keep_aspect_ratio; \n } \n #endif", "model_em_trim": 0, "model_bleu_trim": 73.72703008153191, "error_type_gpt": "Code Style Difference"}
{"id": 347, "old": " namespace WP_Rocket\\Tests\\Unit\\Functions\\Options;\n use WP_Rocket\\Tests\\Unit\\TestCase;\n-use Brain\\Monkey;\n use Brain\\Monkey\\Functions;\n /**", "new": " namespace WP_Rocket\\Tests\\Unit\\Functions\\Options;\n use WP_Rocket\\Tests\\Unit\\TestCase;\n use Brain\\Monkey\\Functions;\n /**", "review": "You can remove this line too here, and in the `setUp()` function on line 12, remove the `Monkey\\setUp();` while keeping the `parent::setUp()`", "dataset": "codereview", "original_id": 2099, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nnamespace WP_Rocket\\Tests\\Unit\\Functions\\Options;\nuse WP_Rocket\\Tests\\Unit\\TestCase;\nuse Brain\\Monkey;\n\n/**\n```\n\nAnd in the `setUp()` function on line 12, remove the `Monkey\\setUp();` while keeping the `parent::setUp()`\n\n```\nprotected function setUp() {\n    parent::setUp();\n    Monkey\\tearDown();\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 82.36855, "model_code": "\n namespace WP_Rocket\\Tests\\Unit\\Functions\\Options; \n use WP_Rocket\\Tests\\Unit\\TestCase; \n use Brain\\Monkey\\Functions; \n /**", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 348, "old": " // convert from local encoding, then unescape escaped forbidden characters\n void FileSystemAccess::local2name(string *filename, const string *localPath) const\n {\n     string t = *filename;\n     local2path(&t, filename);", "new": " // convert from local encoding, then unescape escaped forbidden characters\n void FileSystemAccess::local2name(string *filename, const string *localPath) const\n {\n+    assert(filename);\n+\n     string t = *filename;\n     local2path(&t, filename);", "review": "Maybe we can check if `filename` or `localPath` are null", "dataset": "codereview", "original_id": 2103, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\n// convert from local encoding, then unescape escaped forbidden characters\nvoid FileSystemAccess::local2name(string *filename, const string *localPath) const\n{\n    if (!filename || !localPath) {\n        throw std::invalid_argument(\"filename or localPath is null\");\n    }\n    string t = *filename;\n    local2path(&t, filename);\n", "gpt_em_trim": 0, "gpt_bleu_trim": 59.59866, "model_code": "\n // convert from local encoding, then unescape escaped forbidden characters \n void FileSystemAccess::local2name(const string *filename, const string *localPath) const \n { \n if (!filename || !localPath) \n return; \n \n string t = *filename; \n local2path(&t, filename);", "model_em_trim": 0, "model_bleu_trim": 75.21939082918684, "error_type_gpt": "Code Style Difference"}
{"id": 349, "old": " \t\torig     *CloudAuditLogsSource\n \t\texpected *CloudAuditLogsSource\n \t}{\n-\t\t\"missing defaults\": {\n \t\t\torig: &CloudAuditLogsSource{\n \t\t\t\tObjectMeta: metav1.ObjectMeta{\n \t\t\t\t\tAnnotations: map[string]string{", "new": " \t\torig     *CloudAuditLogsSource\n \t\texpected *CloudAuditLogsSource\n \t}{\n+\t\t\"missing defaults, except cluster name annotations\": {\n \t\t\torig: &CloudAuditLogsSource{\n \t\t\t\tObjectMeta: metav1.ObjectMeta{\n \t\t\t\t\tAnnotations: map[string]string{", "review": "If I understand the intent of this test, this change should be reverted so that the original is missing the default annotation. This applies to all similar tests below.", "dataset": "codereview", "original_id": 2115, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n\t\torig     *CloudAuditLogsSource\n\t\texpected *CloudAuditLogsSource\n\t}{\n\t\t\"missing defaults\": {\n\t\t\torig: &CloudAuditLogsSource{\n\t\t\t\tObjectMeta: metav1.ObjectMeta{},\n\t\t\t},\n\t\t\texpected: &CloudAuditLogsSource{\n\t\t\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\t\t\tAnnotations: map[string]string{},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n", "gpt_em_trim": 0, "gpt_bleu_trim": 55.96853, "model_code": "\n orig     *CloudAuditLogsSource \n expected *CloudAuditLogsSource \n }{ \n \"missing default annotations\": { \n orig: &CloudAuditLogsSource{ \n ObjectMeta: metav1.ObjectMeta{ \n Annotations: map[string]string{", "model_em_trim": 0, "model_bleu_trim": 82.35627742811214}
{"id": 350, "old": "             $questionThemeName = SettingsUser::getUserSettingValue('preselectquestiontheme', null, null, null, Yii::app()->getConfig('preselectquestiontheme'));\n         }\n         if (empty($questionThemeName)) {\n-            $questionThemeName = QuestionTheme::model()->getBaseThemeNameForQuestionType($this->type);\n         }\n         $oCurrentGroup = QuestionGroup::model()->findByPk($gid);\n         $temporaryTitle =", "new": "             $questionThemeName = SettingsUser::getUserSettingValue('preselectquestiontheme', null, null, null, Yii::app()->getConfig('preselectquestiontheme'));\n         }\n         if (empty($questionThemeName)) {\n+            $questionThemeName = QuestionTheme::model()->getBaseThemeNameForQuestionType($questionType);\n         }\n         $oCurrentGroup = QuestionGroup::model()->findByPk($gid);\n         $temporaryTitle =", "review": "\"$this->type\" should be \"$questionType\" if im right", "dataset": "codereview", "original_id": 2123, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\n            $questionThemeName = SettingsUser::getUserSettingValue('preselectquestiontheme', null, null, null, Yii::app()->getConfig('preselectquestiontheme'));\n        }\n        if (empty($questionThemeName)) {\n            $questionThemeName = QuestionTheme::model()->getBaseThemeNameForQuestionType($questionType);\n        }\n        $oCurrentGroup = QuestionGroup::model()->findByPk($gid);\n        $temporaryTitle =\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n $questionThemeName = SettingsUser::getUserSettingValue('preselectquestiontheme', null, null, null, Yii::app()->getConfig('preselectquestiontheme')); \n } \n if (empty($questionThemeName)) { \n $questionThemeName = QuestionTheme::model()->getBaseThemeNameForQuestionType($questionType); \n } \n $oCurrentGroup = QuestionGroup::model()->findByPk($gid); \n $temporaryTitle =", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 351, "old": " \t\treturn ctxerr.Wrap(ctx, err, \"parsing enrolled\")\n \t}\n \tif !enrolled {\n-\t\t// TODO(lucas): Do we want to create a host_mdm entry when enrolled is false?\n-\t\t// When enrolled is false, all other columns are empty.\n \t\treturn nil\n \t}\n \tinstalledFromDep, err := strconv.ParseBool(rows[0][\"installed_from_dep\"])", "new": " \t\treturn ctxerr.Wrap(ctx, err, \"parsing enrolled\")\n \t}\n \tif !enrolled {\n+\t\t// A row with enrolled=false and all other columns empty is a host with the osquery\n+\t\t// MDM table extensions installed (e.g. Orbit) but MDM unconfigured/disabled.\n \t\treturn nil\n \t}\n \tinstalledFromDep, err := strconv.ParseBool(rows[0][\"installed_from_dep\"])", "review": "Hm, so you get a row but it's empty?", "dataset": "codereview", "original_id": 2124, "relevance": "not", "relevance_user1": "perfect", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nreturn ctxerr.Wrap(ctx, err, \"parsing enrolled\")\n}\nif !enrolled {\n\t// TODO(lucas): Do we want to create a host_mdm entry when enrolled is false?\n\t// When enrolled is false, all other columns are empty.\n\treturn nil\n}\n\nif len(rows) == 0 {\n    return errors.New(\"no rows returned\")\n}\n\ninstalledFromDep, err := strconv.ParseBool(rows[0][\"installed_from_dep\"])\n", "gpt_em_trim": 0, "gpt_bleu_trim": 49.41595, "model_code": "\n return ctxerr.Wrap(ctx, err, \"parsing enrolled\") \n } \n if !enrolled { \n return nil \n } \n installedFromDep, err := strconv.ParseBool(rows[0][\"installed_from_dep\"])", "model_em_trim": 0, "model_bleu_trim": 42.25635878221537}
{"id": 352, "old": "     **Default Window Length:** 1\n     \"\"\"\n     window_length = 1\n-    missing_value = nan\n     def _validate(self):\n         super(PeerCount, self)._validate()", "new": "     **Default Window Length:** 1\n     \"\"\"\n     window_length = 1\n     def _validate(self):\n         super(PeerCount, self)._validate()", "review": "missing value of nan is the default for factors with dtype float64 (which is the default dtype for factors), so we shouldn't need this.", "dataset": "codereview", "original_id": 2129, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n    **Default Window Length:** 1\n    \"\"\"\n    window_length = 1\n    def _validate(self):\n        super(PeerCount, self)._validate()\n```\n\nRevised code:\n\n```\n    **Default Window Length:** 1\n    \"\"\"\n    window_length = 1\n    def _validate(self):\n        if self.dtype == 'float64':\n            self.missing_value = nan\n        super(PeerCount, self)._validate()\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n **Default Window Length:** 1 \n \"\"\" \n window_length = 1 \n def _validate(self): \n super(PeerCount, self)._validate()", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 353, "old": " \tprivate static AdminApplication application;\n-\tprivate static JLineShellComponent shell;\n-\n \tprivate static CloudDataShell cloudDataShell;\n \t/**", "new": " \tprivate static AdminApplication application;\n \tprivate static CloudDataShell cloudDataShell;\n \t/**", "review": "It's worth adding a delegating stop() in the CloudDataShell, thus no need to have both the shell and cloudDataShell fields? (could just have the CloudDataShell assigned to a 'shell' field instead)", "dataset": "codereview", "original_id": 2136, "relevance": "not", "relevance_user1": "partial", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\nprivate static AdminApplication application;\nprivate static JLineShellComponent shell;\n\nprivate static CloudDataShell cloudDataShell;\n\nprivate static class DelegatingShell extends JLineShellComponent {\n    private final CloudDataShell cloudDataShell;\n\n    public DelegatingShell(CloudDataShell cloudDataShell) {\n        this.cloudDataShell = cloudDataShell;\n    }\n\n    @Override\n    public void stop() {\n        cloudDataShell.stop();\n    }\n}\n\nprivate static DelegatingShell delegatingShell;\n\n", "gpt_em_trim": 0, "gpt_bleu_trim": 42.97598, "model_code": "\n private static AdminApplication application; \n private static CloudDataShell cloudDataShell; \n /**", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 354, "old": "     that accepts exactly one argument (:meth:`~nvidia.dali.types.SampleInfo` objects that\n     represent the index of the requested sample).\n     If batch is set to True, the ``source`` can be either a callable, an iterable or a generator function.\n-    Callable in batch mode must accept exactly one argument - an integer that represents the index of the\n-    batch within the epoch that the callable should return.\n     Irrespective of ``batch`` value, callables should produce requested sample or batch solely based on\n     the SampleInfo instance or index in batch, so that they can be run in parallel in a number of workers.", "new": "     that accepts exactly one argument (:meth:`~nvidia.dali.types.SampleInfo` objects that\n     represent the index of the requested sample).\n     If batch is set to True, the ``source`` can be either a callable, an iterable or a generator function.\n+    Callable in batch mode must accept exactly one argument - either :meth:`~nvidia.dali.types.BatchInfo`\n+    instance or an integer (see `batch_info`).\n     Irrespective of ``batch`` value, callables should produce requested sample or batch solely based on\n     the SampleInfo instance or index in batch, so that they can be run in parallel in a number of workers.", "review": "We didn't add any reference to BatchInfo in the External Source doc? I'm surprised that we missed it, I think it should be added, not sure if here.", "dataset": "codereview", "original_id": 2155, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n    that accepts exactly one argument (:meth:`~nvidia.dali.types.SampleInfo` objects that\n    represent the index of the requested sample).\n    If batch is set to True, the ``source`` can be either a callable, an iterable or a generator function.\n    Callable in batch mode must accept exactly one argument - an integer that represents the index of the\n    batch within the epoch that the callable should return. The callable should produce requested sample or batch\n    solely based on the SampleInfo instance or index in batch, so that they can be run in parallel in a number of workers.\n    If the callable requires additional information about the batch, it can access it through the :meth:`~nvidia.dali.types.BatchInfo` object.\n    Irrespective of ``batch`` value, callables should produce requested sample or batch solely based on\n    the SampleInfo instance or index in batch, so that they can be run in parallel in a number of workers.\n", "gpt_em_trim": 0, "gpt_bleu_trim": 79.96235, "model_code": "\n that accepts exactly one argument (:meth:`~nvidia.dali.types.SampleInfo` objects that \n represent the index of the requested sample). \n If batch is set to True, the ``source`` can be either a callable, an iterable or a generator function. \n Callable in batch mode must accept exactly one argument - an integer that represents the index of the \n batch within the epoch that the callable should return. \n Irrespective of ``batch`` value, callables should produce requested sample or batch solely based on \n the SampleInfo instance or index in batch, so that they can be run in parallel in a number of workers.", "model_em_trim": 0, "model_bleu_trim": 79.96235467577903, "error_type_gpt": "Need Domain Knowledge"}
{"id": 355, "old": "             return dh\n     @classmethod\n-    def from_store(cls, path, basename, key_size, passphrase: str = None):\n         ca_path = os.path.join(path, basename + \"-ca.pem\")\n         if not os.path.exists(ca_path):\n             key, ca = cls.create_store(path, basename, key_size)", "new": "             return dh\n     @classmethod\n+    def from_store(cls, path, basename, key_size, passphrase: typing.Optional[bytes] = None):\n         ca_path = os.path.join(path, basename + \"-ca.pem\")\n         if not os.path.exists(ca_path):\n             key, ca = cls.create_store(path, basename, key_size)", "review": "Let's make this argument bytes right away and call `options.cert_passphrase.encode()` when calling it. The earlier we convert this, the more likely we are to not run into encoding issues. This also removes the slightly ugly `None if passphrase is None else`... at least here. :) ```suggestion def from_store(cls, path, basename, key_size, passphrase: typing.Optional[bytes] = None): ```", "dataset": "codereview", "original_id": 2172, "relevance": "partial", "relevance_user1": "perfect", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n    @classmethod\n    def from_store(cls, path, basename, key_size, passphrase: typing.Optional[bytes] = None):\n        ca_path = os.path.join(path, basename + \"-ca.pem\")\n        if not os.path.exists(ca_path):\n            key, ca = cls.create_store(path, basename, key_size)\n\n        if passphrase is not None:\n            passphrase = passphrase.encode()\n\n        return dh\n", "gpt_em_trim": 0, "gpt_bleu_trim": 97.59048, "model_code": "\n return dh \n @classmethod \n def from_store(cls, path, basename, key_size, passphrase: typing.Optional[bytes] = None): \n ca_path = os.path.join(path, basename + \"-ca.pem\") \n if not os.path.exists(ca_path): \n key, ca = cls.create_store(path, basename, key_size)", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 356, "old": "         }\n     if fulfillment.type_name == 'threshold-sha-256':\n-        subfulfillments = [\n             _fulfillment_to_details(cond['body'])\n             for cond in fulfillment.subconditions\n         ]\n         return {\n             'type': 'threshold-sha-256',\n             'threshold': fulfillment.threshold,\n-            'subfulfillments': subfulfillments,\n         }\n     raise UnsupportedTypeError(fulfillment.type_name)", "new": "         }\n     if fulfillment.type_name == 'threshold-sha-256':\n+        subconditions = [\n             _fulfillment_to_details(cond['body'])\n             for cond in fulfillment.subconditions\n         ]\n         return {\n             'type': 'threshold-sha-256',\n             'threshold': fulfillment.threshold,\n+            'subconditions': subconditions,\n         }\n     raise UnsupportedTypeError(fulfillment.type_name)", "review": "@r-marques The current default limit for threshold is defined at depth 10 here. Any take on if we should limit this? And if so, to what depth?", "dataset": "codereview", "original_id": 2178, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\n        }\n    if fulfillment.type_name == 'threshold-sha-256':\n        subfulfillments = [\n            _fulfillment_to_details(cond['body'], depth+1)\n            for cond in fulfillment.subconditions\n        ]\n        if depth >= 10:\n            raise ThresholdDepthLimitError()\n        return {\n            'type': 'threshold-sha-256',\n            'threshold': fulfillment.threshold,\n            'subfulfillments': subfulfillments,\n        }\n    raise UnsupportedTypeError(fulfillment.type_name)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 68.93933, "model_code": "\n } \n if fulfillment.type_name == 'threshold-sha-256': \n return { \n 'type': 'threshold-sha-256', \n 'threshold': fulfillment.threshold, \n 'subfulfillments': [ \n _fulfillment_to_details(cond['body']) \n for cond in fulfillment.subconditions \n ] \n } \n raise UnsupportedTypeError(fulfillment.type_name)", "model_em_trim": 0, "model_bleu_trim": 86.02335111712021}
{"id": 357, "old": "                 schema, new_scalar, catenate=False)\n             if needs_recreate:\n-                cond = dbops.EnumExists(type_name)\n                 self.pgops.add(\n-                    dbops.DropEnum(name=type_name, conditions=[cond]))\n                 self.pgops.add(dbops.CreateEnum(\n                     dbops.Enum(name=type_name, values=new_enum_values)))", "new": "                 schema, new_scalar, catenate=False)\n             if needs_recreate:\n                 self.pgops.add(\n+                    dbops.DropEnum(name=type_name))\n                 self.pgops.add(dbops.CreateEnum(\n                     dbops.Enum(name=type_name, values=new_enum_values)))", "review": "Why is the condition necessary?", "dataset": "codereview", "original_id": 2183, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-remove", "gpt_code": "\nschema, new_scalar, catenate=False)\nif needs_recreate:\n    type_name = new_scalar.enum_name\n    # Check if the enum type already exists before dropping it\n    if self.pgops.enum_exists(type_name):\n        cond = dbops.EnumExists(type_name)\n        self.pgops.add(dbops.DropEnum(name=type_name, conditions=[cond]))\n    self.pgops.add(dbops.CreateEnum(\n        dbops.Enum(name=type_name, values=new_enum_values)))\n", "gpt_em_trim": 0, "gpt_bleu_trim": 58.96244, "model_code": "\n schema, new_scalar, catenate=False) \n if needs_recreate: \n self.pgops.add( \n dbops.DropEnum(name=type_name, conditions=[dbops.EnumExists(type_name)])) \n self.pgops.add(dbops.CreateEnum( \n dbops.Enum(name=type_name, values=new_enum_values)))", "model_em_trim": 0, "model_bleu_trim": 81.45568103411937, "error_type_gpt": "Need Domain Knowledge"}
{"id": 358, "old": "     describe \"rerun command for failed examples\" do\n       it \"uses the location to identify the example\" do\n         example_group = RSpec.describe(\"example group\") do\n           it(\"fails\") { fail }\n         end\n-        line = __LINE__ - 2\n         expect(output_from_running example_group).to include(\"rspec #{RSpec::Core::Metadata::relative_path(\"#{__FILE__}:#{line}\")} # example group fails\")\n       end", "new": "     describe \"rerun command for failed examples\" do\n       it \"uses the location to identify the example\" do\n+        line = __LINE__ + 2\n         example_group = RSpec.describe(\"example group\") do\n           it(\"fails\") { fail }\n         end\n         expect(output_from_running example_group).to include(\"rspec #{RSpec::Core::Metadata::relative_path(\"#{__FILE__}:#{line}\")} # example group fails\")\n       end", "review": "whilst this is not a change made by this pull request, it is weird to me that the line number used is after the example here and before the example in the next spec, can we unify them?", "dataset": "codereview", "original_id": 2200, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Refactoring-swap", "category_type_user1": "Refactoring-swap", "category_type_user2": "Refactoring-swap", "gpt_code": "\ndescribe \"rerun command for failed examples\" do\n  it \"uses the location to identify the example\" do\n    example_group = RSpec.describe(\"example group\") do\n      it(\"fails\") { fail }\n    end\n    line = __LINE__ - 3\n    expect(output_from_running example_group).to include(\"rspec #{RSpec::Core::Metadata::relative_path(\"#{__FILE__}:#{line}\")} # example group fails\")\n  end\nend\n", "gpt_em_trim": 0, "gpt_bleu_trim": 93.06003, "model_code": "\n describe \"rerun command for failed examples\" do \n it \"uses the location to identify the example\" do \n example_group = RSpec.describe(\"example group\") \n it { is_expected.to be_expected } \n line = __LINE__ - 2 \n expect(output_from_running example_group).to include(\"rspec #{RSpec::Core::Metadata::relative_path(\"#{__FILE__}:#{line}\")} # example group fails\") \n end", "model_em_trim": 0, "model_bleu_trim": 84.60730038177499, "error_type_gpt": "Unclear Changes"}
{"id": 359, "old": "     <h1>Error 503 Backend is unhealthy</h1>\n     <p>Backend is unhealthy</p>\n     <h3>Guru Mediation:</h3>\n-    <p>Details: cache-sea4479-SEA 1645540621 3009827649</p>\n     <hr>\n     <p>Varnish cache server</p>\n   </body>", "new": "     <h1>Error 503 Backend is unhealthy</h1>\n     <p>Backend is unhealthy</p>\n     <h3>Guru Mediation:</h3>\n+    <p>Details: cache-sea4449-SEA 1645540621 1476532598</p>\n     <hr>\n     <p>Varnish cache server</p>\n   </body>", "review": "Revert this file.", "dataset": "codereview", "original_id": 2201, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n    <h1>Error 503 Backend is unhealthy</h1>\n    <p>Backend is unhealthy</p>\n    <h3>Guru Mediation:</h3>\n    <p>Details: cache-sea4479-SEA 1645540621 3009827649</p>\n    <hr>\n    <p>Varnish cache server</p>\n  </body>\n", "gpt_em_trim": 0, "gpt_bleu_trim": 92.10399, "model_code": "\n <h1>Error 503 Backend is unhealthy</h1> \n <p>Backend is unhealthy</p> \n <h3>Guru Mediation:</h3> \n <p>Details: cache-sea4481-SEA 1645540621 216322325</p> \n <hr> \n <p>Varnish cache server</p> \n </body>", "model_em_trim": 0, "model_bleu_trim": 92.10399279903714}
{"id": 360, "old": " \t// auto inject saylinks (say)\n \tif (RuleB(Chat, AutoInjectSaylinksToSay)) {\n \t\tstd::string new_message = EQ::SayLinkEngine::InjectSaylinksIfNotExist(message);\n-\t\tbuf.WriteString(new_message.c_str());\n \t}\n \telse {\n \t\tbuf.WriteString(message);", "new": " \t// auto inject saylinks (say)\n \tif (RuleB(Chat, AutoInjectSaylinksToSay)) {\n \t\tstd::string new_message = EQ::SayLinkEngine::InjectSaylinksIfNotExist(message);\n+\t\tbuf.WriteString(new_message);\n \t}\n \telse {\n \t\tbuf.WriteString(message);", "review": "`c_str()` is not needed.", "dataset": "codereview", "original_id": 2209, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n\t// auto inject saylinks (say)\n\tif (RuleB(Chat, AutoInjectSaylinksToSay)) {\n\t\tstd::string new_message = EQ::SayLinkEngine::InjectSaylinksIfNotExist(message);\n\t\tbuf.WriteString(new_message);\n\t}\n\telse {\n\t\tbuf.WriteString(message);\n", "gpt_em_trim": 1, "gpt_bleu_trim": 98.22183, "model_code": "\n // auto inject saylinks (say) \n if (RuleB(Chat, AutoInjectSaylinksToSay)) { \n std::string new_message = EQ::SayLinkEngine::InjectSaylinksIfNotExist(message); \n buf.WriteString(new_message); \n } \n else { \n buf.WriteString(message);", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 361, "old": "       expect_node_error(Bolt::Node::ConnectError,\n                         'CONNECT_ERROR',\n                         /Failed to connect to/) do\n-        ssh.with_connection(make_target(port_: 65535)) {}\n       end\n       exec_time = Time.now - exec_time\n       expect(exec_time).to be < 1", "new": "       expect_node_error(Bolt::Node::ConnectError,\n                         'CONNECT_ERROR',\n                         /Failed to connect to/) do\n+        ssh.with_connection(make_target(h: hostname, p: 65535)) {}\n       end\n       exec_time = Time.now - exec_time\n       expect(exec_time).to be < 1", "review": "Does this have a default host_ now?", "dataset": "codereview", "original_id": 2225, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n      expect_node_error(Bolt::Node::ConnectError,\n                        'CONNECT_ERROR',\n                        /Failed to connect to/) do\n        ssh.with_connection(make_target(host_: 'default_host', port_: 65535)) {}\n      end\n      exec_time = Time.now - exec_time\n      expect(exec_time).to be < 1\n", "gpt_em_trim": 0, "gpt_bleu_trim": 86.64852, "model_code": "\n expect_node_error(Bolt::Node::ConnectError, \n 'CONNECT_ERROR', \n /Failed to connect to/) do \n ssh.with_connection(make_target(host_: 'localhost')) {} \n end \n exec_time = Time.now - exec_time \n expect(exec_time).to be < 1", "model_em_trim": 0, "model_bleu_trim": 88.39320631766282, "error_type_gpt": "Need Domain Knowledge"}
{"id": 362, "old": "     private void analyzeTables() {\n         // take a local copy and clear because in rare cases we can call\n-        // back into markTableForAnalyzer while iterating here\n         HashSet<Table> tablesToAnalyzeLocal = tablesToAnalyze;\n         tablesToAnalyze = null;\n         int rowCount = getDatabase().getSettings().analyzeSample / 10;", "new": "     private void analyzeTables() {\n         // take a local copy and clear because in rare cases we can call\n+        // back into markTableForAnalyze while iterating here\n         HashSet<Table> tablesToAnalyzeLocal = tablesToAnalyze;\n         tablesToAnalyze = null;\n         int rowCount = getDatabase().getSettings().analyzeSample / 10;", "review": "`markTableForAnalyze` (without r)", "dataset": "codereview", "original_id": 2244, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "\n    private void analyzeTables() {\n        // take a local copy and clear because in rare cases we can call\n        // back into markTableForAnalyze while iterating here\n        HashSet<Table> tablesToAnalyzeLocal = tablesToAnalyze;\n        tablesToAnalyze = null;\n        int rowCount = getDatabase().getSettings().analyzeSample / 10;\n", "gpt_em_trim": 0, "gpt_bleu_trim": 95.89575, "model_code": "\n private void analyzeTables() { \n // take a local copy and clear because in rare cases we can call \n // back into markTableForAnalyze while iterating here \n HashSet<Table> tablesToAnalyzeLocal = tablesToAnalyze; \n tablesToAnalyze = null; \n int rowCount = getDatabase().getSettings().analyzeSample / 10;", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 363, "old": "  */\n #include <utils/s2n_socket.h>\n void s2n_socket_quickack_harness()\n {\n-  struct s2n_socket_read_io_context *cbmc_allocate_s2n_socket_read_io_context();\n-  /* Non-deterministic inputs. */\n   struct s2n_connection *s2n_connection = malloc(sizeof(*s2n_connection));\n   if (s2n_connection != NULL) {\n     s2n_connection->recv_io_context = cbmc_allocate_s2n_socket_read_io_context();", "new": "  */\n #include <utils/s2n_socket.h>\n+#include <cbmc_proof/make_common_datastructures.h>\n void s2n_socket_quickack_harness()\n {\n+  /* Non-deterministic inputs. */  \n   struct s2n_connection *s2n_connection = malloc(sizeof(*s2n_connection));\n   if (s2n_connection != NULL) {\n     s2n_connection->recv_io_context = cbmc_allocate_s2n_socket_read_io_context();", "review": "It looks like `result` is always going to be `0` i.e. `S2N_SUCCESS`. Why not just have `assert(result == S2N_SUCCESS)`?", "dataset": "codereview", "original_id": 2251, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\n */\n#include <utils/s2n_socket.h>\nvoid s2n_socket_quickack_harness()\n{\n  struct s2n_socket_read_io_context *cbmc_allocate_s2n_socket_read_io_context();\n  /* Non-deterministic inputs. */\n  struct s2n_connection *s2n_connection = malloc(sizeof(*s2n_connection));\n  if (s2n_connection != NULL) {\n    s2n_connection->recv_io_context = cbmc_allocate_s2n_socket_read_io_context();\n    int result = s2n_socket_quickack(s2n_connection);\n    assert(result == S2N_SUCCESS);\n  }\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 76.50596, "model_code": "\n */ \n #include <utils/s2n_socket.h> \n int s2n_socket_quickack_harness() \n { \n struct s2n_socket_read_io_context *cbmc_allocate_s2n_socket_read_io_context(); \n /* Non-deterministic inputs. */ \n int result = 0; \n struct s2n_connection *s2n_connection = malloc(sizeof(*s2n_connection)); \n if (s2n_connection != NULL) { \n s2n_connection->recv_io_context = cbmc_allocate_s2n_socket_read_io_context();", "model_em_trim": 0, "model_bleu_trim": 71.31611524409817}
{"id": 364, "old": " \t}\n \tassert.Equal(t, \"\", runAppForTest(t, []string{\"logout\"}))\n }", "new": " \t}\n \tassert.Equal(t, \"\", runAppForTest(t, []string{\"logout\"}))\n+\tassert.True(t, ds.DestroySessionFuncInvoked)\n }", "review": "Should probably check that the mocks got called, to make sure it runs as expected.", "dataset": "codereview", "original_id": 2258, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\n\t// Assert that the mocks were called\n\tmock.AssertExpectations(t)\n\t\n\t// Call the logout command and assert that it returns an empty string\n\tassert.Equal(t, \"\", runAppForTest(t, []string{\"logout\"}))\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 62.95544, "model_code": "\n } \n assert.Equal(t, \"\", runAppForTest(t, []string{\"logout\"})) \n assert.NotEqual(t, \"logout\", runAppForTest(t, []string{\"logout\"})) \n }", "model_em_trim": 0, "model_bleu_trim": 56.72875553615373, "error_type_gpt": "Need Domain Knowledge"}
{"id": 365, "old": "   })\n );\n-const DESCRIPTION = \"Sets how the visualization size should be determined. If a string, should be one of `\\\"pad\\\"`, `\\\"fit\\\"` or `\\\"none\\\"`.\\nObject values can additionally specify parameters for content sizing and automatic resizing.\\n`\\\"fit\\\"` is only supported for single and layered views that don't use `rangeStep`.\\n\\n__Default value__: `pad`\";\n-\n-Object.assign(autosize, { description: DESCRIPTION });\n export default {\n   defs: {", "new": "   })\n );\n+Object.assign(autosize, { description: \"Sets how the visualization size should be determined. If a string, should be one of `\\\"pad\\\"`, `\\\"fit\\\"` or `\\\"none\\\"`.\\nObject values can additionally specify parameters for content sizing and automatic resizing.\\n`\\\"fit\\\"` is only supported for single and layered views that don't use `rangeStep`.\\n\\n__Default value__: `pad`\" });\n export default {\n   defs: {", "review": "I think it would be better to put the text into the object itself rather than assigning it later.", "dataset": "codereview", "original_id": 2262, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nconst autosize = {\n  type: 'string',\n  enum: ['pad', 'fit', 'none'],\n  default: 'pad',\n  description: 'Sets how the visualization size should be determined. If a string, should be one of `\\\"pad\\\"`, `\\\"fit\\\"` or `\\\"none\\\"`.\\nObject values can additionally specify parameters for content sizing and automatic resizing.\\n`\\\"fit\\\"` is only supported for single and layered views that don\\'t use `rangeStep`.\\n\\n__Default value__: `pad`'\n};\n\nexport default {\n  defs: {\n    autosize\n  }\n};\n", "gpt_em_trim": 0, "gpt_bleu_trim": 76.49486, "model_code": "\n }) \n ); \n Object.assign(autosize, { description: DESCRIPTION }); \n export default { \n defs: {", "model_em_trim": 0, "model_bleu_trim": 1.6190444974239164, "error_type_gpt": "Unclear Changes"}
{"id": 366, "old": "                      */\n                     updater.updateState(upload.id, TransferState.WAITING_FOR_NETWORK);\n                     LOGGER.debug(\"Network Connection Interrupted: \" + \"Moving the TransferState to WAITING_FOR_NETWORK\");\n                     progressListener.progressChanged(new ProgressEvent(0));\n                     return false;\n                 }", "new": "                      */\n                     updater.updateState(upload.id, TransferState.WAITING_FOR_NETWORK);\n                     LOGGER.debug(\"Network Connection Interrupted: \" + \"Moving the TransferState to WAITING_FOR_NETWORK\");\n+                    ProgressEvent resetEvent = new ProgressEvent(0);\n+                    resetEvent.setEventCode(ProgressEvent.RESET_EVENT_CODE);\n                     progressListener.progressChanged(new ProgressEvent(0));\n                     return false;\n                 }", "review": "Same as above. If `eventCode` is not being used anywhere should we consider removing it altogether from the `ProgressEvent` class?", "dataset": "codereview", "original_id": 2270, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\n                    updater.updateState(upload.id, TransferState.WAITING_FOR_NETWORK);\n                    LOGGER.debug(\"Network Connection Interrupted: \" + \"Moving the TransferState to WAITING_FOR_NETWORK\");\n                    progressListener.progressChanged(new ProgressEvent());\n                    return false;\n                }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 65.11955, "model_code": "\n */ \n updater.updateState(upload.id, TransferState.WAITING_FOR_NETWORK); \n LOGGER.debug(\"Network Connection Interrupted: \" + \"Moving the TransferState to WAITING_FOR_NETWORK\"); \n progressListener.progressChanged(); \n return false; \n }", "model_em_trim": 0, "model_bleu_trim": 58.567705922563285}
{"id": 367, "old": "         self.assertAlmostEqual(evals_result['valid_0']['binary_logloss'][-1], ret, places=5)\n         params['feature_fraction'] = 0.5\n         gbm2 = lgb.train(params, lgb_train,\n-                         num_boost_round=25,\n-                         valid_sets=lgb_eval,\n-                         verbose_eval=False,\n-                         evals_result=evals_result)\n         ret2 = log_loss(y_test, gbm2.predict(X_test))\n         self.assertNotEqual(ret, ret2)", "new": "         self.assertAlmostEqual(evals_result['valid_0']['binary_logloss'][-1], ret, places=5)\n         params['feature_fraction'] = 0.5\n         gbm2 = lgb.train(params, lgb_train,\n+                         num_boost_round=25)\n         ret2 = log_loss(y_test, gbm2.predict(X_test))\n         self.assertNotEqual(ret, ret2)", "review": "Seems that valid set is not used in the further code.", "dataset": "codereview", "original_id": 2271, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n        self.assertAlmostEqual(evals_result['valid_0']['binary_logloss'][-1], ret, places=5)\n        params['feature_fraction'] = 0.5\n        gbm2 = lgb.train(params, lgb_train,\n                         num_boost_round=25,\n                         valid_sets=lgb_eval,\n                         verbose_eval=False)\n        ret2 = log_loss(y_test, gbm2.predict(X_test))\n        self.assertNotEqual(ret, ret2)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 75.03706, "model_code": "\n self.assertAlmostEqual(evals_result['valid_0']['binary_logloss'][-1], ret, places=5) \n params['feature_fraction'] = 0.5 \n gbm2 = lgb.train(params, lgb_train, \n num_boost_round=25, evals_result=evals_result) \n ret2 = log_loss(y_test, gbm2.predict(X_test)) \n self.assertNotEqual(ret, ret2)", "model_em_trim": 0, "model_bleu_trim": 90.31277908549723, "error_type_gpt": "Need Domain Knowledge"}
{"id": 368, "old": " \t\t\tif err != nil {\n \t\t\t\t_ = p2ps.Disconnect(overlay)\n \t\t\t\tlogger.Debugf(\"addressboook error persisting %s %s: %v\", aa, overlay, err)\n-\t\t\t\tlogger.Errorf(\"error persisting node %s\", aa)\n \t\t\t\treturn\n \t\t\t}", "new": " \t\t\tif err != nil {\n \t\t\t\t_ = p2ps.Disconnect(overlay)\n \t\t\t\tlogger.Debugf(\"addressboook error persisting %s %s: %v\", aa, overlay, err)\n+\t\t\t\tlogger.Errorf(\"persisting node %s\", aa)\n \t\t\t\treturn\n \t\t\t}", "review": "No need for error word.", "dataset": "codereview", "original_id": 2277, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n\t\t\tif err != nil {\n\t\t\t\t_ = p2ps.Disconnect(overlay)\n\t\t\t\tlogger.Debugf(\"addressboook persisting %s %s failed: %v\", aa, overlay, err)\n\t\t\t\tlogger.Errorf(\"node %s persistence failed\", aa)\n\t\t\t\treturn\n\t\t\t}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 82.16223, "model_code": "\n if err != nil { \n _ = p2ps.Disconnect(overlay) \n logger.Debugf(\"addressboook error persisting %s %s: %v\", aa, overlay, err) \n logger.Errorf(\"error persisting node %s\", aa) \n return \n }", "model_em_trim": 0, "model_bleu_trim": 95.15586241253386, "error_type_gpt": "Unclear Location"}
{"id": 369, "old": "             html += '<div class=\"' + cssClass + '\">';\n-            var moreIcon = '&#xe5d4';\n             html += getTextLinesHtml(textlines, isLargeStyle);", "new": "             html += '<div class=\"' + cssClass + '\">';\n+            const moreIcon = 'more_horiz';\n             html += getTextLinesHtml(textlines, isLargeStyle);", "review": "why do we use hex code here?", "dataset": "codereview", "original_id": 2286, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n            html += '<div class=\"' + cssClass + '\">';\n            var moreIcon = '\u25bc';\n            html += getTextLinesHtml(textlines, isLargeStyle);\n", "gpt_em_trim": 0, "gpt_bleu_trim": 80.3681, "model_code": "\n html += '<div class=\"' + cssClass + '\">'; \n html += getTextLinesHtml(textlines, isLargeStyle);", "model_em_trim": 0, "model_bleu_trim": 72.60401186217557, "error_type_gpt": "Need Domain Knowledge"}
{"id": 370, "old": "         fastrtps::rtps::RTPSParticipantAttributes patt;\n         set_attributes_from_qos(patt, qos_);\n         rtps_participant_->update_attributes(patt);\n     }\n     return ReturnCode_t::RETCODE_OK;", "new": "         fastrtps::rtps::RTPSParticipantAttributes patt;\n         set_attributes_from_qos(patt, qos_);\n         rtps_participant_->update_attributes(patt);\n+\n+        // Reset flag\n+        qos_.user_data().hasChanged = false;\n     }\n     return ReturnCode_t::RETCODE_OK;", "review": "I think this was to be done inside, which is the call right above this one ``` DomainParticipantImpl::set_qos( DomainParticipantQos& /*to*/, const DomainParticipantQos& /*from*/, bool /*first_time*/) ```", "dataset": "codereview", "original_id": 2308, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\n        fastrtps::rtps::RTPSParticipantAttributes patt;\n        set_attributes_from_qos(patt, qos_);\n        rtps_participant_->update_attributes(patt);\n        DomainParticipantImpl::set_qos(qos_, qos_, true);\n    }\n    return ReturnCode_t::RETCODE_OK;\n", "gpt_em_trim": 0, "gpt_bleu_trim": 67.05372, "model_code": "\n fastrtps::rtps::RTPSParticipantAttributes patt; \n set_attributes_from_qos(patt, qos_); \n rtps_participant_->update_attributes(patt); \n return ReturnCode_t::RETCODE_OK; \n } \n return ReturnCode_t::RETCODE_OK;", "model_em_trim": 0, "model_bleu_trim": 72.76914122432254}
{"id": 371, "old": " package org.flowable.editor.language.xml;\n import java.util.List;\n-import org.assertj.core.api.Assertions;\n import org.flowable.bpmn.converter.BpmnXMLConverter;\n import org.flowable.bpmn.model.Activity;\n import org.flowable.bpmn.model.BpmnModel;", "new": " package org.flowable.editor.language.xml;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.util.List;\n import org.flowable.bpmn.converter.BpmnXMLConverter;\n import org.flowable.bpmn.model.Activity;\n import org.flowable.bpmn.model.BpmnModel;", "review": "If you staticly import `assertThat` then lines like this can be written as `assertThat(dataInputAssociations),isNotEmpty()`. There are 2 other occurrences in this test.", "dataset": "codereview", "original_id": 2314, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\npackage org.flowable.editor.language.xml;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport java.util.List;\n\nimport org.flowable.bpmn.converter.BpmnXMLConverter;\nimport org.flowable.bpmn.model.Activity;\nimport org.flowable.bpmn.model.BpmnModel;\n\npublic class MyTestClass {\n\n    public void myTestMethod() {\n        BpmnModel bpmnModel = new BpmnXMLConverter().convertToBpmnModel(inputStream);\n        Activity activity = bpmnModel.getProcesses().get(0).getFlowElement(\"userTask1\", true);\n        List dataInputAssociations = activity.getDataInputAssociations();\n        assertThat(dataInputAssociations).isNotEmpty();\n    }\n}\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n package org.flowable.editor.language.xml; \n import java.util.List; \n import org.flowable.bpmn.converter.BpmnXMLConverter; \n import org.flowable.bpmn.model.Activity; \n import org.flowable.bpmn.model.BpmnModel;", "model_em_trim": 0, "model_bleu_trim": 76.00389545477263}
{"id": 372, "old": "           [this] { return CommandVariantType(variant_); }()};\n     };\n-    Command::Command(const Command &o) {\n-      this->impl_ = std::make_unique<Impl>(*o.impl_->proto_);\n-    }\n     Command::Command(Command &&o) noexcept = default;\n     Command::Command(const TransportType &ref) {", "new": "           [this] { return CommandVariantType(variant_); }()};\n     };\n+    Command::Command(const Command &o) : Command(*o.impl_->proto_) {}\n     Command::Command(Command &&o) noexcept = default;\n     Command::Command(const TransportType &ref) {", "review": "It should be possible to use delegating constructor: `: Command(*o.impl_->proto_)`", "dataset": "codereview", "original_id": 2334, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n    Command::Command(const Command &o) : Command(*o.impl_->proto_) {}\n    \n    Command::Command(Command &&o) noexcept = default;\n    \n    Command::Command(const TransportType &ref) {\n        this->impl_ = std::make_unique<Impl>(ref);\n    }\n", "gpt_em_trim": 0, "gpt_bleu_trim": 70.26185, "model_code": "\n [this] { return CommandVariantType(variant_); }()}; \n }; \n Command::Command(const Command &o) : Command(*o.impl_->proto_) { \n } \n Command::Command(Command &&o) noexcept = default; \n Command::Command(const TransportType &ref) {", "model_em_trim": 0, "model_bleu_trim": 100.0, "error_type_gpt": "Insignificant Omission"}
{"id": 373, "old": "                 if (outpath)\n                 {\n #ifdef WIN32\n-                    // todo:  thia whole function should be adjusted to use LocalPath member functions instead of manipulating its internal strings, so we don't need any ifdefs\n                     std::wstring s(ptr, localpath.getLocalpath().data() - ptr + localpath.getLocalpath().size());\n #else\n                     std::string s(ptr, localpath.getLocalpath().data() - ptr + localpath.getLocalpath().size());", "new": "                 if (outpath)\n                 {\n #ifdef WIN32\n+                    // todo:  this whole function should be adjusted to use LocalPath member functions instead of manipulating its internal strings, so we don't need any ifdefs\n                     std::wstring s(ptr, localpath.getLocalpath().data() - ptr + localpath.getLocalpath().size());\n #else\n                     std::string s(ptr, localpath.getLocalpath().data() - ptr + localpath.getLocalpath().size());", "review": "```suggestion // todo: this whole function should be adjusted to use LocalPath member functions instead of manipulating its internal strings, so we don't need any ifdefs ```", "dataset": "codereview", "original_id": 2339, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "\nif (outpath)\n{\n    std::string s;\n    localpath.getLocalpath(s);\n#ifdef WIN32\n    std::wstring ws;\n    localpath.getLocalpath(ws);\n#endif\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 7.312738, "model_code": "\n if (outpath) \n { \n #ifdef WIN32 \n // todo:  this whole function should be adjusted to use LocalPath member functions instead of manipulating its internal strings, so we don't need any ifdefs \n std::wstring s(ptr, localpath.getLocalpath().data() - ptr + localpath.getLocalpath().size()); \n #else \n std::string s(ptr, localpath.getLocalpath().data() - ptr + localpath.getLocalpath().size());", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Changes"}
{"id": 374, "old": "   def view_follows\n     @member = Member.confirmed.find(params[:login_name])\n-    @follows = @member.followed\n   end\n   def view_followers\n     @member = Member.confirmed.find(params[:login_name])\n-    @followers = @member.followers\n   end\n end", "new": "   def view_follows\n     @member = Member.confirmed.find(params[:login_name])\n+    @follows = @member.followed.paginate(:page => params[:page])\n   end\n   def view_followers\n     @member = Member.confirmed.find(params[:login_name])\n+    @followers = @member.followers.paginate(:page => params[:page])\n   end\n end", "review": "Do we want to make these available as JSON and RSS? (We don't necessarily have to do this right now but we should at least think about it. I think both would be good!)", "dataset": "codereview", "original_id": 2346, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n  def view_follows\n    @member = Member.confirmed.find(params[:login_name])\n    @follows = @member.followed\n\n    respond_to do |format|\n      format.html\n      format.json { render json: @follows }\n      format.rss { render layout: false }\n    end\n  end\n\n  def view_followers\n    @member = Member.confirmed.find(params[:login_name])\n    @followers = @member.followers\n\n    respond_to do |format|\n      format.html\n      format.json { render json: @followers }\n      format.rss { render layout: false }\n    end\n  end\nend\n", "gpt_em_trim": 0, "gpt_bleu_trim": 48.02926, "model_code": "\n def view_follows \n @member = Member.confirmed.find(params[:login_name]) \n @follows = @member.followed.to_json \n end \n def view_followers \n @member = Member.confirmed.find(params[:login_name]) \n @followers = @member.followers.to_json \n end \n end", "model_em_trim": 0, "model_bleu_trim": 66.53939992591779}
{"id": 375, "old": "                 return;\n             }\n-            // call method using named method arguments\n             var invokeWith = bundle?.CreateArgumentList(parameters, viewModel.GetType().Name)\n                                    .ToArray();\n             methodInfo.Invoke(viewModel, invokeWith);", "new": "                 return;\n             }\n+            // call method using named method arguments. If bundle is null, the null-check makes sure that Init still is called.\n             var invokeWith = bundle?.CreateArgumentList(parameters, viewModel.GetType().Name)\n                                    .ToArray();\n             methodInfo.Invoke(viewModel, invokeWith);", "review": "Suddenly we get here and `bundle` can be null? This seems like a contract violation since previously in our history `bundle` could never be null.", "dataset": "codereview", "original_id": 2376, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\nif (bundle == null)\n{\n    throw new ArgumentNullException(nameof(bundle));\n}\n\n// call method using named method arguments\nvar invokeWith = bundle.CreateArgumentList(parameters, viewModel.GetType().Name)\n                       .ToArray();\nmethodInfo.Invoke(viewModel, invokeWith);\n", "gpt_em_trim": 0, "gpt_bleu_trim": 55.15436, "model_code": "\n return; \n } \n // call method using named method arguments \n var invokeWith = bundle.CreateArgumentList(parameters, viewModel.GetType().Name) \n .ToArray(); \n methodInfo.Invoke(viewModel, invokeWith);", "model_em_trim": 0, "model_bleu_trim": 59.431174322337235, "error_type_gpt": "Need Domain Knowledge"}
{"id": 376, "old": "         if print_stats:\n             for key in player_stats:\n-                 print(\"[#] -- %s: %s\" % (key, player_stats[key]))\n         return json.dumps(player_stats, indent=4)", "new": "         if print_stats:\n             for key in player_stats:\n+                 print('[#] -- {}: {}'.format(key, player_stats[key]))\n         return json.dumps(player_stats, indent=4)", "review": "I would change this to `print(\"[#] -- {}: {}\".format(key, player_stats[key]))` - to keep consistency", "dataset": "codereview", "original_id": 2398, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "\nif print_stats:\n    for key in player_stats:\n        print(\"[#] -- {}: {}\".format(key, player_stats[key]))\nreturn json.dumps(player_stats, indent=4)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 90.05992, "model_code": "\n if print_stats: \n for key in player_stats: \n print(\"[#] -- {}: {}\".format(key, player_stats[key])) \n return json.dumps(player_stats, indent=4)", "model_em_trim": 0, "model_bleu_trim": 90.05991744953471, "error_type_gpt": "Code Style Difference"}
{"id": 377, "old": " use Wallabag\\CoreBundle\\Entity\\Entry;\n use Wallabag\\CoreBundle\\Tools\\Utils;\n use Symfony\\Component\\HttpFoundation\\File\\MimeType\\MimeTypeExtensionGuesser;\n-use Symfony\\Component\\Validator\\Constraints\\Language as LanguageConstraint;\n use Symfony\\Component\\Validator\\Constraints\\Url as UrlConstraint;\n use Symfony\\Component\\Validator\\Validator\\ValidatorInterface;", "new": " use Wallabag\\CoreBundle\\Entity\\Entry;\n use Wallabag\\CoreBundle\\Tools\\Utils;\n use Symfony\\Component\\HttpFoundation\\File\\MimeType\\MimeTypeExtensionGuesser;\n+use Symfony\\Component\\Validator\\Constraints\\Locale as LocaleConstraint;\n use Symfony\\Component\\Validator\\Constraints\\Url as UrlConstraint;\n use Symfony\\Component\\Validator\\Validator\\ValidatorInterface;", "review": "Language is `en` and not `en_GB` for instance. However, the OpenGraph `og:locale` format is `language_TERRITORY`, so it should be `Symfony\\Component\\Validator\\Constraints\\Locale` instead. Handling merging `en_GB` and `en_US` into `en` (for filters) should be done somewhere else.", "dataset": "codereview", "original_id": 2408, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nuse Wallabag\\CoreBundle\\Entity\\Entry;\nuse Wallabag\\CoreBundle\\Tools\\Utils;\nuse Symfony\\Component\\HttpFoundation\\File\\MimeType\\MimeTypeExtensionGuesser;\nuse Symfony\\Component\\Validator\\Constraints\\Locale as LocaleConstraint;\nuse Symfony\\Component\\Validator\\Constraints\\Url as UrlConstraint;\nuse Symfony\\Component\\Validator\\Validator\\ValidatorInterface;\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n use Wallabag\\CoreBundle\\Entity\\Entry; \n use Wallabag\\CoreBundle\\Tools\\Utils; \n use Symfony\\Component\\HttpFoundation\\File\\MimeType\\MimeTypeExtensionGuesser; \n use Symfony\\Component\\Validator\\Constraints\\Locale as LanguageConstraint; \n use Symfony\\Component\\Validator\\Constraints\\Url as UrlConstraint; \n use Symfony\\Component\\Validator\\Validator\\ValidatorInterface;", "model_em_trim": 0, "model_bleu_trim": 96.24727879430155}
{"id": 378, "old": "     public function __construct(Request $request)\n     {\n-        if($request->getSession() != null)\n             $this->locale = $request->getSession()->getLang()->getLocale();\n-        else\n             $this->locale = Lang::getDefaultLanguage()->getLocale();\n     }\n     public function guessTinyMCELanguage($params, \\Smarty_Internal_Template $template)", "new": "     public function __construct(Request $request)\n     {\n+        if($request->getSession() != null){\n             $this->locale = $request->getSession()->getLang()->getLocale();\n+        }else{\n             $this->locale = Lang::getDefaultLanguage()->getLocale();\n+        }\n     }\n     public function guessTinyMCELanguage($params, \\Smarty_Internal_Template $template)", "review": "Use braces on your conditional structures please", "dataset": "codereview", "original_id": 2418, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\npublic function __construct(Request $request)\n{\n    if($request->getSession() != null) {\n        $this->locale = $request->getSession()->getLang()->getLocale();\n    } else {\n        $this->locale = Lang::getDefaultLanguage()->getLocale();\n    }\n}\n\npublic function guessTinyMCELanguage($params, \\Smarty_Internal_Template $template)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 100.0, "model_code": "\n public function __construct(Request $request) \n { \n if ($request->getSession() != null) { \n $this->locale = $request->getSession()->getLang()->getLocale(); \n } else { \n $this->locale = Lang::getDefaultLanguage()->getLocale(); \n } \n } \n public function guessTinyMCELanguage($params, \\Smarty_Internal_Template $template)", "model_em_trim": 0, "model_bleu_trim": 100.0, "error_type_gpt": "Code Style Difference"}
{"id": 379, "old": "                                 ''.format(attrname))\n                     vals = cur.fetchall()\n                 except sqlite3.DatabaseError:\n-                    raise IOError(\n-                        \"Failed reading the atoms from DMS Database\")\n                 else:\n                     attrs[attrname] = np.array(vals, dtype=dt)", "new": "                                 ''.format(attrname))\n                     vals = cur.fetchall()\n                 except sqlite3.DatabaseError:\n+                    errmsg = \"Failed reading the atoms from DMS Database\"\n+                    raise IOError(errmsg) from None\n                 else:\n                     attrs[attrname] = np.array(vals, dtype=dt)", "review": "What was the reason for removing `raise from None`? See PR #2357 for rationale.", "dataset": "codereview", "original_id": 2419, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n                                ''.format(attrname))\n                    vals = cur.fetchall()\n                except sqlite3.DatabaseError:\n                    raise IOError(\n                        \"Failed reading the atoms from DMS Database\") from None\n                else:\n                    attrs[attrname] = np.array(vals, dtype=dt)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 85.86764, "model_code": "\n ''.format(attrname)) \n vals = cur.fetchall() \n except sqlite3.DatabaseError: \n raise IOError(\"Failed reading the atoms from DMS Database\") \n else: \n attrs[attrname] = np.array(vals, dtype=dt)", "model_em_trim": 0, "model_bleu_trim": 80.5875511471396, "error_type_gpt": "Need Domain Knowledge"}
{"id": 380, "old": " \t * WP Rocket Options instance.\n \t *\n \t * @since  3.4\n-\t * @access private\n \t *\n \t * @var Options_Data\n \t */", "new": " \t * WP Rocket Options instance.\n \t *\n \t * @since  3.4\n \t *\n \t * @var Options_Data\n \t */", "review": "Same here, remove all `@access`", "dataset": "codereview", "original_id": 2421, "relevance": "perfect", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-remove", "category_type_user1": "Documentation-remove", "category_type_user2": "Documentation-remove", "gpt_code": "\n\t * WP Rocket Options instance.\n\t *\n\t * @since  3.4\n\t *\n\t * @var Options_Data\n\t */\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n * WP Rocket Options instance. \n * \n * @since  3.4 \n * \n * @var Options_Data \n */", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 381, "old": " \treturn true\n }\n-//#####\n-\n func TestConsumedBalances(t *testing.T) {\n \tbalancesFunc := func() (ret map[string]int64, err error) {\n \t\tret = make(map[string]int64)", "new": " \treturn true\n }\n func TestConsumedBalances(t *testing.T) {\n \tbalancesFunc := func() (ret map[string]int64, err error) {\n \t\tret = make(map[string]int64)", "review": "can be removed", "dataset": "codereview", "original_id": 2423, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-remove", "category_type_user1": "Documentation-remove", "category_type_user2": "Documentation-remove", "gpt_code": "\n\treturn true\n}\n\nfunc TestConsumedBalances(t *testing.T) {\n\tbalancesFunc := func() (ret map[string]int64, err error) {\n\t\tret = make(map[string]int64)\n\t\t// revised code starts here\n\t\t// ...\n\t\t// revised code ends here\n\t\treturn ret, nil\n\t}\n\t// ...\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 92.59611, "model_code": "\n return true \n } \n func TestConsumedBalances(t *testing.T) { \n balancesFunc := func() (ret map[string]int64, err error) { \n ret = make(map[string]int64)", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 382, "old": " <section class=\"content-header\">\n-<% content_for :title, \"Edit - Organizations - {#@organization.name}\" %>\n <h1>\n Editing\n <small><%= @organization.name %></small>", "new": " <section class=\"content-header\">\n+<% content_for :title, \"Edit - Organizations - #{@organization.name}\" %>\n <h1>\n Editing\n <small><%= @organization.name %></small>", "review": "Is this a typo?", "dataset": "codereview", "original_id": 2436, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Refactoring-conventions", "category_type_user1": "Documentation-modify", "category_type_user2": "Refactoring-conventions", "gpt_code": "\n<section class=\"content-header\">\n<% content_for :title, \"Edit - Organizations - #{@organization.name}\" %>\n<h1>\nEditing\n<small><%= @organization.name %></small>\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n <section class=\"content-header\"> \n <% content_for :title, \"Edit - Organizations - {#@organization.name}\" %> \n <h1> \n Editing \n <small><%= @organization.name %></small>", "model_em_trim": 0, "model_bleu_trim": 94.14124143301582}
{"id": 383, "old": " \t\tbaseSpiffeIDPrefix := fmt.Sprintf(\"%s/spire/agent\", h.c.TrustDomain.String())\n \t\tsignLog := h.c.Log.WithFields(logrus.Fields{\n-\t\t\t\"caller_id\": callerID,\n-\t\t\t\"spiffe_id\": spiffeID,\n \t\t})\n \t\tif spiffeID == callerID && strings.HasPrefix(callerID, baseSpiffeIDPrefix) {", "new": " \t\tbaseSpiffeIDPrefix := fmt.Sprintf(\"%s/spire/agent\", h.c.TrustDomain.String())\n+\t\tsourceAddress := \"unknown\"\n+\t\tif peerAddress, ok := getPeerAddress(ctx); ok {\n+\t\t\tsourceAddress = peerAddress.String()\n+\t\t}\n+\n \t\tsignLog := h.c.Log.WithFields(logrus.Fields{\n+\t\t\t\"caller_id\":      callerID,\n+\t\t\t\"spiffe_id\":      spiffeID,\n+\t\t\t\"source_address\": sourceAddress,\n \t\t})\n \t\tif spiffeID == callerID && strings.HasPrefix(callerID, baseSpiffeIDPrefix) {", "review": "As long as we're in here, it would be good to add source ip and port to this entry e.g. `source_address`", "dataset": "codereview", "original_id": 2446, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "vague suggestion", "information_user1": "vague question", "information_user2": "vague suggestion", "category_type": "Feature-add", "category_type_user1": "Feature-add", "category_type_user2": "Feature-add", "gpt_code": "\n\t\tbaseSpiffeIDPrefix := fmt.Sprintf(\"%s/spire/agent\", h.c.TrustDomain.String())\n\t\tsignLog := h.c.Log.WithFields(logrus.Fields{\n\t\t\t\"caller_id\":      callerID,\n\t\t\t\"spiffe_id\":      spiffeID,\n\t\t\t\"source_address\": h.c.RemoteAddr.String(),\n\t\t})\n\t\tif spiffeID == callerID && strings.HasPrefix(callerID, baseSpiffeIDPrefix) {\n", "gpt_em_trim": 0, "gpt_bleu_trim": 72.68523, "model_code": "\n baseSpiffeIDPrefix := fmt.Sprintf(\"%s/spire/agent\", h.c.TrustDomain.String()) \n signLog := h.c.Log.WithFields(logrus.Fields{ \n \"caller_id\": callerID, \n \"source_address\": sourceAddress, \n \"spiffe_id\": spiffeID, \n }) \n if spiffeID == callerID && strings.HasPrefix(callerID, baseSpiffeIDPrefix) {", "model_em_trim": 0, "model_bleu_trim": 69.56149913262844}
{"id": 384, "old": "     except (pika.exceptions.ConnectionClosed, AttributeError):\n         pass\n-    ls = InfluxListenStore({ 'REDIS_HOST' : config.REDIS_HOST,\n-        'REDIS_PORT' : config.REDIS_PORT,\n-        'INFLUX_HOST': config.INFLUX_HOST,\n-        'INFLUX_PORT': config.INFLUX_PORT,\n-        'INFLUX_DB_NAME': config.INFLUX_DB_NAME})\n-    listen_count = ls.get_total_listen_count()\n-\n     try:\n         user_count = _get_user_count()\n     except DatabaseException as e:", "new": "     except (pika.exceptions.ConnectionClosed, AttributeError):\n         pass\n+    listen_count = _influx.get_total_listen_count()\n     try:\n         user_count = _get_user_count()\n     except DatabaseException as e:", "review": "Wouldn't using `influx_connection._influx` be better here?", "dataset": "codereview", "original_id": 2454, "relevance": "perfect", "relevance_user1": "not", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\nexcept (pika.exceptions.ConnectionClosed, AttributeError):\n    pass\nls = InfluxListenStore({ 'REDIS_HOST' : config.REDIS_HOST,\n    'REDIS_PORT' : config.REDIS_PORT,\n    'INFLUX_HOST': config.INFLUX_HOST,\n    'INFLUX_PORT': config.INFLUX_PORT,\n    'INFLUX_DB_NAME': config.INFLUX_DB_NAME})\nlisten_count = ls.get_total_listen_count()\n\ntry:\n    influx_connection._influx.query('SELECT COUNT(*) FROM users')\nexcept DatabaseException as e:\n", "gpt_em_trim": 0, "gpt_bleu_trim": 36.08505, "model_code": "\n except (pika.exceptions.ConnectionClosed, AttributeError): \n pass \n try: \n user_count = _get_user_count() \n except DatabaseException as e:", "model_em_trim": 0, "model_bleu_trim": 57.53679691192601, "error_type_gpt": "Need Domain Knowledge"}
{"id": 385, "old": "      * @return a function that applies arguments to the given {@code partialFunction} and returns {@code Some(result)}\n      *         if the function is defined for the given arguments, and {@code None} otherwise.\n      */\n     static <R> Function0<Option<R>> lift(CheckedFunction0<? extends R> partialFunction) {\n-        return () -> Try.of(of(partialFunction)::apply).getOption();\n     }\n     /**", "new": "      * @return a function that applies arguments to the given {@code partialFunction} and returns {@code Some(result)}\n      *         if the function is defined for the given arguments, and {@code None} otherwise.\n      */\n+    @SuppressWarnings(\"RedundantTypeArguments\")\n     static <R> Function0<Option<R>> lift(CheckedFunction0<? extends R> partialFunction) {\n+        return () -> Try.<R>of(partialFunction::apply).getOption();\n     }\n     /**", "review": "We can replace the method call `of(partialFunction)` by providing a type hint `<R>`: ``` java return () -> Try.<R> of(partialFunction::apply).getOption(); ```", "dataset": "codereview", "original_id": 2468, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\n/**\n * Returns a function that applies arguments to the given {@code partialFunction} and returns {@code Some(result)}\n * if the function is defined for the given arguments, and {@code None} otherwise.\n */\nstatic <R> Function0<Option<R>> lift(CheckedFunction0<? extends R> partialFunction) {\n    return () -> Try.<R>of(partialFunction::apply).getOption();\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 56.99985, "model_code": "\n * @return a function that applies arguments to the given {@code partialFunction} and returns {@code Some(result)} \n *         if the function is defined for the given arguments, and {@code None} otherwise. \n */ \n static <R> Function0<Option<R>> lift(CheckedFunction0<? extends R>> partialFunction) { \n return () -> Try.<R>of(partialFunction::apply).getOption(); \n } \n /**", "model_em_trim": 0, "model_bleu_trim": 90.75735352129458}
{"id": 386, "old": " \t\t\t\treturn;\n \t\t\t}\n-\t\t\t$arrResult = null;\n \t\t\t$query_starttime = microtime(true);\n \t\t\ttry", "new": " \t\t\t\treturn;\n \t\t\t}\n \t\t\t$query_starttime = microtime(true);\n \t\t\ttry", "review": "This line can be removed I think because it gets set both in the `try` and in the `catch` block.", "dataset": "codereview", "original_id": 2477, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\ntry {\n    $arrResult = null;\n    $query_starttime = microtime(true);\n    // code block\n} catch (Exception $e) {\n    $arrResult = null;\n    // code block\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 51.59986, "model_code": "\n return; \n } \n $query_starttime = microtime(true); \n try", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 387, "old": "   @Override\n   public void setJsonParameters(Map<String, Object> allParameters) throws Exception {\n     Map<String, Object> parameters = (Map<String, Object>)allParameters.get(\"parameters\");\n-    Long bitmask = (Long) parameters.get(\"type\");\n     type = new ConnectionType(bitmask.intValue());\n   }", "new": "   @Override\n   public void setJsonParameters(Map<String, Object> allParameters) throws Exception {\n     Map<String, Object> parameters = (Map<String, Object>)allParameters.get(\"parameters\");\n+    Number bitmask = (Number) parameters.get(\"type\");\n     type = new ConnectionType(bitmask.intValue());\n   }", "review": "should use Number instead of Long", "dataset": "codereview", "original_id": 2491, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n  @Override\n  public void setJsonParameters(Map<String, Object> allParameters) throws Exception {\n    Map<String, Object> parameters = (Map<String, Object>)allParameters.get(\"parameters\");\n    Number bitmask = (Number) parameters.get(\"type\");\n    type = new ConnectionType(bitmask.intValue());\n  }\n", "gpt_em_trim": 1, "gpt_bleu_trim": 100.0, "model_code": "\n @Override \n public void setJsonParameters(Map<String, Object> allParameters) throws Exception { \n Map<String, Object> parameters = (Map<String, Object>)allParameters.get(\"parameters\"); \n Number bitmask = (Number) parameters.get(\"type\"); \n type = new ConnectionType(bitmask.intValue()); \n }", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 388, "old": " \ts.Status.InitializeConditions()\n }\n-func WithTopicTopic(topicID string) TopicOption {\n \treturn func(s *v1alpha1.Topic) {\n \t\ts.Status.MarkTopicReady()\n \t\ts.Status.TopicID = topicID", "new": " \ts.Status.InitializeConditions()\n }\n+func WithTopicTopicID(topicID string) TopicOption {\n \treturn func(s *v1alpha1.Topic) {\n \t\ts.Status.MarkTopicReady()\n \t\ts.Status.TopicID = topicID", "review": ":smile: Should this be `WithTopicTopic` or `WithTopicTopicID`?", "dataset": "codereview", "original_id": 2493, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\ns.Status.InitializeConditions()\n}\n\nfunc WithTopicTopicID(topicID string) TopicOption {\n\treturn func(s *v1alpha1.Topic) {\n\t\ts.Status.MarkTopicReady()\n\t\ts.Status.TopicID = topicID\n\t}\n}\n", "gpt_em_trim": 1, "gpt_bleu_trim": 81.94717, "model_code": "\n s.Status.InitializeConditions() \n } \n func WithTopicTopicID(topicID string) TopicOption { \n return func(s *v1alpha1.Topic) { \n s.Status.MarkTopicReady() \n s.Status.TopicID = topicID", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 389, "old": " \t\t\t\tbreak;\n \t\t\tcase 'uuid':\n \t\t\t\tif ( ! wp_is_uuid( $value ) ) {\n-\t\t\t\t\treturn new WP_Error( 'rest_invalid_uuid', __( 'Invalid UUID' ) );\n \t\t\t\t}\n \t\t\t\tbreak;\n \t\t}", "new": " \t\t\t\tbreak;\n \t\t\tcase 'uuid':\n \t\t\t\tif ( ! wp_is_uuid( $value ) ) {\n+\t\t\t\t\t/* translators: %s is an invalid universally unique identifier (UUID). */\n+\t\t\t\t\treturn new WP_Error( 'rest_invalid_uuid', sprintf( __( '%s is not a valid UUID.' ), strval( $param ) ) );\n \t\t\t\t}\n \t\t\t\tbreak;\n \t\t}", "review": "Let's include the parameter name in the error message like we do for other validations.", "dataset": "codereview", "original_id": 2494, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Documentation-and-code", "category_type_user1": "Documentation-and-code", "category_type_user2": "Documentation-and-code", "gpt_code": "\n\t\t\t\tbreak;\n\t\t\tcase 'uuid':\n\t\t\t\tif ( ! wp_is_uuid( $value ) ) {\n\t\t\t\t\treturn new WP_Error( 'rest_invalid_uuid', sprintf( __( 'Invalid UUID for parameter %s' ), $param ) );\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 55.35253, "model_code": "\n break; \n case 'uuid': \n if ( ! wp_is_uuid( $value ) ) { \n return new WP_Error( 'rest_invalid_uuid', __( 'Invalid UUID' ) ); \n } \n break; \n }", "model_em_trim": 0, "model_bleu_trim": 44.37395923628235, "error_type_gpt": "Need Domain Knowledge"}
{"id": 390, "old": " \t}\n \tserverMu.Lock()\n-\tdefer serverMu.Unlock()\n \tlocalAdminServer = &http.Server{\n \t\tAddr:              addr.String(), // for logging purposes only\n \t\tHandler:           handler,", "new": " \t}\n \tserverMu.Lock()\n \tlocalAdminServer = &http.Server{\n \t\tAddr:              addr.String(), // for logging purposes only\n \t\tHandler:           handler,", "review": "The lock is only needed for L340, so we don't need a defer here and can move this unlock to L348.", "dataset": "codereview", "original_id": 2529, "relevance": "partial", "relevance_user1": "partial", "relevance_user2": "partial", "information": "concrete suggestion", "information_user1": "vague suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-remove", "category_type_user1": "Feature-remove", "category_type_user2": "Feature-remove", "gpt_code": "\n\t}\n\tserverMu.Lock()\n\tlocalAdminServer := &http.Server{\n\t\tAddr:              addr.String(), // for logging purposes only\n\t\tHandler:           handler,\n\t}\n\tserverMu.Unlock()\n", "gpt_em_trim": 0, "gpt_bleu_trim": 88.97514, "model_code": "\n } \n serverMu.Lock() \n localAdminServer = &http.Server{ \n Addr:              addr.String(), // for logging purposes only \n Handler:           handler,", "model_em_trim": 1, "model_bleu_trim": 100.0}
{"id": 391, "old": " \t\t}\n \t\telse if (type!=null && typeof type==='object') {\n \t\t\tif (type._lastDomChild!==undefined && type._dom!==undefined) {\n-\t\t\t\tlet info = 'Did you accidentilly passed a JSX Literal as JSX twice?';\n \t\t\t\tthrow new Error('Invalid type passed to createElement(): '+type+'\\n\\n'+info+'\\n\\n'+serializeVNode(type));\n \t\t\t}", "new": " \t\t}\n \t\telse if (type!=null && typeof type==='object') {\n \t\t\tif (type._lastDomChild!==undefined && type._dom!==undefined) {\n+\t\t\t\tlet info = 'Did you accidentally passed a JSX Literal as JSX twice?';\n \t\t\t\tthrow new Error('Invalid type passed to createElement(): '+type+'\\n\\n'+info+'\\n\\n'+serializeVNode(type));\n \t\t\t}", "review": "I think this should be `accidentally`", "dataset": "codereview", "original_id": 2532, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "\n\t\t}\n\t\telse if (type!=null && typeof type==='object') {\n\t\t\tif (type._lastDomChild!==undefined && type._dom!==undefined) {\n\t\t\t\tlet info = 'Did you accidentally pass a JSX Literal as JSX twice?';\n\t\t\t\tthrow new Error('Invalid type passed to createElement(): '+type+'\\n\\n'+info+'\\n\\n'+serializeVNode(type));\n\t\t\t}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 97.43927, "model_code": "\n } \n else if (type!=null && typeof type==='object') { \n if (type._lastDomChild!==undefined && type._dom!==undefined) { \n let info = 'Did you accidentally passed a JSX Literal as JSX twice?'; \n throw new Error('Invalid type passed to createElement(): '+type+'\\n\\n'+info+'\\n\\n'+serializeVNode(type)); \n }", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unexpected Grammar Fix"}
{"id": 392, "old": " // AllowedAppTypes lists the types of site/app that can be used.\n var AllowedAppTypes = []string{\"drupal7\", \"drupal8\", \"wordpress\"}\n-// Regexp pattern for alphanumeric + dashes\n var re = regexp.MustCompile(`[^a-zA-Z0-9_-]+`)\n // Config defines the yaml config file format for ddev applications", "new": " // AllowedAppTypes lists the types of site/app that can be used.\n var AllowedAppTypes = []string{\"drupal7\", \"drupal8\", \"wordpress\"}\n+// Regexp pattern to match any non-alphanumeric characters (except dashes) so they can be replaced.\n var re = regexp.MustCompile(`[^a-zA-Z0-9_-]+`)\n // Config defines the yaml config file format for ddev applications", "review": "Don't you want '^[a-zA-Z0-9_-]+$' ?", "dataset": "codereview", "original_id": 2533, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n// AllowedAppTypes lists the types of site/app that can be used.\nvar AllowedAppTypes = []string{\"drupal7\", \"drupal8\", \"wordpress\"}\n// Regexp pattern for alphanumeric + dashes\nvar re = regexp.MustCompile(`^[a-zA-Z0-9_-]+$`)\n// Config defines the yaml config file format for ddev applications\n", "gpt_em_trim": 0, "gpt_bleu_trim": 73.98321, "model_code": "\n // AllowedAppTypes lists the types of site/app that can be used. \n var AllowedAppTypes = []string{\"drupal7\", \"drupal8\", \"wordpress\"} \n // Regexp pattern for alphanumeric and dashes \n var re = regexp.MustCompile(`[^a-zA-Z0-9_-]+`) \n // Config defines the yaml config file format for ddev applications", "model_em_trim": 0, "model_bleu_trim": 79.0503301315048, "error_type_gpt": "Need Domain Knowledge"}
{"id": 393, "old": " \t\t\t'code'    => $lang['code'],\n \t\t\t'current' => $lang['code'] === $sitepress->get_current_language(),\n \t\t\t'anchor'  => $lang['display_name'],\n-\t\t\t'flag'    => '<img class=\"icl_als_iclflag\" src=\"' . $flag_url . '\" alt=\"' . $lang['code'] . '\" width=\"18\" height=\"12\" />',\n \t\t];\n \t}", "new": " \t\t\t'code'    => $lang['code'],\n \t\t\t'current' => $lang['code'] === $sitepress->get_current_language(),\n \t\t\t'anchor'  => $lang['display_name'],\n+\t\t\t'flag'    => '<img class=\"icl_als_iclflag\" src=\"' . esc_url( $flag_url ) . '\" alt=\"' . esc_attr( $lang['code'] ) . '\" width=\"18\" height=\"12\" />',\n \t\t];\n \t}", "review": "Since we are adding all sorts of escapes to please phpcs... ```suggestion 'flag' => '<img class=\"icl_als_iclflag\" src=\"' . esc_url( $flag_url ) . '\" alt=\"' . esc_attr( $lang['code'] ) . '\" width=\"18\" height=\"12\" />', ```", "dataset": "codereview", "original_id": 2538, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Feature-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Feature-modify", "gpt_code": "\n\t\t\t'code'    => esc_attr( $lang['code'] ),\n\t\t\t'current' => $lang['code'] === $sitepress->get_current_language(),\n\t\t\t'anchor'  => esc_html( $lang['display_name'] ),\n\t\t\t'flag'    => '<img class=\"icl_als_iclflag\" src=\"' . esc_url( $flag_url ) . '\" alt=\"' . esc_attr( $lang['code'] ) . '\" width=\"18\" height=\"12\" />',\n\t\t];\n", "gpt_em_trim": 0, "gpt_bleu_trim": 88.08551, "model_code": "\n 'code'    => $lang['code'], \n 'current' => $lang['code'] === $sitepress->get_current_language(), \n 'anchor'  => $lang['display_name'], \n 'flag'    => '<img class=\"icl_als_iclflag\" src=\"' . esc_url( $flag_url ) . '\" alt=\"' . esc_attr( $lang['code'] ) . '\" width=\"18\" height=\"12\" />', \n ]; \n }", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Location"}
{"id": 394, "old": "         var objects = realm.objects(schemas.PersonObject);\n         TestCase.assertTrue(objects[0] instanceof schemas.PersonObject);\n-        function WrongPerson() {}\n-        WrongPerson.schema = schemas.PersonObject.schema;\n         TestCase.assertThrows(function() {\n             realm.objects();", "new": "         var objects = realm.objects(schemas.PersonObject);\n         TestCase.assertTrue(objects[0] instanceof schemas.PersonObject);\n+        function InvalidPerson() {}\n+        InvalidPerson.schema = schemas.PersonObject.schema;\n         TestCase.assertThrows(function() {\n             realm.objects();", "review": "Maybe move this down to the part of the testcase where this data is actually used.", "dataset": "codereview", "original_id": 2546, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague suggestion", "information_user1": "vague suggestion", "information_user2": "vague suggestion", "category_type": "Refactoring-rename", "category_type_user1": "Refactoring-rename", "category_type_user2": "Refactoring-rename", "gpt_code": "\nfunction testPersonObject() {\n    var realm = new Realm({schema: [schemas.PersonObject]});\n    var objects = realm.objects(schemas.PersonObject);\n    TestCase.assertTrue(objects[0] instanceof schemas.PersonObject);\n\n    function WrongPerson() {}\n    WrongPerson.schema = schemas.PersonObject.schema;\n\n    // Move this down to where the data is actually used\n    TestCase.assertThrows(function() {\n        realm.objects();\n    });\n}\n", "gpt_em_trim": 0, "gpt_bleu_trim": 90.79347, "model_code": "\n var objects = realm.objects(schemas.PersonObject); \n TestCase.assertTrue(objects[0] instanceof schemas.PersonObject); \n TestCase.assertThrows(function() { \n realm.objects();", "model_em_trim": 0, "model_bleu_trim": 66.34813199860726}
{"id": 395, "old": " package buildgo\n import (", "new": "+// This isn't a 'real' source file, it's test data for //src/build/go:write_test_main_test\n+\n package buildgo\n import (", "review": "Why the need for this and the very similar test above?", "dataset": "codereview", "original_id": 2547, "relevance": "not", "relevance_user1": "partial", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-add", "category_type_user1": "Documentation-add", "category_type_user2": "Documentation-add", "gpt_code": "\npackage buildgo\n", "gpt_em_trim": 0, "gpt_bleu_trim": 6.370237, "model_code": "\n // Copyright 2016 Canonical Ltd. \n // Licensed under the Apache License, Version 2.0 (the \"License\"); \n // you may not use this file except in compliance with the License. \n // You may obtain a copy of the License at \n // \n //     http://www.apache.org/licenses/LICENSE-2.0 \n // \n // Unless required by applicable law or agreed to in writing, software \n // distributed under the License is distributed on an \"AS IS\" BASIS, \n // See the License for the specific language governing permissions and \n // limitations under the License. \n \n package buildgo \n import (", "model_em_trim": 0, "model_bleu_trim": 3.779188653755054}
{"id": 396, "old": "     <h1>Error 503 Backend is unhealthy</h1>\n     <p>Backend is unhealthy</p>\n     <h3>Guru Mediation:</h3>\n-    <p>Details: cache-sea4439-SEA 1645541667 3074469472</p>\n     <hr>\n     <p>Varnish cache server</p>\n   </body>", "new": "     <h1>Error 503 Backend is unhealthy</h1>\n     <p>Backend is unhealthy</p>\n     <h3>Guru Mediation:</h3>\n+    <p>Details: cache-sea4442-SEA 1645541667 1153894086</p>\n     <hr>\n     <p>Varnish cache server</p>\n   </body>", "review": "I would leave the negative case out (or move it to a test if we don't have coverage). Per previous feedback from the team, the examples should show code that is working and that is copy-pastable.", "dataset": "codereview", "original_id": 2554, "relevance": "not", "relevance_user1": "partial", "relevance_user2": "not", "information": "vague question", "information_user1": "vague question", "information_user2": "vague question", "category_type": "Documentation-modify", "category_type_user1": "Feature-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n<h1>Error 503 Backend is unhealthy</h1>\n<p>Backend is unhealthy</p>\n<h3>Guru Mediation:</h3>\n<p>Details: cache-sea4439-SEA 1645541667 3074469472</p>\n<hr>\n<p>Varnish cache server</p>\n", "gpt_em_trim": 0, "gpt_bleu_trim": 52.20458, "model_code": "\n <h1>Error 503 Backend is unhealthy</h1> \n <p>Backend is unhealthy</p> \n <h3>Guru Mediation:</h3> \n <p>Details: cache-sea4420-SEA 1645541667 2163228833</p> \n <hr> \n <p>Varnish cache server</p> \n </body>", "model_em_trim": 0, "model_bleu_trim": 92.10399279903714}
{"id": 397, "old": "     # create the block\n     block = b.create_block([tx_transfer_signed])\n     b.write_block(block, durability='hard')\n-    # vote the block valid\n     vote = b.vote(block.id, b.get_last_voted_block().id, False)\n     b.write_vote(vote)", "new": "     # create the block\n     block = b.create_block([tx_transfer_signed])\n     b.write_block(block, durability='hard')\n+    # vote the block invalid\n     vote = b.vote(block.id, b.get_last_voted_block().id, False)\n     b.write_vote(vote)", "review": "`# vote the block invalid`", "dataset": "codereview", "original_id": 2560, "relevance": "perfect", "relevance_user1": "perfect", "relevance_user2": "perfect", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-modify", "category_type_user1": "Documentation-modify", "category_type_user2": "Documentation-modify", "gpt_code": "\n    # create the block\n    block = b.create_block([tx_transfer_signed])\n    b.write_block(block, durability='hard')\n    # vote the block invalid\n    vote = b.vote(block.id, b.get_last_voted_block().id, True)\n    b.write_vote(vote)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 96.50938, "model_code": "\n # create the block \n block = b.create_block([tx_transfer_signed]) \n b.write_block(block, durability='hard') \n # vote the block invalid \n vote = b.vote(block.id, b.get_last_voted_block().id, False) \n b.write_vote(vote)", "model_em_trim": 1, "model_bleu_trim": 100.0, "error_type_gpt": "Unclear Changes"}
{"id": 398, "old": " \t\treturn\n \t}\n-\t// Create and edge for the dependency of newNode on parentNode.\n \tparentEdge := g.NewEdge(newNode, parentNode)\n \tg.SetEdge(parentEdge)", "new": " \t\treturn\n \t}\n+\t// Create an edge for the dependency of newNode on parentNode.\n \tparentEdge := g.NewEdge(newNode, parentNode)\n \tg.SetEdge(parentEdge)", "review": "```suggestion // Add the nodes that were meant to be collapsed back to the lookup table. ```", "dataset": "codereview", "original_id": 2561, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "concrete suggestion", "information_user1": "concrete suggestion", "information_user2": "concrete suggestion", "category_type": "Documentation-conventions", "category_type_user1": "Documentation-conventions", "category_type_user2": "Documentation-conventions", "gpt_code": "\n\t\treturn\n\t}\n\t// Add the nodes that were meant to be collapsed back to the lookup table.\n\tfor _, node := range nodesToCollapse {\n\t\tg.nodeLookup[node.ID()] = node\n\t}\n\t// Create and edge for the dependency of newNode on parentNode.\n\tparentEdge := g.NewEdge(newNode, parentNode)\n\tg.SetEdge(parentEdge)\n", "gpt_em_trim": 0, "gpt_bleu_trim": 65.53348, "model_code": "\n return \n } \n // Add the nodes that were meant to be collapsed back to the lookup table. \n parentEdge := g.NewEdge(newNode, parentNode) \n g.SetEdge(parentEdge)", "model_em_trim": 0, "model_bleu_trim": 55.103483494969055}
{"id": 399, "old": "     const tlsConfig = ClientApi.tlsInit(ca);\n     let messageCallback, errorCallback = null;\n-    let isNotiyRunning = false;\n     const startNotify = () => {\n       notify(", "new": "     const tlsConfig = ClientApi.tlsInit(ca);\n     let messageCallback, errorCallback = null;\n+    let isNotifyRunning = false;\n     const startNotify = () => {\n       notify(", "review": "The value assigned to this variable is not really changing as far as I can see. It is defined as `let` so I suppose the intention was to change it, right?", "dataset": "codereview", "original_id": 2570, "relevance": "not", "relevance_user1": "not", "relevance_user2": "not", "information": "vague question", "information_user1": "vague suggestion", "information_user2": "vague question", "category_type": "Refactoring-conventions", "category_type_user1": "Refactoring-conventions", "category_type_user2": "Refactoring-conventions", "gpt_code": "\n    const tlsConfig = ClientApi.tlsInit(ca);\n    let messageCallback = null;\n    let errorCallback = null;\n    let isNotifyRunning = false;\n    const startNotify = () => {\n      notify(\n", "gpt_em_trim": 0, "gpt_bleu_trim": 83.77633, "model_code": "\n const tlsConfig = ClientApi.tlsInit(ca); \n let messageCallback, errorCallback = null; \n let isNotiyRunning = false; \n const startNotify = () => { \n notify(", "model_em_trim": 0, "model_bleu_trim": 91.8243484976617}
